<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="shortcut icon" type="image/x-icon" href="odlogo_small.ico" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
    <meta name="keywords" content="object detection, object recognition, detection, recognition, vision, computer vision, image processing, point cloud, opens ource"/>
    <meta name="description" content="Open Detection, OD, is a standalone open source project for object detection and recognition in images and 3D point clouds."/>
    <meta name="author" content="Kripasindhu Sarkar"/>
<title>Open Detection: GSoC 2016 Blog - Abhishek</title>
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <style>
        .carousel-inner > .item > img,
        .carousel-inner > .item > a > img {
            width: 70%;
            margin: auto;
        }
    </style>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
</script>
<link rel="search" href="search-opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="Open Detection"/>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Open Detection
   &#160;<span id="projectnumber">1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>OD</span></a></li>
      <li><a href="tutorial_root.html"><span>User&#160;Guide</span></a></li>
      <li><a href="usergroup0.html"><span>API&#160;Documentation</span></a></li>
      <li><a href="usergroup1.html"><span>GSoC16&#160;Blogs</span></a></li>
      <li><a href="idea_list_gsoc2016.html"><span>GSoC&#160;2016&#160;Ideas</span></a></li>
      <li><a href="installation_instruction.html"><span>Downloads</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
          <div class="left">
            <form id="FSearchBox" action="search.php" method="get">
              <img id="MSearchSelect" src="search/mag.png" alt=""/>
              <input type="text" id="MSearchField" name="query" value="Search" size="20" accesskey="S" 
                     onfocus="searchBox.OnSearchFieldFocus(true)" 
                     onblur="searchBox.OnSearchFieldFocus(false)"/>
            </form>
          </div><div class="right"></div>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('gsoc2016_blog_abhishek.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">GSoC 2016 Blog - Abhishek </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#gsoc2016_blog_abhishek1">CNN based object localization and recognition for openDetection library  </a></li>
<li class="level1"><a href="#general_description">General Description  </a></li>
<li class="level1"><a href="#commit1">Commit 1  </a></li>
<li class="level1"><a href="#commit2">Commit 2 </a><ul><li class="level2"><a href="#commit2_first_change">2.1)Enabling caffe inclusion, commit 2.1 </a></li>
<li class="level2"><a href="#commit2_second_change">2.2)Adding cnn based classifier, commit 2.2 </a></li>
<li class="level2"><a href="#commit2_second_change_1">2.2.1) Enabling cpu gpu mode switch </a></li>
<li class="level2"><a href="#commit2_second_change_2">2.2.2) Classification base class </a></li>
<li class="level2"><a href="#commit2_second_change_3">2.2.3) Mnist Classification Example </a></li>
<li class="level2"><a href="#commit2_second_change_4">2.2.4) Adding cnn based classifier: Features and benefits  </a></li>
<li class="level2"><a href="#commit2_third_change">2.3)Adding cnn based trainer, commit 2.3 </a></li>
<li class="level2"><a href="#commit2_third_change_1">2.3.1)Training Base Class </a></li>
<li class="level2"><a href="#commit2_third_change_2">2.3.2)Mnist Training Example</a></li>
<li class="level2"><a href="#commit2_fourth_change">2.4)Introducing a custom solver, commit 2.4 </a></li>
<li class="level2"><a href="#commit2_fourth_change_1">2.4.1)How the GTKMM library was introduced </a></li>
<li class="level2"><a href="#commit2_fourth_change_2">2.4.2) Custom Solver base class and usage details for every parameter/entry in the gui</a></li>
<li class="level2"><a href="#commit2_fourth_change_3">2.4.3)Pressing the buttons in the solver creator</a></li>
<li class="level2"><a href="#commit2_fourth_change_4">2.4.4) Calling the solverProperties class from ODConvTrainer and the corresponding mnist curtom solver example</a></li>
<li class="level2"><a href="#commit2_fourth_change_5">2.4.5)Features of solver creator</a></li>
<li class="level2"><a href="#commit2_fifth">2.5) Network creator first version, commit 2.5</a></li>
<li class="level2"><a href="#commit2_fifth_change_1">2.5.1) Current Features to the </a></li>
<li class="level2"><a href="#commit2_fifth_change_2">2.5.2) How to introduce a dropdown menu example</a></li>
<li class="level2"><a href="#commit2_fifth_change_3">2.5.3) Idea of multiple windows</a></li>
<li class="level2"><a href="#commit2_fifth_change_4">2.5.4) Displaying network and deleting layer at the end</a></li>
<li class="level2"><a href="#commit2_sixth">2.6) Network creator second version, commit 2.6</a></li>
<li class="level2"><a href="#commit2_seventh">2.7) Selective search based object locatization version 1, commit 2.7</a></li>
<li class="level2"><a href="#commit2_seventh_change_1">2.7.1) Graph based image segmentation</a></li>
<li class="level2"><a href="#commit2_seventh_change_2">2.7.2) Selective Search Base class</a></li>
<li class="level2"><a href="#commit2_seventh_change_3">2.7.3) Selective Search Model class and example of selective search based localization</a></li>
</ul>
</li>
<li class="level1"><a href="#commit3">Commit 3 </a></li>
<li class="level1"><a href="#commit4">Commit 4 </a></li>
<li class="level1"><a href="#commit5">Commit 5 </a></li>
<li class="level1"><a href="#commit6">Commit 6 </a></li>
<li class="level1"><a href="#commit7">Commit 7 </a><ul><li class="level2"><a href="#commit7_first">7.1) Features and usage of the version 1 of the annotator </a></li>
<li class="level2"><a href="#commit7_second">7.2) Annotation base class </a></li>
<li class="level2"><a href="#commit7_third">7.3)Upper Annotator Class </a></li>
</ul>
</li>
<li class="level1"><a href="#commit8">Commit 8 </a></li>
<li class="level1"><a href="#commit9">Commit 9 </a></li>
<li class="level1"><a href="#commit10">Commit 10 </a><ul><li class="level2"><a href="#commit10_first">10.1) How to delete a particular layer in network creator </a></li>
</ul>
</li>
<li class="level1"><a href="#commit11">Commit 11 </a><ul><li class="level2"><a href="#commit11_first">11.1) Using the feature of adding layers in between already created layers in network creator</a></li>
</ul>
</li>
<li class="level1"><a href="#commit12">Commit 12 </a><ul><li class="level2"><a href="#commit12_first">12.1) Cropping multiple sections from same image</a></li>
</ul>
</li>
<li class="level1"><a href="#commit13">Commit 13 </a></li>
<li class="level1"><a href="#commit14">Commit 14 </a></li>
<li class="level1"><a href="#commit15">Commit 15 </a><ul><li class="level2"><a href="#commit15_first">15.1) Non rectangualr ROI with label using annotator</a></li>
</ul>
</li>
<li class="level1"><a href="#commit16_17_18">Commits 16, 17 and 18 </a></li>
<li class="level1"><a href="#commit19">Conclusion</a></li>
<li class="level1"><a href="#oldBlog">Older sections of the blog</a><ul><li class="level2"><a href="#target1">Classification of digits in Mnist Library using CNN </a></li>
<li class="level2"><a href="#target2">Training a classifier for digits in Mnist Library using CNN. Part 1 </a></li>
<li class="level2"><a href="#target2_1">Training a classifier for digits in Mnist Library using CNN. Part 2 </a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="gsoc2016_blog_abhishek1"></a>
CNN based object localization and recognition for openDetection library  </h1>
<ul>
<li><a href="https://summerofcode.withgoogle.com/serve/6621875723567104/">Link to Proposal</a></li>
<li><a href="https://github.com/abhi-kumar/opendetection/tree/cnn_cpu_gpu">Link to working branch</a></li>
</ul>
<h1><a class="anchor" id="general_description"></a>
General Description  </h1>
<p>The following blog has been modified to accompany the entire GSOC project duration. The code, commits and examples explained in this blog is related to the commits that have been made to the branch cnn_cpu_gpu as mentioned in this <a href="https://github.com/abhi-kumar/opendetection/tree/cnn_cpu_gpu">link</a>. The set of commits to other temporary branches and the commits with unsuccessful builds from the same have been corrected and put into the mentioned branch with a set of around 20 commits. On the whole, the entire project has been mentioned here as a project report.</p>
<p>The set of other branches, for reference purposes can be found here: <a href="https://github.com/abhi-kumar/opendetection/tree/master">link</a></p>
<p>The project started with the first basic task of making the repository compile on a cpu platform. Examples of Mnist Classifier, custom cnn caffe solver maker, etc were added. After the availability of a gpu based system, the corresponding work was translated to that system. Later the work was combined and new additions like AAM classifier, custom trainer, segnet classifier, annotator, selective search localization, etc, were added. The upcoming part of the blog emphasizes on the important commits made, including all the successful commits to one unified single branch in a cronological order. This branch was specifically created to explain the work done in the GSOC period. It also includes the usage and explanation of the examples for user feasibility. The earlier blog posts have been pushed as later sections. Link to commits to other branches (all already combined and pushed to cnn_cpu_gpu branch and explained below) are also mentioned at the end for reference.</p>
<p><b>Work1:</b> The primary task undertaken was to make sure that the library compiled on both GPU and non-GPU based platforms. Earlier, the library was restricted to only GPU based platforms, due to the fact that, irrespective of the fact whether cuda library is installed in the system, the library fetched for headers from cuda. With a set of 45 additions and 1 deletions over 7 files, this task was undertaken.</p>
<p><b>Work2:</b> The next target was to include caffe library components into the opendetection library. Opendetection library is like a pool of object detection elements, and without the integration of Convolutional Neural Networks, it would remain incomplete. there exist a lot of opensource library which support training and classification using CNN, like caffe, keras, torch, theone, etc, of which after we selected caffe, because of its simplicity in usage and availability of blogs, tutorials and high end documentation on the same.</p>
<p><b>Work3:</b> Once the library was included, the next was to include CNN based image classifier, over a c++ based code. Usually, researchers use the python wrapper provided by the caffe library to train a network or to use trained wieghts and network in classifying an image, i.e., assigning a predicted label to the test image. Herein, the task was completed with around 400 lines of code over 7 files. Any python wrapper reduces the speed of execution and in turn provides a lag in real time based applications. Also, the transfer of memory from cpu to gpu, on gpu based systems, is quite slow when the upper level code is in python. For this reason, we have directly accessed the c++ code files from the library and linked to our opendetection ODDetector class. As an example we have provided the standard Mnist digit classifier. In the example the user just needs to point a network file, trained weights and a test image, and the classification result will be obtained.</p>
<p><b>Work4:</b> Just adding the classification abilities would make the library only half complete. Hence, for this reason we added the module which would enable the users to train their own module. With a total of around 250 changes made to 5 files, this training class was added to ODTrainer. User would only need to point towards the network and the solver file. Here again, a training example is added using Mnist digit dataset.</p>
<p><b>Work5:</b> As stated above, a cnn based training requires a network file and a solver file. Any solver in caffe library has around 20 parameters. It is a tedious job to write the solver file from scratch, everytime a training has to be commenced. For this reason, to facilitate user feasibility over the solver properties a GUI has been introduced. This GUI has all the parameters involved in solver file. Also the user while using the gui has the facility to include or exclude a parameter. This particular commit had changes added or additions made to 9 files. The most crucial one was to add gtkmm library files to the source. GTKMM, link to understand gtkmm, is a library for involvement of gui based applications. We decided to move with GUI inclusion because, to make user handle solver file in an effective way, a set of 19 parameters had to be handled. If it were upto the c++ arguments to facilitate these 19 parameters, the outcome would have been a very cumbersome application. Also, not all parameters were to be added to the solver always, so a GUI appeared to be the most feasible option from the user's end. A set of around 1250 lines of code made this module integrated into the opendetection library. The following are a few features of the GUI:</p>
<ul>
<li>The above code promts the user if any mistake is made from user-end.</li>
<li>Pressing update button every time may be time consuming, hence the latest commits involve the fact that without pressing the buttons the parameters cab ne edited</li>
<li>The main function of the update buttons after every parameter is make sure that, for future developments, if the intermediate parameters are to be accessed, the current version enables it.</li>
<li>Not many open source libraries had this functionality</li>
</ul>
<p><b>Work6:</b> After solver, the next important thing to training is network file. A network file in CNN has the structure of the CNN, the layers, their individual properties, weight initializers, etc. Like the solver maker, we have created a module which provides a GUI to make this network. Every network has lot many properties, writing them manually into the file is a time consuming process. For this reason, the GUI was implemented, so that with just a few clicks and details any layer could be added to the network. a) The activation category includes the following activation layers</p>
<ul>
<li>Absolute Value (AbsVal) Layer</li>
<li>Exponential (Exp) Layer</li>
<li>Log Layer</li>
<li>Power Layer</li>
<li>Parameterized rectified linear unit (PReLU) Layer</li>
<li>Rectified linear unit (ReLU) Layer</li>
<li>Sigmoid Layer</li>
<li>Hyperbolic tangent (TanH) Layer</li>
</ul>
<p>b) The critical category includes the most crucial layers</p>
<ul>
<li>Accuracy Layer</li>
<li>Convolution Layer</li>
<li>Deconvolution layer</li>
<li>Dropout Layer</li>
<li>InnerProduct (Fully Connected) Layer</li>
<li>Pooling Layer</li>
<li>Softmax classification Layer</li>
</ul>
<p>c) The weight initializers include the following options</p>
<ul>
<li>Constant</li>
<li>Uniform</li>
<li>Gaussian</li>
<li>Positive Unit Ball</li>
<li>Xavier</li>
<li>MSRA</li>
<li>Bilinear</li>
</ul>
<p>d) Normalization layer includes the following options</p>
<ul>
<li>Batch Normalization (BatchNorm) Layer</li>
<li>Local Response Normalization (LRN) Layer</li>
<li>Multivariate Response Normalization (MVN) Layer</li>
</ul>
<p>e) Loss Layer includes the followin optons: -Hinge Loss Layer</p>
<ul>
<li>Contrastive Loss Layer</li>
<li>Eucledean Loss Layer</li>
<li>Multinomial Logistig Loss Layer</li>
<li>Sigmoid Cross Entropy Loss Layer</li>
</ul>
<p>f) Data and Extra Layers:</p>
<ul>
<li>Maximum Argument (ArgMAx) Layer</li>
<li>Binomial Normal Log Likelihood (BNLL) Layer</li>
<li>Element wise operation (Eltwise) Layer</li>
<li>Image Data Layer</li>
<li>LMDB/LEVELDB Data Layer</li>
</ul>
<p>g) Every Layer has all the parameters listed in the GUI, of which the non compulsory parameters can be kept commented using the radiobutton in the GUI,</p>
<p>h) One more important feature included is that user can display the layers.</p>
<p>i) Once the structure is made, and is displayed, the user may also be able to delete any layer he/she has added to the layer.</p>
<p>These properties of the GUI were made possible with a set of aorund 6500 lines of code over a range of arounf 12-15 files.</p>
<p><b>Work7:</b> Active Appereance Model feature points over the face have had many application like emotion detection, face recognition etc. It's of the personal researches we have undertaken which is based on finding these feature points using Convolutional Neural Networks. The network and the trained weights presented in the example in the library is one of the base models we have used. The main reason to add this feature was to show as to how widespread the uses of the integration of caffe library with opendetection could be to the users. Very few works exist on this end, and hence the purpose behind taking up the research. This is a very crude and preliminary model of the research, just for the young users to be encouraged as to the extent to which cnn may work and how opendetection algorithm would help facilitate the same.</p>
<p><b>Work8:</b> Object reconition has two components: object localization and then classification. Classification module has already be included in the system, the localization part is introduced in this work. The task of object localization has been completed using selective search algorithm. The algo, when put simply, involves, Graph based image segmentation, followed by finding different features of the all the segmented parts, then finding closeness between the features of the neighboring parts and finally merging the closest parts and continuing futher till the algorithm is breaked. The image segmentation was adopted from Graph based image segementation mentioned <a href="http://cs.brown.edu/~pff/segment/">here</a> with proper permissions. The next part involved image preprocessing, which had conversion of BGR image to YCrCb, equalizing the first channel and reconversion of equalized YCrCb image to BGR color type. This was followed by the steps: image is stored in ".ppm" format as the segmentation code only prefers image in that format. Image is then segmented using the segment_image function and to find the number of segments, num, it is converted to grayscale and the number of colors there then represent the number of segments. The next step is to create a list of those segments. It is not often possible to create an uchar grayscale image mask with opencv here, because, opencv supports color version from 0 to 255 and in most cases the segments are greater than 255. Thus, we first store, every pixel's value in the previous rgb image with the pixel's location into a text file named "segmented.txt".Finally, the steps were adopted, calculating histogram of the different features ( hessian matrix, orientation matrix, color matrix, differential excitation matrix), finding neighbors for each of the clustered region, finding similarities( or closure distance) between two regions based on the histogram of different features, merging the closest regions removing very small and very big clusters, and adding ROIs to images based on merged regions. This selective search has a set of 13 parameters which drive the entire algo here. The work here was completed with addition of around 2000 lines of code.</p>
<p><b>Work9:</b> Segnet is a caffe derived library used for object recognition and segmentation purposes. It is a widely used library and the components are very much similar to caffe library. Thus there existed this logical compulsion to include the library so that the users may use segnet based training/classification/segmentation through opendetection wrapper. Addition of this library would allow segnet library users to attach it to opendetection in way as done with caffe library. Herein, the example included for now, is a python wrapper based image segmentation preview. The network and the weights are adopted from segnet example module.</p>
<p><b>Work10:</b> Any image classifier training requires the dataset to be annotated. For this reason, we have added an annotation tool, which will enable users to label, crop or create bounding boxes over an object in image. The output of this tool is customized in a way which is required by the caffe library.</p>
<p>The features and some usage points involved are:</p>
<ul>
<li>User may load a single image from a location using the "Select the image location" button or the user may point towards a complete image dataset folder.</li>
<li>Even if the user points to a dataset folder, there exists an option of choosing an image from some another location while the annotation process is still on.</li>
<li>Even if user selects a single image, the user may load more single images without changing the type of annotation.</li>
<li>The first type of annotation facility is, annotating one bounding box per image.</li>
<li>The second, annotating and cropping one bounding box per image.</li>
<li>The third one, annotating multiple bounding boxes per image, with attached labels.</li>
<li>The fourth one, cropping multiple sections from same image, with attached labels.</li>
<li>The fifth one, annotationg a non rectangular ROI, with attached labels.</li>
<li>If a user makes mistake in annotation, the annotation can be reset too.</li>
</ul>
<p>Note: Every image that is loaded, is resized to 640x480 dimensions, but the output file has points of the bounding boxes as the original image size</p>
<p>The output files generated in the cases have annotation details as,</p>
<ul>
<li>First case, every line in the output text file has a image name followed by four points x1 y2 x2 y2, first two representing top left coordinate of the box and the last two representing bottom right coordinates of the box.</li>
<li>Second case, every line in the output text file has a image name followed by four points x1 y2 x2 y2, first two representing top left coordinate of the box and the last two representing bottom right coordinates of the box. The cropped images are stored in the same folder as the original image, with name, &lt;original_image_name&gt;_cropped.&lt;extension_of_the_original_image&gt;</li>
<li>Third case, every line in the output text file has a image name followed by a lebel and then the four points x1 y2 x2 y2, first two representing top left coordinate of the box and the last two representing bottom right coordinates of the box. If there are multiple bounding boxes, then after image name there is a label, then four points, followed another label, and the corresponding four points and so on.</li>
<li>Fourth case, Once the file is saved, the cropped images will be saved in the same forlder as the original image with name as &lt;original_image_name&gt;_cropped_&lt;label&gt;_&lt;unique_serial_id&gt;.&lt;extension_of_the_original_image&gt;.</li>
<li>Fifth case, The output of the file will be saved as filename, followed by an unique id to the ROI, label of the roi, set of points in the roi, then again another id, its label and the points and so on.</li>
</ul>
<p>To select any of these cases, select the image/dataset and then press the "Load the image" button.</p>
<p>First case usage</p>
<ul>
<li>Select the image or the dataset folder.</li>
<li>Press the "Load the image" button.</li>
<li>To create any roi, first left click on top left point of the supposed roi and then right click on the bottom right point of the supposed roi. A green rectangular box will appear.</li>
<li>Now, if its not the one you meant it, please click "Reset Markings" Button and repoint the new roi.</li>
<li>If the ROI is fine, press "Select the ROI" button.</li>
<li>Now, load another image or save the file.</li>
</ul>
<p>Second case usage</p>
<ul>
<li>Select the image or the dataset folder.</li>
<li>Press the "Load the image" button.</li>
<li>To create any roi, first left click on top left point of the supposed roi and then right click on the bottom right point of the supposed roi. A green rectangular box will appear.</li>
<li>Now, if its not the one you meant it, please click "Reset Markings" Button and repoint the new roi.</li>
<li>If the ROI is fine, press "Select the ROI" button.</li>
<li>Now, load another image or save the file.</li>
</ul>
<p>Third case usage</p>
<ul>
<li>Select the image or the dataset folder.</li>
<li>Press the "Load the image" button.</li>
<li>To create any roi, first left click on top left point of the supposed roi and then right click on the bottom right point of the supposed roi. A green rectangular box will appear.</li>
<li>Now, if its not the one you meant it, please click "Reset Markings" Button and repoint the new roi.</li>
<li>If the ROI is fine, please type an integer label in the text box and press "Select the ROI" button.</li>
<li>Now, you may draw another roi, or load another image, save the file.</li>
<li>Note: In the third case, the one with multiple ROIs per image, if a boundix box is selected for an image and you are trying to make another and press the reset button, the selected roi will not be deleted. Any selected roi cannot be deleted as of now.</li>
</ul>
<p>Fourth case usage</p>
<ul>
<li>Select the image or the dataset folder.</li>
<li>Press the "Load the image" button.</li>
<li>To create any roi, first left click on top left point of the supposed roi and then right click on the bottom right point of the supposed roi. A green rectangular box will appear.</li>
<li>Now, if its not the one you meant it, please click "Reset Markings" Button and repoint the new roi.</li>
<li>If the ROI is fine, please type an integer label in the text box and press "Select the ROI" button.</li>
<li>Now, you may draw another roi, or load another image, save the file.</li>
<li>Once the file is saved, the cropped images will be saved in the same forlder as the original image with name as &lt;original_image_name&gt;_cropped_&lt;label&gt;_&lt;unique_serial_id&gt;.&lt;extension_of_the_original_image&gt;</li>
</ul>
<p>Fifth case usage</p>
<ul>
<li>Select the image or the dataset folder.</li>
<li>Press the "Load the image" button.</li>
<li>To create any roi, Click on the points needed only with left click.</li>
<li>Now, if its not the one you meant it, please click "Reset Markings" Button and repoint the new roi.</li>
<li>If the ROI is fine, please type an integer label in the text box and press "Select the ROI" button. A gree color marking covering the region and passing through the points you have selected will appear.</li>
<li>Now, you may draw another roi, or load another image, save the file.</li>
</ul>
<p>Thus, this tool, is an extremely important addition to the project and was added as a set of 1600 lines of code on around 6-8 files in the opendetection library.</p>
<p>This was the overview of the tasks completed in the GSOC project. Lets understand these tasks with the respective code snippets.</p>
<p><b>Note: The following commits were not the only ones, a set of commits have been combined and stated here, to facilitate clear explanation and organization.</b></p>
<h1><a class="anchor" id="commit1"></a>
Commit 1  </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/245f821631452de18aaad13bab3e1f8fd7f333a4">link to commit: First attempt to make cpu and gpu repos together</a>, was issued to resolve the earlier problem faced while compiling the library. Earlier, even with the option of WITH_GPU=OFF in cmake, the library fetched for cuda and gpu based files in the system. To resolve the following changes were applied.</p>
<p>The task was to take a flag from CMakeLists files into the source codes in order proper management of inclusion of libraries, thus, in CmakeLists.txt files,</p>
<div class="fragment"><div class="line"><span class="keywordflow">if</span> (WITH_GPU) #This is the cmake option variable</div>
<div class="line">  target_compile_definitions(<span class="stringliteral">&quot;${LIB_NAME}&quot;</span> PUBLIC WITH_GPU=${WITH_GPU})</div>
<div class="line">endif()</div>
</div><!-- fragment --><p>, was added. The flag WITH_GPU now was translated into the source codes. This was done to files, detectors/global3D/CMakeLists.txt, and common/CMakeLists.txt.</p>
<p>This particular flag, WITH_GPU was taken into four set of files, common/utils/ODFeatureDetector2D.cpp, <a class="el" href="_o_d_feature_detector2_d_8h_source.html">common/utils/ODFeatureDetector2D.h</a>, detectors/local2D/detection/simple_ransac_detection/RobustMatcher.cpp and detectors/local2D/detection/simple_ransac_detection/RobustMatcher.h. Herein, for example, cuda variable declaration, involvement of gpu based variables, and inclusion of cuda libraries were put inside the flag WITH_GPU, eg,</p>
<div class="fragment"><div class="line"><span class="preprocessor">#if(WITH_GPU)</span></div>
<div class="line"><span class="preprocessor"></span>  cv::Ptr&lt;cv::cuda::DescriptorMatcher&gt; matcher_gpu_;</div>
<div class="line"><span class="preprocessor">#endif</span></div>
</div><!-- fragment --><p>The third change was to include the linking libraries in CMakeLists.txt files of specific gpu based libraries with the cmake option WITH_GPU option. The library linkage siftgpu in the file detectors/global3D/CMakeLists.txt, was modified to</p>
<div class="fragment"><div class="line"><span class="keywordflow">if</span>(WITH_GPU)</div>
<div class="line">  set(SUBSYS_DEPS ${SUBSYS_DEPS} siftgpu)</div>
<div class="line">endif()</div>
</div><!-- fragment --><p>.</p>
<p>The library then compiled successfully on my laptop(cpu system) as well as my PC(Nvidia 980 TI based gpu system).</p>
<p>Happy Coding!!!!</p>
<h1><a class="anchor" id="commit2"></a>
Commit 2 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/3c4a12b5711e84f2145851b526ab2e84d9127889">link to commit:CNN_CPU branch successfully added</a>, was issued to add cnn-caffe based applications, only for cpu based systems, for now. The commit shows 42055 files changes, its because Mnist image dataset was added in order to introduce the object classifier trainer in the commit. Its a combination of set of commits made earler, put here together for better explanation of each of them. The links to these frangmented commits are also mentioned here.</p>
<p>To better understand neural networks, please refer to <a href="https://abhishek4273.com/2015/11/01/artificial-neural-networks-and-the-magic-behind-introductory-chapter/">ARTIFICIAL NEURAL NETWORKS AND THE MAGIC BEHIND – INTRODUCTORY CHAPTER</a></p>
<p>To get into the working of mathematics behind neural networks, please refer to <a href="https://abhishek4273.com/2015/11/04/artificial-neural-networks-and-the-magic-behind-chapter-1/">ARTIFICIAL NEURAL NETWORKS AND THE MAGIC BEHIND – CHAPTER 1</a></p>
<p>To understand as to how every part of caffe works, please refer to <a href="https://abhishek4273.com/2016/02/07/ann-chapter-3-deep-learning-using-caffe-python/">ANN: CHAPTER 3. DEEP LEARNING USING CAFFE-PYTHON</a>.</p>
<h2><a class="anchor" id="commit2_first_change"></a>
2.1)Enabling caffe inclusion, commit 2.1 </h2>
<p>The first change, <a href="https://github.com/abhi-kumar/opendetection/commit/166b6426acbc3253ee53d09a8ce240d3739c8762">link to the fragmented part commit</a>, to enable the compilation of based codes, the file, cmake/od_mandatory_dependency.cmake, was changed. Herein, caffe library was made as a mandatory dependency and the libraries and include directories were added.</p>
<h2><a class="anchor" id="commit2_second_change"></a>
2.2)Adding cnn based classifier, commit 2.2 </h2>
<p>The next part, was to add a cnn based classifier, based on caffe library, to the opendetection source. The <a href="https://github.com/abhi-kumar/opendetection/commit/166b6426acbc3253ee53d09a8ce240d3739c8762">fragmented commit: Commit for Mnist Classification Example Using Caffe Library</a>, involved additions of 7 code source files, 1 trained binary file and 5 sample test images.</p>
<h2><a class="anchor" id="commit2_second_change_1"></a>
2.2.1) Enabling cpu gpu mode switch </h2>
<p>The file, detectors/global2D/CMakeLists.txt, was added the following lines</p>
<div class="fragment"><div class="line">ADD_DEFINITIONS(</div>
<div class="line">  -std=c++11 </div>
<div class="line">  ${Caffe_DEFINITIONS}</div>
<div class="line">)</div>
</div><!-- fragment --><p> This was to make sure that, while invoking caffe, the mode can be specified without compilation errors.</p>
<h2><a class="anchor" id="commit2_second_change_2"></a>
2.2.2) Classification base class </h2>
<p>The base class for cnn based classification, is ODConvClassification, and is introduced with two new files, <a class="el" href="_o_d_conv_classification_8h_source.html">detectors/global2D/detection/ODConvClassification.h</a> and detectors/global2D/detection/ODConvClassification.cpp.</p>
<p>Lets talk about the codes in these files.</p>
<p>It involves the inclusion of following opendetection libraries </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;common/pipeline/ODDetector.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;common/pipeline/ODScene.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;common/utils/utils.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;common/utils/ODFeatureDetector2D.h&quot;</span> </div>
</div><!-- fragment --><p> The <a class="el" href="_o_d_detector_8h_source.html">ODDetector.h</a> is included in order to make sure that the ODConvolutionClassification is a derivative of ODDetector class.</p>
<p>Opencv library </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;opencv2/opencv.hpp&gt;</span></div>
</div><!-- fragment --><p> is added to make sure that image loading and saving is done using the Mat constructor</p>
<p>A set of general c++ header are also added </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;cstring&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;cstdlib&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;vector&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;string&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;stdio.h&gt;</span></div>
</div><!-- fragment --><p>Three header files from caffe library are added in order to make sure that caffe library can be succesfully included. </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;caffe/caffe.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/util/io.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/blob.hpp&quot;</span></div>
</div><!-- fragment --><p>Under the namespace <a class="el" href="namespaceod_1_1g2d.html">od::g2d</a>, the class at this stage had the following public variables, see comments in code. </p>
<div class="fragment"><div class="line"><span class="keywordtype">string</span> weightModelFileLoaction;   <span class="comment">// Stores the trained weight caffemodel&#39;s location</span></div>
<div class="line"><span class="keywordtype">string</span> networkFileLocation;   <span class="comment">// Stores the network prototxt file&#39;s location</span></div>
<div class="line"><span class="keywordtype">string</span> imageFileLocation;     <span class="comment">// Stores the test image&#39;s location</span></div>
<div class="line">Datum strucBlob;      <span class="comment">// A structure to load the test image</span></div>
<div class="line">BlobProto protoBlob;      <span class="comment">// Medium for conversion</span></div>
<div class="line">vector&lt;Blob&lt;float&gt;*&gt; inputBlob;   <span class="comment">// Modified structure to be given to caffe network </span></div>
</div><!-- fragment --><p><br/>
 At this stage, the classifier had 8 functions which were created for classification purpose, and the 4 functions which had to be included because the base class had them as abstract.</p>
<p><b>Function1</b> </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvClassification::setWeightModelFileLocation(<span class="keywordtype">string</span> location)</div>
<div class="line">{</div>
<div class="line">  ODConvClassification::weightModelFileLoaction = location;</div>
<div class="line">}</div>
</div><!-- fragment --><p> It is for taking the weight caffemodel's location from user and storing in the string weightModelFileLoaction.</p>
<p><b>Function2</b> </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvClassification::setNetworkModelFileLocation(<span class="keywordtype">string</span> location)</div>
<div class="line">{</div>
<div class="line">  networkFileLocation = location;</div>
<div class="line">}</div>
</div><!-- fragment --><p> It is for taking the network prototxt file's location from user and storing in the string networkFileLocation.</p>
<p><b>Function3</b> </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvClassification::setImageFileLocation(<span class="keywordtype">string</span> location)</div>
<div class="line">{</div>
<div class="line">  imageFileLocation = location;</div>
<div class="line">}</div>
</div><!-- fragment --><p> It is for taking the test_image file's location from user and storing in the string imageFileLocation.</p>
<p><b>Function 4, 5 and 6</b> </p>
<div class="fragment"><div class="line"><span class="keywordtype">string</span> ODConvClassification::getWeightModelFileLocation()</div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Weight Model File Location = &quot;</span> &lt;&lt; weightModelFileLoaction &lt;&lt; endl;</div>
<div class="line">  <span class="keywordflow">return</span> weightModelFileLoaction;</div>
<div class="line">}</div>
<div class="line"><span class="keywordtype">string</span> ODConvClassification::getNetworkModelFileLocation()</div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Network Model File Location = &quot;</span> &lt;&lt; networkFileLocation &lt;&lt; endl;</div>
<div class="line">  <span class="keywordflow">return</span> networkFileLocation;</div>
<div class="line">}</div>
<div class="line"><span class="keywordtype">string</span> ODConvClassification::getImageFileLocation()</div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Image File Location = &quot;</span> &lt;&lt; imageFileLocation &lt;&lt; endl;</div>
<div class="line">  <span class="keywordflow">return</span> imageFileLocation;</div>
<div class="line">}</div>
</div><!-- fragment --><p> These are retrieving the the three variables, weightModelFileLoaction, networkFileLocation and imageFileLocation.</p>
<p><b>Function 7</b> The function, setTestBlob, is one of the very important functions. It takes the image and converts it into the format required by the caffe library. It takes three inputs,</p>
<ul>
<li>numChannels: Number of channels in image. If grayscale image it has to be 1, else if the image is in RGB or HSV format, then it has to be 3.</li>
<li>imgHeight: Height to which it should be resized to fit into the caffe network.</li>
<li>imgWidth: Width to which it should be resized to fit into the caffe network.</li>
</ul>
<div class="fragment"><div class="line"><span class="keywordflow">if</span> (!ReadImageToDatum(imageFileLocation, numChannels, imgHeight, imgWidth, &amp;strucBlob)) </div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Image File Not Found&quot;</span> &lt;&lt; endl;</div>
<div class="line">  exit(0);</div>
<div class="line">} </div>
</div><!-- fragment --><p>This particular section, makes sure that the image is converted to datum format, more details on <a href="https://github.com/BVLC/caffe/wiki/The-Datum-Object">datum</a></p>
<p>Next, </p>
<div class="fragment"><div class="line">Blob&lt;float&gt;* dataBlob = <span class="keyword">new</span> Blob&lt;float&gt;(1, strucBlob.channels(), strucBlob.height(), strucBlob.width());</div>
</div><!-- fragment --><p> Herein, a new blob, <a href="http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html">link to understand blob</a>, is created. This is the blob which will hold the input image in format reuired by caffe.</p>
<p>Now, </p>
<div class="fragment"><div class="line">protoBlob.set_num(1);</div>
<div class="line">protoBlob.set_channels(strucBlob.channels());</div>
<div class="line">protoBlob.set_height(strucBlob.height());</div>
<div class="line">protoBlob.set_width(strucBlob.width());</div>
<div class="line"><span class="keyword">const</span> <span class="keywordtype">int</span> data_size = strucBlob.channels() * strucBlob.height() * strucBlob.width();</div>
<div class="line"><span class="keywordtype">int</span> sizeStrucBlob = std::max&lt;int&gt;(strucBlob.data().size(), strucBlob.float_data_size());</div>
<div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; sizeStrucBlob; ++i) </div>
<div class="line">{</div>
<div class="line">  protoBlob.add_data(0.);</div>
<div class="line">}</div>
<div class="line"><span class="keyword">const</span> <span class="keywordtype">string</span>&amp; data = strucBlob.data();</div>
<div class="line"><span class="keywordflow">if</span> (data.size() != 0) </div>
<div class="line">{</div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; sizeStrucBlob; ++i)</div>
<div class="line">  {</div>
<div class="line">    protoBlob.set_data(i, protoBlob.data(i) + (uint8_t)data[i]);</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line">dataBlob-&gt;FromProto(protoBlob);</div>
<div class="line">inputBlob.push_back(dataBlob);</div>
</div><!-- fragment --><p> In this last section, the temporary structure, protoBlob, takes input image and converts into the reuired format with assigned dimensions, which is forwaded to dataBlob and in turn, finally, pushed into inputBlob structure vector.</p>
<p><b>Function 8</b> This is the function which takes the inputBlob, invokes a caffe network and pushes the inputBlob into the caffe network for classification.</p>
<div class="fragment"><div class="line">Caffe::set_mode(Caffe::CPU);</div>
</div><!-- fragment --><p> This particular part sets mode as cpu mode, which was later changed by keeping it as a option. In later commits somewhere, it was modified in a way that when gpu is detected use gpu mode.</p>
<div class="fragment"><div class="line">Net&lt;float&gt;  net(networkFileLocation, TEST);</div>
<div class="line">net.CopyTrainedLayersFrom(weightModelFileLoaction); </div>
</div><!-- fragment --><p> The first line creates the network, and the nest loads the trained weights into it.</p>
<div class="fragment"><div class="line"><span class="keyword">const</span> vector&lt;Blob&lt;float&gt;*&gt;&amp; result =  net.Forward(inputBlob, &amp;type);</div>
</div><!-- fragment --><p> This particular line is what makes the network compute the result. A set of predictions for each class, in case of mnist 10 classes, is obtained. The one with highest probability is what the classifier states as the most propable class of the object in the image. This is obtained using the following section of code</p>
<div class="fragment"><div class="line"><span class="keywordtype">float</span> max = 0;</div>
<div class="line"><span class="keywordtype">float</span> max_i = 0;</div>
<div class="line"><span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; 10; ++i)</div>
<div class="line">{</div>
<div class="line">  <span class="keywordtype">float</span> value = result[0]-&gt;cpu_data()[i];</div>
<div class="line">  <span class="keywordflow">if</span> (max &lt; value)</div>
<div class="line">  {</div>
<div class="line">  max = value;</div>
<div class="line">  max_i = i;</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line">cout &lt;&lt; endl &lt;&lt; endl &lt;&lt; <span class="stringliteral">&quot;****** OUTPUT *******&quot;</span> &lt;&lt; endl;</div>
<div class="line">cout &lt;&lt; <span class="stringliteral">&quot;classified image is digit &quot;</span> &lt;&lt; max_i &lt;&lt; endl &lt;&lt; endl;</div>
</div><!-- fragment --><p>This ends the base file additions for this commit</p>
<h2><a class="anchor" id="commit2_second_change_3"></a>
2.2.3) Mnist Classification Example </h2>
<p>An example which takes use of the functions stated above in this commit is included, examples/objectdetector/od_cnn_mnist_classification.cpp. It takes data from folder examples/objectdetector/Mnist_Classify. This folder has a network file, a trained weight file, and 5 test images.</p>
<p>The cpp file has the following components,</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;detectors/global2D/detection/ODConvClassification.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;common/utils/ODFrameGenerator.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;common/pipeline/ObjectDetector.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;common/pipeline/ODDetection.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;fstream&gt;</span></div>
</div><!-- fragment --><p> These are the libraries included in the code, of which the first one is the most important one.</p>
<p>The rest of the code is explained in the comments(inside code) below </p>
<div class="fragment"><div class="line"><a class="code" href="classod_1_1g2d_1_1_o_d_conv_classification.html">od::g2d::ODConvClassification</a> *mnist_classifier = <span class="keyword">new</span> <a class="code" href="classod_1_1g2d_1_1_o_d_conv_classification.html">od::g2d::ODConvClassification</a>(<span class="stringliteral">&quot;&quot;</span>);  <span class="comment">//Create object of class ODConvClassification</span></div>
<div class="line">mnist_classifier-&gt;<a class="code" href="classod_1_1g2d_1_1_o_d_conv_classification.html#a4135a5a839cccae68509f9726d3fac6a">setWeightModelFileLocation</a>(argv[1]);            <span class="comment">//Set location for trained wieghts file</span></div>
<div class="line">mnist_classifier-&gt;<a class="code" href="classod_1_1g2d_1_1_o_d_conv_classification.html#a1e50c374c7908e1a6f657ae499f98871">setNetworkModelFileLocation</a>(argv[2]);           <span class="comment">//Set location for nework structure file </span></div>
<div class="line">mnist_classifier-&gt;<a class="code" href="classod_1_1g2d_1_1_o_d_conv_classification.html#a541bab88c9ae875d3f76c20ea6e67921">setImageFileLocation</a>(argv[3]);            <span class="comment">//Set location for test image file</span></div>
<div class="line">mnist_classifier-&gt;<a class="code" href="classod_1_1g2d_1_1_o_d_conv_classification.html#ae1745a0165917990da160d65582f800e">setTestBlob</a>(1,28,28);               <span class="comment">//Convert image to caffe required format and dimensions 28,28 and grayscale input</span></div>
<div class="line">mnist_classifier-&gt;<a class="code" href="classod_1_1g2d_1_1_o_d_conv_classification.html#aae8f7d0a25dbd8a084d24cf9a749ec7d">classify</a>();                 <span class="comment">//Invoke the classifier</span></div>
</div><!-- fragment --><p>This code has a help function, </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> help()</div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; endl &lt;&lt; <span class="stringliteral">&quot;Usage: ./examples/objectdetector/od_cnn_mnist_classification &lt;path to weight caffemodel file&gt; &lt;path to network file&gt; &lt;path to image file&gt;&quot;</span> &lt;&lt; endl;</div>
<div class="line">  cout &lt;&lt; endl &lt;&lt; <span class="stringliteral">&quot;Example: ./examples/objectdetector/od_cnn_mnist_classification ../examples/objectdetector/Mnist_Classify/mnist.caffemodel ../examples/objectdetector/Mnist_Classify/lenet.prototxt ../examples/objectdetector/Mnist_Classify/3.png&quot;</span> &lt;&lt; endl &lt;&lt; endl;</div>
<div class="line">  exit(0);</div>
<div class="line">}</div>
</div><!-- fragment --><p> This is for the cases when user puts in wrong command to invoke the classifier.</p>
<p>Example: Mentioned is classification of</p>
<div class="image">
<img src="3.png" alt="3.png"/>
</div>
<p>with the function </p>
<div class="fragment"><div class="line">./examples/objectdetector/od_cnn_mnist_classification ../examples/objectdetector/Mnist_Classify/mnist.caffemodel ../examples/objectdetector/Mnist_Classify/lenet.prototxt ../examples/objectdetector/Mnist_Classify/3.png</div>
</div><!-- fragment --><p>and the result is shown in the terminal in the image below as "****** OUTPUT *******
classified image is digit 4"</p>
<div class="image">
<img src="Example_For_Documentation.png" alt="Example_For_Documentation.png"/>
</div>
<p>Happy Coding!!!</p>
<h2><a class="anchor" id="commit2_second_change_4"></a>
2.2.4) Adding cnn based classifier: Features and benefits  </h2>
<p><b>1)Usually the caffe is invoked using a python wrapper, this is the first time any open source library will be having a c++ based classifier</b></p>
<p><b>2)It makes use of the caffe data convertor in c++ format, so that opendetection user may load image in either opencv Mat format or just point to image's location</b></p>
<p><b>3)User just needs to point a network file, trained weights and a test image, and the classification result can be obtained using the mentioned examples</b></p>
<h2><a class="anchor" id="commit2_third_change"></a>
2.3)Adding cnn based trainer, commit 2.3 </h2>
<p>Here, the goal was to add a cnn based trainer, based on caffe library, to the opendetection source. The <a href="https://github.com/abhi-kumar/opendetection/commit/1adb044f5bc10b3a29d190273d86527e0e4d1f53">fragmented commit: Commit for Mnist Training Example Using Caffe Library</a>, involved additions of 3 code source files and 42000 training images (Mnist Dataset), a link file to these images with labels, a training etwork file and a solver file.</p>
<h2><a class="anchor" id="commit2_third_change_1"></a>
2.3.1)Training Base Class </h2>
<p>At this stage, two files were introduced, detectors/global2D/training/ODConvTrainer.cpp and <a class="el" href="_o_d_conv_trainer_8h_source.html">detectors/global2D/training/ODConvTrainer.h</a></p>
<p>In the header file,</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;common/pipeline/ODTrainer.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;common/utils/utils.h&quot;</span></div>
</div><!-- fragment --><p> these headers were used to make sure that the training class ODConvTrainer is derived from public elements of ODTrainer.</p>
<p>Opencv library </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;opencv2/opencv.hpp&gt;</span></div>
</div><!-- fragment --><p> is added to make sure that image loading and saving is done using the Mat constructor</p>
<p>A set of general c++ header are also added </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;cstring&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;cstdlib&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;vector&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;string&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;stdio.h&gt;</span></div>
</div><!-- fragment --><p>Three header files from caffe library are added in order to make sure that caffe library can be succesfully included. </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;caffe/caffe.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/util/io.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/blob.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/solver.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/sgd_solvers.hpp&quot;</span></div>
</div><!-- fragment --><p> Tha last two files have been used to invoke the solver properties and run the trainer from caffe library.</p>
<p>At this stage the class had only one variable </p>
<div class="fragment"><div class="line"><span class="keywordtype">string</span> solverLocation;</div>
</div><!-- fragment --><p> which stores the location pointing towards the solver file.</p>
<p><b>Function1</b></p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvTrainer::setSolverLocation(<span class="keywordtype">string</span> location)</div>
<div class="line">{</div>
<div class="line">  ODConvTrainer::solverLocation = location; </div>
<div class="line">}</div>
</div><!-- fragment --><p> Sets the variable solverLocation.</p>
<p><b>Function2</b> </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvTrainer::startTraining()</div>
<div class="line">{</div>
<div class="line">  Caffe::set_mode(Caffe::CPU);</div>
<div class="line">  SGDSolver&lt;float&gt; s(solverLocation);</div>
<div class="line">  s.Solve();</div>
<div class="line">}</div>
</div><!-- fragment --><p> this function is the sole of this simple trainer. The first line sets the mode, in later commits mode is decided on the basis of the system specifications and user commands. The next line creates a caffe solver, <a href="http://caffe.berkeleyvision.org/tutorial/solver.html">link to understand solver</a> and the last line starts the training.</p>
<h2><a class="anchor" id="commit2_third_change_2"></a>
2.3.2)Mnist Training Example</h2>
<p>The file added for this is: examples/objectdetector/od_cnn_train_mnist_simple.cpp</p>
<div class="fragment"><div class="line"><a class="code" href="classod_1_1g2d_1_1_o_d_conv_trainer.html">od::g2d::ODConvTrainer</a> *mnist_trainer = <span class="keyword">new</span> <a class="code" href="classod_1_1g2d_1_1_o_d_conv_trainer.html">od::g2d::ODConvTrainer</a>(<span class="stringliteral">&quot;&quot;</span>,<span class="stringliteral">&quot;&quot;</span>);</div>
<div class="line">mnist_trainer-&gt;<a class="code" href="classod_1_1g2d_1_1_o_d_conv_trainer.html#a3c46e781fb546a23279ab29fbd23a77e">setSolverLocation</a>(argv[1]);</div>
<div class="line">mnist_trainer-&gt;<a class="code" href="classod_1_1g2d_1_1_o_d_conv_trainer.html#af1f228bf3c1e80be799768f85fb95ad1">startTraining</a>();</div>
</div><!-- fragment --><p> The above code is self explanatory. The trained weights are saved into a location as stated in the solver file.</p>
<h2><a class="anchor" id="commit2_fourth_change"></a>
2.4)Introducing a custom solver, commit 2.4 </h2>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/6777c6cdc39289d4abe443d4cc564fb25135148b">fragmented commit: Commit for Mnist Training Example Using Caffe Library With Customized Solver</a>, to enable the user to create a solver file, as required in caffe library format, so that user may make the file using a graphical user interface rather than writing the entire text.</p>
<p>This particular commit had changes added or additions made to 9 files. The most crucial one was to add gtkmm library files to the source. GTKMM, <a href="http://www.gtkmm.org/en/">link to understand gtkmm</a>, is a library for involvement of gui based applications. We decided to move with GUI inclusion because, to make user handle solver file in an effective way, a set of 19 parameters had to be handled. If it were upto the c++ arguments to facilitate these 19 parameters, the outcome would have been a very cumbersome application. Also, not all parameters were to be added to the solver always, so a GUI appeared to be the most feasible option from the user's end.</p>
<p>To install gtkmm use the following command, </p>
<div class="fragment"><div class="line">sudo apt-<span class="keyword">get</span> install libglib2.0-dev libatk1.0* libpango1.0-dev libcairo2-dev gdk-pixbuf2.0-0 libsigc++-2.0-dev libgtk-3-dev libcairomm-1.0-dev libpangomm-1.4-dev libatkmm-1.6-dev libgtkmm-3.0-dev</div>
</div><!-- fragment --><h2><a class="anchor" id="commit2_fourth_change_1"></a>
2.4.1)How the GTKMM library was introduced </h2>
<p>This involved making changes to the cmake/od_mandatory_dependency.cmake file.</p>
<p>This, </p>
<div class="fragment"><div class="line">set(GTKMM_INCLUDE_DIRS_3 -pthread /usr/include/gtkmm-3.0 /usr/lib/x86_64-linux-gnu/gtkmm-3.0/include /usr/include/atkmm-1.6 /usr/include/giomm-2.4 /usr/lib/x86_64-linux-gnu/giomm-2.4/include /usr/include/pangomm-1.4 /usr/lib/x86_64-linux-gnu/pangomm-1.4/include /usr/include/gtk-3.0 /usr/include/cairomm-1.0 /usr/lib/x86_64-linux-gnu/cairomm-1.0/include /usr/include/gdk-pixbuf-2.0 /usr/include/gtk-3.0/unix-print /usr/include/gdkmm-3.0 /usr/lib/x86_64-linux-gnu/gdkmm-3.0/include /usr/include/atk-1.0 /usr/include/glibmm-2.4 /usr/lib/x86_64-linux-gnu/glibmm-2.4/include /usr/include/glib-2.0 /usr/lib/x86_64-linux-gnu/glib-2.0/include /usr/include/sigc++-2.0 /usr/lib/x86_64-linux-gnu/sigc++-2.0/include /usr/include/pango-1.0 /usr/include/cairo /usr/include/pixman-1 /usr/include/freetype2 /usr/include/libpng12 /usr/include/at-spi2-atk/2.0 /usr/include/gio-unix-2.0/ /usr/include/harfbuzz)</div>
</div><!-- fragment --><p> created a link to the header files involved with GTKMM</p>
<p>and, </p>
<div class="fragment"><div class="line">set(GTKMM_LIBRARIES_3 gtkmm-3.0 atkmm-1.6 gdkmm-3.0 giomm-2.4 pangomm-1.4 gtk-3 glibmm-2.4 cairomm-1.0 gdk-3 atk-1.0 gio-2.0 pangocairo-1.0 gdk_pixbuf-2.0 cairo-gobject pango-1.0 cairo sigc-2.0 gobject-2.0 glib-2.0)</div>
</div><!-- fragment --><p> created a link to the .so library files required for proper compilation. These two variables where used whenever reuired in other cmakelists files.</p>
<h2><a class="anchor" id="commit2_fourth_change_2"></a>
2.4.2) Custom Solver base class and usage details for every parameter/entry in the gui</h2>
<p>The base class named, <a class="el" href="class_solver_properties.html">SolverProperties</a>, was introduced using two files, detectors/global2D/training/solver.cpp and <a class="el" href="solver_8h_source.html">detectors/global2D/training/solver.h</a>.</p>
<p>The header file takes in the following headers from gtkmm library </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;gtkmm/grid.h&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;gtkmm/entry.h&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;gtkmm/button.h&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;gtkmm/radiobutton.h&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;gtkmm/messagedialog.h&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;gtkmm/window.h&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;gtkmm/scrolledwindow.h&gt;</span></div>
</div><!-- fragment --><ul>
<li>grid.h to make sure the elements, eg., buttons, display windows, text boxes etc, lie on a particular place even with differential resolutions of user's systems</li>
<li>entry.h for textboxes</li>
<li>button.h for buttons</li>
<li>radiobutton.h for radio type buttons</li>
<li>messagedialog.h for invoking warning or user oriented messages on completion of events</li>
<li>window.h as the base class to invoke a window based application</li>
<li>scrollwindow.h to make sure that the window created can be scrolled either ways.</li>
</ul>
<p>The gui has a total of</p>
<ul>
<li>23 buttons</li>
<li>19 text box entries</li>
<li>17 labels</li>
<li>28 radio buttons grouped into 9 groups</li>
<li>21 string (here ustring) variables</li>
</ul>
<p><b>Initializing the elements, i.e., the buttons, text boxes, labels, and radio buttons</b></p>
<p>In the cpp file, these are initialized accroding to the following rules</p>
<ul>
<li>Like any object initalization, these are done just after the mentioning the constructor and before writing the matter of the constructor.</li>
<li>for button, nameOfButton("Text to print on the button")</li>
<li>for text boxes, nameOfTextBox()</li>
<li>for labels, nameOfLabel("")</li>
<li>for radioButton, nameOfButton("Text to print on the button") and it is done as shown below <div class="fragment"><div class="line"><a class="code" href="class_solver_properties.html#a97c503ecaf6c3d20d2ec509aa2c7b2ad">SolverProperties::SolverProperties</a>(): </div>
<div class="line">  label_solverFileName(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_solverFileName(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_solverFileName(),</div>
<div class="line">  label_trainNetworkFileType(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  rbutton_trainNetworkFileType_net(<span class="stringliteral">&quot;net&quot;</span>), rbutton_trainNetworkFileType_tt(<span class="stringliteral">&quot;train_net&quot;</span>),</div>
<div class="line">  label_trainNetworkFileName(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_trainNetworkFileName(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_trainNetworkFileName(),</div>
<div class="line">  label_enableTestNet(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  rbutton_enableTestNet_no(<span class="stringliteral">&quot;No&quot;</span>), rbutton_enableTestNet_yes(<span class="stringliteral">&quot;Yes&quot;</span>),</div>
<div class="line">  label_testNetworkFileName(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_testNetworkFileName(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_testNetworkFileName(),</div>
<div class="line">  label_enableValidationParameters(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  rbutton_enableValidationParameters_no(<span class="stringliteral">&quot;No&quot;</span>), rbutton_enableValidationParameters_yes(<span class="stringliteral">&quot;Yes&quot;</span>),</div>
<div class="line">  label_testIter(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_testIter(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_testIter(),</div>
<div class="line">  label_testInterval(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_testInterval(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_testInterval(),</div>
<div class="line">  label_enableAverageLoss(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  rbutton_enableAverageLoss_no(<span class="stringliteral">&quot;No&quot;</span>), rbutton_enableAverageLoss_yes(<span class="stringliteral">&quot;Yes&quot;</span>),</div>
<div class="line">  label_averageLoss(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_averageLoss(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_averageLoss(),</div>
<div class="line">  label_enableRandomSample(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  rbutton_enableRandomSample_no(<span class="stringliteral">&quot;No&quot;</span>), rbutton_enableRandomSample_yes(<span class="stringliteral">&quot;Yes&quot;</span>),</div>
<div class="line">  label_randomSample(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_randomSample(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_randomSample(),</div>
<div class="line">  label_display(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_display(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_display(),</div>
<div class="line">  label_enableDebugInfo(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  rbutton_enableDebugInfo_no(<span class="stringliteral">&quot;No&quot;</span>), rbutton_enableDebugInfo_yes(<span class="stringliteral">&quot;Yes&quot;</span>),</div>
<div class="line">  button_debugInfo(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  label_snapshot(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_snapshot(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_snapshot(),</div>
<div class="line">  label_enableTestComputeLoss(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  rbutton_enableTestComputeLoss_no(<span class="stringliteral">&quot;No&quot;</span>), rbutton_enableTestComputeLoss_yes(<span class="stringliteral">&quot;Yes&quot;</span>),</div>
<div class="line">  button_testComputeLoss(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  label_snapshotPrefix(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_snapshotPrefix(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_snapshotPrefix(),</div>
<div class="line">  label_maxIter(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_maxIter(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_maxIter(),</div>
<div class="line">  label_type(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_type(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  rbutton_typeSGD_yes(<span class="stringliteral">&quot;SGD&quot;</span>), rbutton_typeAdadelta_yes(<span class="stringliteral">&quot;AdaDelta&quot;</span>), rbutton_typeAdagrad_yes(<span class="stringliteral">&quot;AdaGrad&quot;</span>), rbutton_typeAdam_yes(<span class="stringliteral">&quot;Adam&quot;</span>),</div>
<div class="line">  rbutton_typeRMSProp_yes(<span class="stringliteral">&quot;RMSProp&quot;</span>), rbutton_typeNesterov_yes(<span class="stringliteral">&quot;Nesterov&quot;</span>),</div>
<div class="line">  label_learningRatePolicy(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_learningRatePolicy(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  rbutton_learningRatePolicyFixed_yes(<span class="stringliteral">&quot;fixed&quot;</span>), rbutton_learningRatePolicyExp_yes(<span class="stringliteral">&quot;exp&quot;</span>), rbutton_learningRatePolicyStep_yes(<span class="stringliteral">&quot;step&quot;</span>),</div>
<div class="line">  rbutton_learningRatePolicyInv_yes(<span class="stringliteral">&quot;inv&quot;</span>), rbutton_learningRatePolicyMultistep_yes(<span class="stringliteral">&quot;multistep&quot;</span>), </div>
<div class="line">  rbutton_learningRatePolicyPoly_yes(<span class="stringliteral">&quot;poly&quot;</span>), rbutton_learningRatePolicySigmoid_yes(<span class="stringliteral">&quot;sigmoid&quot;</span>),</div>
<div class="line">  label_baseLearningRate(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_baseLearningRate(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_baseLearningRate(),</div>
<div class="line">  label_gamma(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_gamma(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_gamma(),</div>
<div class="line">  label_power(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_power(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_power(),</div>
<div class="line">  label_stepSize(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_stepSize(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_stepSize(),</div>
<div class="line">  label_stepSizeValue(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_stepSizeValue(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_stepSizeValue(),</div>
<div class="line">  label_weightDecay(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_weightDecay(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_weightDecay(),</div>
<div class="line">  label_momentum(<span class="stringliteral">&quot;&quot;</span>),</div>
<div class="line">  button_momentum(<span class="stringliteral">&quot;Update&quot;</span>),</div>
<div class="line">  text_momentum(),</div>
<div class="line">  button_saveFile(<span class="stringliteral">&quot;Save File&quot;</span>)</div>
<div class="line">{</div>
<div class="line">  <span class="comment">//Constructor code starts from here</span></div>
</div><!-- fragment --></li>
</ul>
<p><b>Invoking the Grid</b></p>
<p>The grids for putting the elemets was invoked using the following code snippet </p>
<div class="fragment"><div class="line">set_title(<span class="stringliteral">&quot;Solver&quot;</span>);</div>
<div class="line">set_border_width(10);</div>
<div class="line">add(m_sw1);</div>
<div class="line">m_grid1.set_column_spacing (10);</div>
<div class="line">m_grid1.set_row_spacing (50);</div>
</div><!-- fragment --><ul>
<li>The fist line sets the title for the window</li>
<li>Border width is added in second line to make sure that no element is hidden over boundaries</li>
<li>Line 4 and 5 set the spacing between the elements</li>
<li>line 3 adds m_sw1, this m_sw1 is nothing but a scrollwindow</li>
</ul>
<p>This scroll window is added to the window. This scroll window takes the grid, m_grid1 into the scrollwindow at the end of the constructor as </p>
<div class="fragment"><div class="line">m_sw1.add(m_grid1);</div>
<div class="line">m_sw1.set_policy(Gtk::POLICY_AUTOMATIC, Gtk::POLICY_AUTOMATIC);</div>
<div class="line"><span class="comment">//  m_grid1.show();</span></div>
<div class="line">show_all_children();</div>
<div class="line">m_sw1.show();</div>
</div><!-- fragment --><p> where, all the children of the grid are shown with the end line causing the scrolls to show up.</p>
<p><b>Example Entry1: Name and location of the file to be created</b></p>
<p>This entry asks user to give a proper name, with relative location( from build folder), of the solver file to be saved. This entry looks like the one mentioned in the red colored box in the image below:</p>
<div class="image">
<img src="solver_1.png" alt="solver_1.png"/>
</div>
<p>This involves a label </p>
<div class="fragment"><div class="line">label_solverFileName.set_text(<span class="stringliteral">&quot;1) Give a proper name to the solver file: &quot;</span>);  <span class="comment">//set label text</span></div>
<div class="line">label_solverFileName.set_line_wrap();           </div>
<div class="line">label_solverFileName.set_justify(Gtk::JUSTIFY_FILL);</div>
<div class="line">m_grid1.attach(label_solverFileName,0,0,2,1);         <span class="comment">//set position of the element into the grid</span></div>
<div class="line">label_solverFileName.show();</div>
</div><!-- fragment --><p>followed by a text entry, this entry has an initial text "../examples/objectdetector/Mnist_Train/solverCustom1.prototxt". The folder Mnist_Train is already created in the source, hence, as a sample a pointer to the location, the text there points to the folder. solverCustom1.prototxt is the name of the file that will be created. </p>
<div class="fragment"><div class="line">text_solverFileName.set_max_length(100);              <span class="comment">//set max length of the text</span></div>
<div class="line">text_solverFileName.set_text(<span class="stringliteral">&quot;../examples/objectdetector/Mnist_Train/solverCustom1.prototxt&quot;</span>);  <span class="comment">//set initial text</span></div>
<div class="line">text_solverFileName.select_region(0, text_solverFileName.get_text_length());    </div>
<div class="line">m_grid1.attach(text_solverFileName,2,0,5,1);              <span class="comment">//set position of the element into the grid</span></div>
<div class="line">text_solverFileName.show();</div>
</div><!-- fragment --><p>This is later followed by a button, to update the output file location and name. Once the location and name are updated in the text box, the button needs to be pressed. After pressing the button a messagebox appears stating the outcome. How the button works is stated in the section 2.4.3. </p>
<div class="fragment"><div class="line">button_solverFileName.signal_clicked().connect(sigc::bind&lt;Glib::ustring&gt;(</div>
<div class="line">             sigc::mem_fun(*<span class="keyword">this</span>, &amp;<a class="code" href="class_solver_properties.html#a9375f30ecdbabc55ce73b8faa18f3152">SolverProperties::on_button_clicked</a>), <span class="stringliteral">&quot;solverFileName&quot;</span>));  <span class="comment">//link the button to a function with a key, here the key is &quot;solverFileName&quot;</span></div>
<div class="line">m_grid1.attach(button_solverFileName,7,0,1,1);</div>
<div class="line">button_solverFileName.show();</div>
</div><!-- fragment --><p><b>Entry2: Type of network link</b></p>
<p>There exsists two types of links</p>
<ul>
<li>net: this is to be selected when training and validation details are put into a same prototxt file</li>
<li>train_net: this is to be selected when the training parameters and validation parameters are put into two different prototxt files</li>
</ul>
<p>It can be seen in the red rectangular section box in the image below</p>
<div class="image">
<img src="solver_2.png" alt="solver_2.png"/>
</div>
<p>This entry has two elements, first is the label, </p>
<div class="fragment"><div class="line">label_trainNetworkFileType.set_text(<span class="stringliteral">&quot;2)Select type of training network file type.\nUsually trese exists two types,\nfirst adds validation and training in the same file,\nWhile other adds them in two different files&quot;</span>);</div>
<div class="line">label_trainNetworkFileType.set_line_wrap();</div>
<div class="line">label_trainNetworkFileType.set_justify(Gtk::JUSTIFY_FILL);</div>
<div class="line">m_grid1.attach(label_trainNetworkFileType,0,1,2,1);</div>
<div class="line">label_trainNetworkFileType.show();</div>
</div><!-- fragment --><p>and a set of two radio buttons to select the two types of links </p>
<div class="fragment"><div class="line">Gtk::RadioButton::Group group = rbutton_trainNetworkFileType_net.get_group();   <span class="comment">//set a group</span></div>
<div class="line">rbutton_trainNetworkFileType_tt.set_group(group);         <span class="comment">// add other buttons to this group</span></div>
<div class="line">rbutton_trainNetworkFileType_net.set_active();            <span class="comment">//set one of them as active</span></div>
<div class="line">m_grid1.attach(rbutton_trainNetworkFileType_net,2,1,1,1);</div>
<div class="line">rbutton_trainNetworkFileType_net.show();</div>
<div class="line">m_grid1.attach(rbutton_trainNetworkFileType_tt,3,1,1,1);</div>
<div class="line">rbutton_trainNetworkFileType_tt.show();</div>
</div><!-- fragment --><p><br/>
 <b>Example Entry 2.1: Add the location of the network file</b></p>
<p>This entry asks user to give a proper name, with relative location( from build folder), of the network/training cnn architecture file on which training has to be done. This entry looks like the one mentioned in the red colored box in the image below:</p>
<div class="image">
<img src="solver_3.png" alt="solver_3.png"/>
</div>
<p>It has three fields</p>
<p>a label, </p>
<div class="fragment"><div class="line">label_trainNetworkFileName.set_text(<span class="stringliteral">&quot;2.1) net: or train_net:\n(Parameter Details: Give location of \nthe net file or the train_net file) &quot;</span>);</div>
<div class="line">label_trainNetworkFileName.set_line_wrap();</div>
<div class="line">label_trainNetworkFileName.set_justify(Gtk::JUSTIFY_FILL);</div>
<div class="line">m_grid1.attach(label_trainNetworkFileName,0,2,2,1);</div>
<div class="line">label_trainNetworkFileName.show();</div>
</div><!-- fragment --><p>a text entry element, </p>
<div class="fragment"><div class="line">text_trainNetworkFileName.set_max_length(500);</div>
<div class="line">text_trainNetworkFileName.set_text(<span class="stringliteral">&quot;../examples/objectdetector/Mnist_Train/train1.prototxt&quot;</span>);</div>
<div class="line">text_trainNetworkFileName.select_region(0, text_solverFileName.get_text_length());</div>
<div class="line">m_grid1.attach(text_trainNetworkFileName,2,2,5,1);  </div>
<div class="line">text_trainNetworkFileName.show(); </div>
</div><!-- fragment --><p>and an update button to update the text added/removed/changed in the the text box next to it. After pressing the button a messagebox appears stating the outcome. How the button works is stated in the section 2.4.3. </p>
<div class="fragment"><div class="line">button_trainNetworkFileName.signal_clicked().connect(sigc::bind&lt;Glib::ustring&gt;(</div>
<div class="line">              sigc::mem_fun(*<span class="keyword">this</span>, &amp;<a class="code" href="class_solver_properties.html#a9375f30ecdbabc55ce73b8faa18f3152">SolverProperties::on_button_clicked</a>), <span class="stringliteral">&quot;trainNetworkFileName&quot;</span>)); <span class="comment">////link the button to a function with a key, here the key is &quot;trainNetworkFileName&quot;</span></div>
<div class="line"><span class="comment"></span>m_grid1.attach(button_trainNetworkFileName,7,2,1,1);</div>
<div class="line">button_trainNetworkFileName.show();</div>
</div><!-- fragment --><p><br/>
 <b>Example Entry 3: Selecting to whether add a test net file separately, and if yes pointing location of the file</b></p>
<p>This entry is for the purpose when, a separate test/validation net is to be added. It is observed in the solver gui as in the red box stated in the image below</p>
<div class="image">
<img src="solver_4.png" alt="solver_4.png"/>
</div>
<p>It has five elements, two labels, one text file, one radio button group and a final button to update the location</p>
<div class="fragment"><div class="line">label_enableTestNet.set_text(<span class="stringliteral">&quot;3) Enable Test Network Parameter:\n(Enable only with using \&quot;train_net\&quot; parameter.)&quot;</span>);</div>
<div class="line">label_enableTestNet.set_line_wrap();</div>
<div class="line">label_enableTestNet.set_justify(Gtk::JUSTIFY_FILL);</div>
<div class="line">m_grid1.attach(label_enableTestNet,0,3,2,1);</div>
<div class="line">label_enableTestNet.show();</div>
<div class="line"></div>
<div class="line">Gtk::RadioButton::Group group2 = rbutton_enableTestNet_no.get_group();</div>
<div class="line">rbutton_enableTestNet_yes.set_group(group2);</div>
<div class="line">rbutton_enableTestNet_no.set_active();</div>
<div class="line">m_grid1.attach(rbutton_enableTestNet_no,2,3,1,1);</div>
<div class="line">rbutton_enableTestNet_no.show();</div>
<div class="line">m_grid1.attach(rbutton_enableTestNet_yes,3,3,1,1);</div>
<div class="line">rbutton_enableTestNet_yes.show();</div>
<div class="line"></div>
<div class="line">label_testNetworkFileName.set_text(<span class="stringliteral">&quot;3.1) test_net:&quot;</span>);</div>
<div class="line">label_testNetworkFileName.set_line_wrap();</div>
<div class="line">label_testNetworkFileName.set_justify(Gtk::JUSTIFY_FILL);</div>
<div class="line">m_grid1.attach(label_testNetworkFileName,4,3,1,1);</div>
<div class="line">label_testNetworkFileName.show();</div>
<div class="line">  </div>
<div class="line">text_testNetworkFileName.set_max_length(500);</div>
<div class="line">text_testNetworkFileName.set_text(<span class="stringliteral">&quot;../examples/objectdetector/Mnist_Train/test1.prototxt&quot;</span>);</div>
<div class="line">text_testNetworkFileName.select_region(0, text_testNetworkFileName.get_text_length());</div>
<div class="line">m_grid1.attach(text_testNetworkFileName,5,3,3,1); </div>
<div class="line">text_testNetworkFileName.show();  </div>
<div class="line"></div>
<div class="line">button_testNetworkFileName.signal_clicked().connect(sigc::bind&lt;Glib::ustring&gt;(</div>
<div class="line">              sigc::mem_fun(*<span class="keyword">this</span>, &amp;<a class="code" href="class_solver_properties.html#a9375f30ecdbabc55ce73b8faa18f3152">SolverProperties::on_button_clicked</a>), <span class="stringliteral">&quot;testNetworkFileName&quot;</span>));</div>
<div class="line">m_grid1.attach(button_testNetworkFileName,8,3,1,1);</div>
<div class="line">button_testNetworkFileName.show();</div>
</div><!-- fragment --><p><br/>
</p>
<p><b>Thus the rest of the entries are planned and coded. Whenever a parameter is set to be enabled as "no", its presense is commented in the output text file</b></p>
<p><br/>
 </p>
<h2><a class="anchor" id="commit2_fourth_change_3"></a>
2.4.3)Pressing the buttons in the solver creator</h2>
<p><b>Example when a file name or parameter has to be saved</b></p>
<p>When "Update", next to filename textbox, is pressed the following happens,</p>
<div class="fragment"><div class="line">solverFileName = text_solverFileName.get_text();</div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Solver File Name set as: &quot;</span> &lt;&lt; solverFileName &lt;&lt; std::endl;</div>
<div class="line">Gtk::MessageDialog dialog(*<span class="keyword">this</span>, <span class="stringliteral">&quot;FileName Updated&quot;</span>);</div>
<div class="line">    dialog.set_secondary_text(<span class="stringliteral">&quot;New name and location: &quot;</span> + solverFileName);</div>
<div class="line">dialog.run();</div>
</div><!-- fragment --><p>The first line, saves the name into a string variable and the next lines prompt the user about the update</p>
<p><br/>
 <b>Example when the entire file is saved</b></p>
<p>When "Save File" button is pressed, a string is created in the format required by caffe solver file. All elements which are enabled are uncommented and the rest are set as commented. This string is pushed into a text file as mentioned by the user.</p>
<p>The code which does this is shown below,</p>
<div class="fragment"><div class="line"><span class="keywordflow">if</span>(rbutton_enableTestNet_yes.get_active() == 1 and rbutton_trainNetworkFileType_net.get_active() == 1)</div>
<div class="line">{</div>
<div class="line">  Gtk::MessageDialog dialog(*<span class="keyword">this</span>, <span class="stringliteral">&quot;\&quot;test_net\&quot; parameter not required&quot;</span>);</div>
<div class="line">  dialog.set_secondary_text(<span class="stringliteral">&quot;\&quot;test_net\&quot; parameter is only required when \&quot;train_net\&quot; parameter is specified&quot;</span>);</div>
<div class="line">  dialog.run();</div>
<div class="line">}</div>
<div class="line"><span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_enableTestNet_yes.get_active() == 1 and rbutton_enableValidationParameters_no.get_active() == 1)</div>
<div class="line">{</div>
<div class="line">  Gtk::MessageDialog dialog(*<span class="keyword">this</span>, <span class="stringliteral">&quot;Validation parameters required&quot;</span>);</div>
<div class="line">  dialog.set_secondary_text(<span class="stringliteral">&quot;Validation parameters are required when \&quot;test_net\&quot; parameter is specified.&quot;</span>);</div>
<div class="line">  dialog.run();</div>
<div class="line">}</div>
<div class="line"><span class="keywordflow">else</span></div>
<div class="line">{</div>
<div class="line">  solverFileName = text_solverFileName.get_text();</div>
<div class="line">  trainNetworkFileName = text_trainNetworkFileName.get_text();</div>
<div class="line">  testNetworkFileName = text_testNetworkFileName.get_text();</div>
<div class="line">  testIter = text_testIter.get_text();</div>
<div class="line">  testInterval = text_testInterval.get_text();</div>
<div class="line">  averageLoss = text_averageLoss.get_text();</div>
<div class="line">  randomSample = text_randomSample.get_text();</div>
<div class="line">  display = text_display.get_text();</div>
<div class="line">  snapshot = text_snapshot.get_text();</div>
<div class="line">  snapshotPrefix = text_snapshotPrefix.get_text();</div>
<div class="line">  maxIter = text_maxIter.get_text();</div>
<div class="line">  baseLearningRate = text_baseLearningRate.get_text();</div>
<div class="line">  gamma = text_gamma.get_text();</div>
<div class="line">  power = text_power.get_text();</div>
<div class="line">  stepSize = text_stepSize.get_text();</div>
<div class="line">  stepSizeValue = text_stepSizeValue.get_text();</div>
<div class="line">  weightDecay = text_weightDecay.get_text();</div>
<div class="line">  momentum = text_momentum.get_text();</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">if</span>(rbutton_enableDebugInfo_yes.get_active())</div>
<div class="line">    debugInfo = <span class="stringliteral">&quot;1&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_enableDebugInfo_no.get_active())</div>
<div class="line">    debugInfo = <span class="stringliteral">&quot;0&quot;</span>;</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">if</span>(rbutton_enableTestComputeLoss_yes.get_active())</div>
<div class="line">    testComputeLoss = <span class="stringliteral">&quot;1&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_enableTestComputeLoss_no.get_active())</div>
<div class="line">    testComputeLoss = <span class="stringliteral">&quot;0&quot;</span>;</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">if</span>(rbutton_typeSGD_yes.get_active())</div>
<div class="line">    type = <span class="stringliteral">&quot;1&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_typeAdadelta_yes.get_active())</div>
<div class="line">    type = <span class="stringliteral">&quot;AdaDelta&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_typeAdagrad_yes.get_active())</div>
<div class="line">    type = <span class="stringliteral">&quot;AdaGrad&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_typeAdam_yes.get_active())</div>
<div class="line">    type = <span class="stringliteral">&quot;Adam&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_typeRMSProp_yes.get_active())</div>
<div class="line">    type = <span class="stringliteral">&quot;RMSProp&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_typeNesterov_yes.get_active())</div>
<div class="line">    type = <span class="stringliteral">&quot;Nesterov&quot;</span>;</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">if</span>(rbutton_learningRatePolicyFixed_yes.get_active())</div>
<div class="line">    learningRatePolicy = <span class="stringliteral">&quot;fixed&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_learningRatePolicyExp_yes.get_active())</div>
<div class="line">    learningRatePolicy = <span class="stringliteral">&quot;exp&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_learningRatePolicyStep_yes.get_active())</div>
<div class="line">    learningRatePolicy = <span class="stringliteral">&quot;step&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_learningRatePolicyInv_yes.get_active())</div>
<div class="line">    learningRatePolicy = <span class="stringliteral">&quot;inv&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_learningRatePolicyMultistep_yes.get_active())</div>
<div class="line">    learningRatePolicy = <span class="stringliteral">&quot;multistep&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_learningRatePolicyPoly_yes.get_active())</div>
<div class="line">    learningRatePolicy = <span class="stringliteral">&quot;poly&quot;</span>;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_learningRatePolicySigmoid_yes.get_active())</div>
<div class="line">    learningRatePolicy = <span class="stringliteral">&quot;sigmoid&quot;</span>;</div>
<div class="line"></div>
<div class="line">  std::ofstream myfile;</div>
<div class="line">  myfile.open(solverFileName);</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;#File generated using OpenDetection&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">  myfile.close();</div>
<div class="line">  myfile.open(solverFileName);</div>
<div class="line">  <span class="keywordflow">if</span>(!myfile)</div>
<div class="line">  {</div>
<div class="line">    Gtk::MessageDialog dialog(*<span class="keyword">this</span>, <span class="stringliteral">&quot;File Could not be Created&quot;</span>);</div>
<div class="line">    dialog.set_secondary_text(<span class="stringliteral">&quot;Make sure the destination exists or the file is writable&quot;</span>);</div>
<div class="line">    dialog.run();</div>
<div class="line">  }</div>
<div class="line">  std::cout &lt;&lt; <span class="stringliteral">&quot;Solver File Name saved as: &quot;</span> &lt;&lt; solverFileName &lt;&lt; std::endl;</div>
<div class="line">  </div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">if</span>(rbutton_trainNetworkFileType_net.get_active() == 1)</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;net: &quot;</span> &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; trainNetworkFileName &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_trainNetworkFileType_tt.get_active() == 1)</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;train_net: &quot;</span> &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; trainNetworkFileName &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; std::endl;</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">if</span>(rbutton_enableTestNet_yes.get_active() == 1)</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;test_net: &quot;</span> &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; testNetworkFileName &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_enableTestNet_no.get_active() == 1)</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;#test_net: &quot;</span> &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; testNetworkFileName &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; std::endl;</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">if</span>(rbutton_enableValidationParameters_yes.get_active() == 1)</div>
<div class="line">  {</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;test_iter: &quot;</span> &lt;&lt; testIter &lt;&lt; std::endl;</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;test_interval: &quot;</span> &lt;&lt; testInterval &lt;&lt; std::endl;</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_enableValidationParameters_no.get_active() == 1)</div>
<div class="line">  {</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;#test_iter: &quot;</span> &lt;&lt; testIter &lt;&lt; std::endl;</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;#test_interval: &quot;</span> &lt;&lt; testInterval &lt;&lt; std::endl;</div>
<div class="line">  }</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">if</span>(rbutton_enableAverageLoss_yes.get_active() == 1)</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;average_loss: &quot;</span> &lt;&lt; averageLoss &lt;&lt; std::endl;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_enableAverageLoss_no.get_active() == 1)</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;#average_loss: &quot;</span> &lt;&lt; averageLoss &lt;&lt; std::endl;</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">if</span>(rbutton_enableRandomSample_yes.get_active() == 1)</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;random_seed: &quot;</span> &lt;&lt; randomSample &lt;&lt; std::endl;</div>
<div class="line">  <span class="keywordflow">else</span> <span class="keywordflow">if</span>(rbutton_enableRandomSample_no.get_active() == 1)</div>
<div class="line">    myfile &lt;&lt; <span class="stringliteral">&quot;#random_seed: &quot;</span> &lt;&lt; randomSample &lt;&lt; std::endl;</div>
<div class="line"></div>
<div class="line">  </div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;display: &quot;</span> &lt;&lt; display &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;debug_info: &quot;</span> &lt;&lt; debugInfo &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;snapshot: &quot;</span> &lt;&lt; snapshot &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;test_compute_loss: &quot;</span> &lt;&lt; testComputeLoss &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;snapshot_prefix: &quot;</span> &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; snapshotPrefix &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;max_iter: &quot;</span> &lt;&lt; maxIter &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;type: &quot;</span> &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; type &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">  </div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;lr_policy: &quot;</span> &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; learningRatePolicy &lt;&lt; <span class="stringliteral">&quot;\&quot;&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;base_lr: &quot;</span> &lt;&lt; baseLearningRate &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;gamma: &quot;</span> &lt;&lt; gamma &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;power: &quot;</span> &lt;&lt; power &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;stepsize: &quot;</span> &lt;&lt; stepSize &lt;&lt; std::endl;  </div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;stepvalue: &quot;</span> &lt;&lt; stepSizeValue &lt;&lt; std::endl;</div>
<div class="line">  </div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;weight_decay: &quot;</span> &lt;&lt; weightDecay &lt;&lt; std::endl;</div>
<div class="line">  myfile &lt;&lt; <span class="stringliteral">&quot;momentum: &quot;</span> &lt;&lt; momentum &lt;&lt; std::endl;</div>
<div class="line"></div>
<div class="line"></div>
<div class="line">  myfile.close();</div>
<div class="line">}</div>
</div><!-- fragment --><h2><a class="anchor" id="commit2_fourth_change_4"></a>
2.4.4) Calling the solverProperties class from ODConvTrainer and the corresponding mnist curtom solver example</h2>
<p>The solverProperties class object was called in detectors/global2D/training/ODConvTrainer.cpp file. This was done using the code </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvTrainer::setSolverProperties(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> *argv[])</div>
<div class="line">{</div>
<div class="line">  <span class="keyword">auto</span> app = Gtk::Application::create(argc, argv, <span class="stringliteral">&quot;org.gtkmm.example&quot;</span>);</div>
<div class="line">  <a class="code" href="class_solver_properties.html">SolverProperties</a> solverProperties;</div>
<div class="line">  solverProperties.set_default_geometry (10000, 10000);</div>
<div class="line">  app-&gt;run(solverProperties);</div>
<div class="line">  ODConvTrainer::solverLocation = solverProperties.<a class="code" href="class_solver_properties.html#ac0260a1e7771629f57258bb251b86d4f">solverFileName</a>;</div>
<div class="line">}</div>
</div><!-- fragment --><p> The above code invoked the gtkmm solver window and in turn was used by the file examples/objectdetector/od_cnn_mnist_train_customSolver.cpp, where a ODTrainer object called the function mentioned above, the snippet from examples/objectdetector/od_cnn_mnist_train_customSolver.cpp is below, </p>
<div class="fragment"><div class="line"><a class="code" href="classod_1_1g2d_1_1_o_d_conv_trainer.html">od::g2d::ODConvTrainer</a> *mnist_trainer = <span class="keyword">new</span> <a class="code" href="classod_1_1g2d_1_1_o_d_conv_trainer.html">od::g2d::ODConvTrainer</a>(<span class="stringliteral">&quot;&quot;</span>,<span class="stringliteral">&quot;&quot;</span>);</div>
<div class="line">mnist_trainer-&gt;<a class="code" href="classod_1_1g2d_1_1_o_d_conv_trainer.html#a2399291c99f19af594b691835419e6ee">setSolverProperties</a>(argc,argv);</div>
<div class="line">mnist_trainer-&gt;<a class="code" href="classod_1_1g2d_1_1_o_d_conv_trainer.html#af1f228bf3c1e80be799768f85fb95ad1">startTraining</a>();</div>
</div><!-- fragment --><p><br/>
 </p>
<h2><a class="anchor" id="commit2_fourth_change_5"></a>
2.4.5)Features of solver creator</h2>
<p><b>1) The above code promts the user if any mistake is made from user-end.</b></p>
<p><b>2) Pressing update button every time may be time consuming, hence the latest commits involve the fact that without pressing the buttons the parameters cab ne edited</b></p>
<p><b>3) The main function of the update buttons after every parameter is make sure that, for future developments, if the intermediate parameters are to be accessed, the current version enables it.</b></p>
<p><b>4) Not many open source libraries had this functionality</b></p>
<p><br/>
 </p>
<h2><a class="anchor" id="commit2_fifth"></a>
2.5) Network creator first version, commit 2.5</h2>
<p>In this commit, network creator was added, based on gtkmm and caffe library, to the opendetection source. The <a href="https://github.com/abhi-kumar/opendetection/commit/768364812b6c30c2f27b6f768fec1366a9cb0885">fragmented commit: Custom network designer added part1</a>, involved additions of 15 code source files.</p>
<p>To understand each layer in caffe, please refer to <a href="https://abhishek4273.com/2016/02/07/ann-chapter-3-deep-learning-using-caffe-python/">blog post here</a></p>
<p>The major class, networkCreator, was introduced into files <a class="el" href="network_8h_source.html">detectors/global2D/training/network.h</a> and detectors/global2D/training/network.cpp.</p>
<p>As mentioned earlier, it is clear as to how to initialize, place and show elements like buttons, radiobuttons, labels, and text-boxes. Herein a new elemet is introduced, dropDownBox.</p>
<h2><a class="anchor" id="commit2_fifth_change_1"></a>
2.5.1) Current Features to the </h2>
<p>a) The activation category includes the following activation layers</p>
<ul>
<li>Absolute Value (AbsVal) Layer</li>
<li>Exponential (Exp) Layer</li>
<li>Log Layer</li>
<li>Power Layer</li>
<li>Parameterized rectified linear unit (PReLU) Layer</li>
<li>Rectified linear unit (ReLU) Layer</li>
<li>Sigmoid Layer</li>
<li>Hyperbolic tangent (TanH) Layer</li>
</ul>
<p>b) The critical category includes the most crucial layers</p>
<ul>
<li>Accuracy Layer</li>
<li>Convolution Layer</li>
<li>Deconvolution layer</li>
<li>Dropout Layer</li>
<li>InnerProduct (Fully Connected) Layer</li>
<li>Pooling Layer</li>
<li>Softmax classification Layer c) The weight initializers include the following options</li>
<li>Constant</li>
<li>Uniform</li>
<li>Gaussian</li>
<li>Positive Unit Ball</li>
<li>Xavier</li>
<li>MSRA</li>
<li>Bilinear d) One more important feature included is that user can display the layers and simultaneously during the display delete the layers at the end.</li>
</ul>
<h2><a class="anchor" id="commit2_fifth_change_2"></a>
2.5.2) How to introduce a dropdown menu example</h2>
<p>In the header file, example, in <a class="el" href="network_8h_source.html">network.h</a> the following was added, please refer comments in the code below </p>
<div class="fragment"><div class="line">Gtk::ComboBox     combo_activationLayerType;        <span class="comment">//Box to hold all the drop boxes</span></div>
<div class="line"><span class="keyword">class </span>ModelColumns : <span class="keyword">public</span> Gtk::TreeModel::ColumnRecord</div>
<div class="line">{</div>
<div class="line">  <span class="keyword">public</span>:</div>
<div class="line">    ModelColumns(){ add(m_col_id); add(m_col_name); add(m_col_extra);}</div>
<div class="line">    Gtk::TreeModelColumn&lt;int&gt; m_col_id;       <span class="comment">//Id to every drop down list</span></div>
<div class="line">    Gtk::TreeModelColumn&lt;Glib::ustring&gt; m_col_name;     <span class="comment">//Name to every drop down list</span></div>
<div class="line">    Gtk::TreeModelColumn&lt;Glib::ustring&gt; m_col_extra;    <span class="comment">//Extra details to every drop down list</span></div>
<div class="line">};</div>
<div class="line">ModelColumns      column_activationLayerType;</div>
<div class="line">Gtk::CellRendererText   cell_activationLayerType;</div>
<div class="line">Glib::RefPtr&lt;Gtk::ListStore&gt;  ref_activationLayerType;</div>
</div><!-- fragment --><p>Similarly, for all other layer types comboCoxes, modelColumns etc were added into the code.</p>
<p>Now, in the cpp file, example, in network.cpp the following was added, please refer comments in the code below </p>
<div class="fragment"><div class="line">ref_activationLayerType = Gtk::ListStore::create(column_activationLayerType);   <span class="comment">//create an object for list</span></div>
<div class="line">combo_activationLayerType.set_model(ref_activationLayerType);       <span class="comment">//add list to combobox</span></div>
<div class="line"></div>
<div class="line"><span class="comment">//First item in list</span></div>
<div class="line">Gtk::TreeModel::Row row_activationLayerType = *(ref_activationLayerType-&gt;append());</div>
<div class="line">row_activationLayerType[column_activationLayerType.m_col_id] = 1;</div>
<div class="line">row_activationLayerType[column_activationLayerType.m_col_name] = <span class="stringliteral">&quot;AbsVal&quot;</span>;</div>
<div class="line">row_activationLayerType[column_activationLayerType.m_col_extra] = <span class="stringliteral">&quot;Absolute Value Layer&quot;</span>;</div>
<div class="line">combo_activationLayerType.set_active(row_activationLayerType);</div>
<div class="line"></div>
<div class="line"><span class="comment">//Second item in list</span></div>
<div class="line">row_activationLayerType = *(ref_activationLayerType-&gt;append());</div>
<div class="line">row_activationLayerType[column_activationLayerType.m_col_id] = 2;</div>
<div class="line">row_activationLayerType[column_activationLayerType.m_col_name] = <span class="stringliteral">&quot;Exp&quot;</span>;</div>
<div class="line">row_activationLayerType[column_activationLayerType.m_col_extra] = <span class="stringliteral">&quot;Exponential Layer&quot;</span>;</div>
<div class="line"></div>
<div class="line"><span class="comment">//Attach list items to combobox</span></div>
<div class="line">combo_activationLayerType.pack_start(column_activationLayerType.m_col_id);</div>
<div class="line">combo_activationLayerType.pack_start(column_activationLayerType.m_col_name);</div>
<div class="line">combo_activationLayerType.set_cell_data_func(cell_activationLayerType,  sigc::mem_fun(*<span class="keyword">this</span>, &amp;<a class="code" href="class_network_creator.html#aafce580a3d8794bfd8fa443e0c56da2f">NetworkCreator::on_cell_data_extra</a>));</div>
<div class="line">combo_activationLayerType.pack_start(cell_activationLayerType);</div>
<div class="line">  </div>
<div class="line"><span class="comment">//Attach combobox to grid</span></div>
<div class="line">m_grid1.attach(combo_activationLayerType,2,1,2,1);</div>
<div class="line">combo_activationLayerType.signal_changed().connect( sigc::mem_fun(*<span class="keyword">this</span>, &amp;<a class="code" href="class_network_creator.html#a6227c9c0bf0ec68669cdc816bde618ae">NetworkCreator::on_combo_changed</a>) );</div>
</div><!-- fragment --><p>Thus rest of the thisngs can be added.</p>
<p>The color to the listbox is added into the <a class="el" href="main_window_8h_source.html">detectors/global2D/training/mainWindow.h</a> file. The snippet below is added to the function on_cell_data_extra()</p>
<div class="fragment"><div class="line"><span class="keyword">auto</span> row_activationLayerType = *iter;</div>
<div class="line"><span class="keyword">const</span> Glib::ustring extra_activationLayerType = row_activationLayerType[column_activationLayerType.m_col_extra];</div>
<div class="line"><span class="keywordflow">if</span>(extra_activationLayerType.empty())</div>
<div class="line">  cell_activationLayerType.property_text() = <span class="stringliteral">&quot;(none)&quot;</span>;</div>
<div class="line"><span class="keywordflow">else</span></div>
<div class="line">  cell_activationLayerType.property_text() = <span class="stringliteral">&quot;-&quot;</span> + extra_activationLayerType + <span class="stringliteral">&quot;-&quot;</span>;</div>
<div class="line">cell_activationLayerType.property_foreground() = <span class="stringliteral">&quot;green&quot;</span>; </div>
</div><!-- fragment --><p>The main function here, when combo is changed, data stored in the variables are updated using the on_combo_changed() function. The following is the code snippet</p>
<div class="fragment"><div class="line">Gtk::TreeModel::iterator iter_activationLayerType = combo_activationLayerType.get_active();</div>
<div class="line"><span class="keywordflow">if</span>(iter_activationLayerType)</div>
<div class="line">{</div>
<div class="line">  Gtk::TreeModel::Row row_activationLayerType = *iter_activationLayerType;</div>
<div class="line">  <span class="keywordflow">if</span>(row_activationLayerType)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordtype">int</span> id_activationLayerType = row_activationLayerType[column_activationLayerType.m_col_id];</div>
<div class="line">    Glib::ustring name_activationLayerType = row_activationLayerType[column_activationLayerType.m_col_name];</div>
<div class="line"><span class="comment">//      std::cout &lt;&lt; &quot; ID=&quot; &lt;&lt; id_activationLayerType &lt;&lt; &quot;, name=&quot; &lt;&lt; name_activationLayerType &lt;&lt; std::endl;</span></div>
<div class="line">    activationLayerTypeData = name_activationLayerType;</div>
<div class="line">  }</div>
<div class="line">}</div>
</div><!-- fragment --><h2><a class="anchor" id="commit2_fifth_change_3"></a>
2.5.3) Idea of multiple windows</h2>
<p>For every drop down comboBox, one selected, and clicked the button next to it, a new window is opened. It as done usinf the functions</p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> showWindow_main();</div>
<div class="line"><span class="keywordtype">void</span> showWindow_activationLayerType(Glib::ustring data);</div>
<div class="line"><span class="keywordtype">void</span> showWindow_displayWindow();</div>
<div class="line"><span class="keywordtype">void</span> showWindow_criticalLayerType(Glib::ustring data);</div>
<div class="line"><span class="keywordtype">void</span> showWindow_normalizationLayerType(Glib::ustring data);</div>
<div class="line"><span class="keywordtype">void</span> showWindow_lossLayerType(Glib::ustring data);</div>
<div class="line"><span class="keywordtype">void</span> showWindow_extraLayerType(Glib::ustring data);</div>
</div><!-- fragment --><p>Each of these functions has an argument "data", this argument refers to the type of layer that needs to be appended. Each of these functions are placed into a different file, <a class="el" href="activation_window_8h_source.html">activationWindow.h</a>, <a class="el" href="loss_window_8h_source.html">lossWindow.h</a>, <a class="el" href="main_window_8h_source.html">mainWindow.h</a>, <a class="el" href="critical_window_8h_source.html">criticalWindow.h</a>, <a class="el" href="normalization_window_8h_source.html">normalizationWindow.h</a> and <a class="el" href="extra_window_8h_source.html">extraWindow.h</a>. And with regards to the argument "data", different elemts are shown on window. Say, when TanH is the argument, different elements are shown as to when Sigmoid is the argument.</p>
<h2><a class="anchor" id="commit2_fifth_change_4"></a>
2.5.4) Displaying network and deleting layer at the end</h2>
<p>When the layers are appended into the network, displaying it is a necessity to make sure if everythig is proper or not. The code at <a class="el" href="display_window_8h_source.html">displayWindow.h</a>, inside the function showWindow_displayWindow(), enables the display.</p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> <a class="code" href="class_network_creator.html#abdb5dea439a1e5f77d3c2d0b89ac3fdd">NetworkCreator::showWindow_displayWindow</a>()</div>
<div class="line">{</div>
<div class="line">  <span class="keyword">remove</span>();</div>
<div class="line">  set_title(<span class="stringliteral">&quot;Display Entire Network&quot;</span>);</div>
<div class="line">  set_border_width(10);</div>
<div class="line">  add(<a class="code" href="class_network_creator.html#a1ad29720ef2b66130a5ac60452c9f5d3">box_fullCnnLayerMatter</a>);</div>
<div class="line">  <a class="code" href="class_network_creator.html#a0a25dd9ad0c4d678aa0bcea61c7bb66c">buffer_fullCnnLayerMatter</a>-&gt;set_text(fullCnnLayerMatter);</div>
<div class="line">  <a class="code" href="class_network_creator.html#ac567cf3cb8e83cc7d88b89bc66f1e251">textView_fullCnnLayerMatter</a>.set_buffer(<a class="code" href="class_network_creator.html#a0a25dd9ad0c4d678aa0bcea61c7bb66c">buffer_fullCnnLayerMatter</a>);</div>
<div class="line">  show_all_children();</div>
<div class="line">}</div>
</div><!-- fragment --><p>Herein, the variable fullCnnLayerMatter gets updated everytime before display using a node from <a class="el" href="node_8h_source.html">detectors/global2D/training/node.h</a>. This file implements a linked list, which whenever a layer is added, adds a node to the list.</p>
<p>The structre of the node is as follows </p>
<div class="fragment"><div class="line"><span class="keyword">struct </span><a class="code" href="struct_node.html">Node</a> {</div>
<div class="line">  Glib::ustring <a class="code" href="struct_node.html#afa559ea7058661196a500141f93c9205">data</a>;</div>
<div class="line">  <a class="code" href="struct_node.html">Node</a>* <a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a>;</div>
<div class="line">};</div>
</div><!-- fragment --><p>If no layer has been added before, the list is initialized using, </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> initializeLayer(<span class="keyword">struct</span> <a class="code" href="struct_node.html">Node</a> *headLayer, Glib::ustring data)</div>
<div class="line">{</div>
<div class="line">  headLayer-&gt;<a class="code" href="struct_node.html#afa559ea7058661196a500141f93c9205">data</a> = data;</div>
<div class="line">  headLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a> = NULL;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line">If list has been initialised, next upcoming layers are appended <span class="keyword">using</span>,</div>
<div class="line">\code{.cpp}</div>
<div class="line"><span class="keywordtype">void</span> appendLayer(<span class="keyword">struct</span> <a class="code" href="struct_node.html">Node</a> *headLayer, Glib::ustring data)</div>
<div class="line">{</div>
<div class="line">  <a class="code" href="struct_node.html">Node</a> *newLayer = <span class="keyword">new</span> <a class="code" href="struct_node.html">Node</a>;</div>
<div class="line">  newLayer-&gt;<a class="code" href="struct_node.html#afa559ea7058661196a500141f93c9205">data</a> = data;</div>
<div class="line">  newLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a> = NULL;</div>
<div class="line"></div>
<div class="line">  <a class="code" href="struct_node.html">Node</a> *currentLayer = headLayer;</div>
<div class="line">  <span class="keywordflow">while</span>(currentLayer)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">if</span>(currentLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a> == NULL)</div>
<div class="line">    {</div>
<div class="line">      currentLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a> = newLayer;</div>
<div class="line">      <span class="keywordflow">return</span>;</div>
<div class="line">    }</div>
<div class="line">    currentLayer = currentLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a>;</div>
<div class="line">  }</div>
<div class="line">}</div>
</div><!-- fragment --><p>The network's last layer can be deleted, with the button "Delete Layer" in the display window, with the base function as, </p>
<div class="fragment"><div class="line"><span class="keywordtype">bool</span> deleteLayer(<span class="keyword">struct</span> <a class="code" href="struct_node.html">Node</a> **headLayer, <a class="code" href="struct_node.html">Node</a> *deleteLayer) </div>
<div class="line">{</div>
<div class="line">  <a class="code" href="struct_node.html">Node</a> *currentLayer = *headLayer;</div>
<div class="line">  <span class="keywordflow">if</span>(deleteLayer == *headLayer)</div>
<div class="line">  {</div>
<div class="line">    *headLayer = currentLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a>;</div>
<div class="line">    <span class="keyword">delete</span> deleteLayer;</div>
<div class="line">    <span class="keywordflow">return</span> <span class="keyword">true</span>;</div>
<div class="line">  }</div>
<div class="line">  </div>
<div class="line">  <span class="keywordflow">while</span>(currentLayer)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">if</span>(currentLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a> == deleteLayer)</div>
<div class="line">    {</div>
<div class="line">      currentLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a> = deleteLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a>;</div>
<div class="line">      <span class="keyword">delete</span> deleteLayer;</div>
<div class="line">      <span class="keywordflow">return</span> <span class="keyword">true</span>;</div>
<div class="line">    }</div>
<div class="line">    currentLayer = currentLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a>;</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">return</span> <span class="keyword">false</span>;</div>
<div class="line">}</div>
</div><!-- fragment --><p>Any layer properties can be searched using the folowwing search function, </p>
<div class="fragment"><div class="line"><span class="keyword">struct </span><a class="code" href="struct_node.html">Node</a> *searchLayer(<span class="keyword">struct</span> <a class="code" href="struct_node.html">Node</a> *headLayer, Glib::ustring data)</div>
<div class="line">{</div>
<div class="line">  <a class="code" href="struct_node.html">Node</a> *currentLayer = headLayer;</div>
<div class="line">  <span class="keywordflow">while</span>(currentLayer)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">if</span>(currentLayer-&gt;<a class="code" href="struct_node.html#afa559ea7058661196a500141f93c9205">data</a> == data) </div>
<div class="line">    {</div>
<div class="line">      <span class="keywordflow">return</span> currentLayer;</div>
<div class="line">    }</div>
<div class="line">    currentLayer = currentLayer-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a>;</div>
<div class="line">  }</div>
<div class="line">  std::cout &lt;&lt; <span class="stringliteral">&quot;No Layer &quot;</span> &lt;&lt; data &lt;&lt; <span class="stringliteral">&quot; in the CNN.&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">}</div>
</div><!-- fragment --><p>As mentioned earlier, when the network is to be displayed, the variable fullCnnLayerMatter, is updated, using the following function, </p>
<div class="fragment"><div class="line">Glib::ustring displayCNN(<span class="keyword">struct</span> <a class="code" href="struct_node.html">Node</a> *headLayer)</div>
<div class="line">{</div>
<div class="line">  <a class="code" href="struct_node.html">Node</a> *cnn = headLayer;</div>
<div class="line">  Glib::ustring fullCnnLayerMatter = <span class="stringliteral">&quot;&quot;</span>;</div>
<div class="line">  <span class="keywordflow">while</span>(cnn)</div>
<div class="line">  {</div>
<div class="line"><span class="comment">//    std::cout &lt;&lt; cnn-&gt;data &lt;&lt; std::endl;</span></div>
<div class="line">    fullCnnLayerMatter += cnn-&gt;<a class="code" href="struct_node.html#afa559ea7058661196a500141f93c9205">data</a>;</div>
<div class="line">    cnn = cnn-&gt;<a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a>;</div>
<div class="line">  }</div>
<div class="line"><span class="comment">//  std::cout &lt;&lt; std::endl;</span></div>
<div class="line"><span class="comment">//  std::cout &lt;&lt; std::endl;</span></div>
<div class="line">  <span class="keywordflow">return</span> fullCnnLayerMatter;</div>
<div class="line">}</div>
</div><!-- fragment --><p><br/>
 </p>
<h2><a class="anchor" id="commit2_sixth"></a>
2.6) Network creator second version, commit 2.6</h2>
<p>In this commit, network creator was appended with Normalization layer properties, based on gtkmm and caffe library, to the opendetection source. The <a href="https://github.com/abhi-kumar/opendetection/commit/c99ec1a7c8a5b913a50d62ceecbfea59fb8222a1">fragmented commit: Added Normalization layers in the customised trainer</a>, involved additions/changes of 3 code source files.</p>
<p>It had three Normalization layer options involved,</p>
<ul>
<li>Batch Normalization (BatchNorm) Layer</li>
<li>Local Response Normalization (LRN) Layer</li>
<li>Multivariate Response Normalization (MVN) Layer</li>
</ul>
<p><br/>
</p>
<h2><a class="anchor" id="commit2_seventh"></a>
2.7) Selective search based object locatization version 1, commit 2.7</h2>
<p>In this commit, slective serach based object localization, based on opencv library, was added to the opendetection source. The <a href="https://github.com/abhi-kumar/opendetection/commit/eebfee93d4fc82296d38dbb0e54d0a9a6e9f94fa">fragmented commit: Selective Search Beta Version Added</a>, involved additions/changes of 21 code source files.</p>
<p>To understand the selective search based object localization algorithm, please refer to <a href="http://koen.me/research/selectivesearch/">link here</a></p>
<p>The algo, when put simply, involves,</p>
<ul>
<li>Graph based image segmentation</li>
<li>Finding different features of the all the segmented parts</li>
<li>Finding closeness between the features of the neighboring parts</li>
<li>Merging the closest parts and continuing futher till the algorithm is breaked.</li>
</ul>
<h2><a class="anchor" id="commit2_seventh_change_1"></a>
2.7.1) Graph based image segmentation</h2>
<p>With proper permission (conversation through mails) from the author P. Felzenszwalb(<a href="#" onclick="location.href='mai'+'lto:'+'pff'+'@a'+'i.m'+'it'+'.ed'+'u'; return false;">pff@a<span style="display: none;">.nosp@m.</span>i.mi<span style="display: none;">.nosp@m.</span>t.edu</a>) the code from <a href="http://cs.brown.edu/~pff/segment/">link</a> was adopted and slightly modified in order to get the results as per the requirement of the algorithm.</p>
<p>This part involves 10 major code files, all adopted from the same link mentioned above</p>
<ul>
<li><a class="el" href="convolve_8h_source.html">detectors/global2D/localization/convolve.h</a></li>
<li><a class="el" href="disjoint-set_8h_source.html">detectors/global2D/localization/disjoint-set.h</a></li>
<li><a class="el" href="filter_8h_source.html">detectors/global2D/localization/filter.h</a></li>
<li><a class="el" href="image_8h_source.html">detectors/global2D/localization/image.h</a></li>
<li><a class="el" href="imconv_8h_source.html">detectors/global2D/localization/imconv.h</a></li>
<li><a class="el" href="imutil_8h_source.html">detectors/global2D/localization/imutil.h</a></li>
<li><a class="el" href="misc_8h_source.html">detectors/global2D/localization/misc.h</a></li>
<li><a class="el" href="pnmfile_8h_source.html">detectors/global2D/localization/pnmfile.h</a></li>
<li><a class="el" href="segment-graph_8h_source.html">detectors/global2D/localization/segment-graph.h</a></li>
<li><a class="el" href="segment-image_8h_source.html">detectors/global2D/localization/segment-image.h</a></li>
</ul>
<h2><a class="anchor" id="commit2_seventh_change_2"></a>
2.7.2) Selective Search Base class</h2>
<p>The class ODSelectiveSearchBase, detectors/global2D/localization/ODSelectiveSearchBase.cpp and <a class="el" href="_o_d_selective_search_base_8h_source.html">detectors/global2D/localization/ODSelectiveSearchBase.h</a>, derived over the public elements of ODDetector2D, has a set of very important functions,</p>
<ul>
<li>acuiring the image</li>
<li>preprocessing the image</li>
<li>using the 10 major code files mentioned above in a customized way to segment the image and extract the clustered components.</li>
</ul>
<p>The header file has the following variables, </p>
<div class="fragment"><div class="line">Mat img, cluster, outputImg, sp_preProcessed, gray_mask;</div>
<div class="line"><span class="keywordtype">int</span> inputImageHeight;</div>
<div class="line"><span class="keywordtype">int</span> inputImageWidth;</div>
<div class="line"><span class="keywordtype">int</span> total_masks;</div>
<div class="line">vector &lt; vector &lt;int&gt; &gt; sp;   <span class="comment">//container for the cluster </span></div>
</div><!-- fragment --><p>In this class, the image is aquired using the following snippet from the .cpp file, </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> ODSelectiveSearchBase::acquireImages(<span class="keywordtype">string</span> imageLocation, <span class="keywordtype">int</span> imgWidth, <span class="keywordtype">int</span> imgHeight)</div>
<div class="line">{</div>
<div class="line">  inputImageHeight = imgHeight;</div>
<div class="line">  inputImageWidth = imgWidth;</div>
<div class="line">  img = imread(imageLocation,1);</div>
<div class="line">  resize(img, img, Size(imgWidth,imgHeight));</div>
<div class="line">  cluster = imread(imageLocation,1);</div>
<div class="line">  resize(cluster, cluster, Size(imgWidth,imgHeight));</div>
<div class="line">  outputImg = imread(imageLocation,1);</div>
<div class="line">  resize(outputImg, outputImg, Size(imgWidth,imgHeight));</div>
<div class="line">  }</div>
</div><!-- fragment --><p>The preprocessing is done using, </p>
<div class="fragment"><div class="line">Mat ODSelectiveSearchBase::preProcessImg(Mat <a class="code" href="classimage.html">image</a>)</div>
<div class="line">{</div>
<div class="line">  vector&lt;Mat&gt; channels; </div>
<div class="line">  Mat img_hist_equalized;</div>
<div class="line">  cvtColor(image, img_hist_equalized, CV_BGR2YCrCb); </div>
<div class="line">  split(img_hist_equalized,channels); </div>
<div class="line">  equalizeHist(channels[0], channels[0]); </div>
<div class="line">  merge(channels,img_hist_equalized);</div>
<div class="line">  cvtColor(img_hist_equalized, img_hist_equalized, CV_YCrCb2BGR);</div>
<div class="line">  <span class="keywordflow">return</span> img_hist_equalized;</div>
<div class="line">}</div>
</div><!-- fragment --><p> Herein, the preprocessing is done as</p>
<ul>
<li>conversion of BGR image to YCrCb</li>
<li>first channel, the one with the intensity is eualized</li>
<li>reconversion of eualized YCrCb image to BGR color type</li>
</ul>
<p>This is followed by graph based image segmentation by the function, </p>
<div class="fragment"><div class="line">vector &lt; vector &lt;int&gt; &gt; ODSelectiveSearchBase::getSuperPixels(Mat im, <span class="keywordtype">int</span> &amp;totalMasks, <span class="keywordtype">float</span> Sigma, <span class="keywordtype">float</span> K, <span class="keywordtype">float</span> Min_size, <span class="keywordtype">string</span> imageLocation)</div>
</div><!-- fragment --><p>Image is first segmented, </p>
<div class="fragment"><div class="line"><span class="keywordtype">string</span> img_location = imageLocation + <span class="stringliteral">&quot;img.ppm&quot;</span>;</div>
<div class="line">imwrite(img_location, im);</div>
<div class="line"><span class="keywordtype">float</span> sigma = Sigma;</div>
<div class="line"><span class="keywordtype">float</span> k = K;</div>
<div class="line"><span class="keywordtype">int</span> min_size = Min_size;</div>
<div class="line"><a class="code" href="classimage.html">image&lt;rgb&gt;</a> *input = loadPPM(img_location.c_str());</div>
<div class="line"><span class="keywordtype">int</span> num_ccs; </div>
<div class="line"><a class="code" href="classimage.html">image&lt;rgb&gt;</a> *seg = segment_image(input, sigma, k, min_size, &amp;num_ccs);</div>
<div class="line"><a class="code" href="classimage.html">image&lt;uchar&gt;</a> *gr = imageRGBtoGRAY(seg);</div>
<div class="line"><span class="keywordtype">int</span> num = imRef(seg,0,0).r;</div>
</div><!-- fragment --><p> Herein, the image after preprocessing, is stored in ".ppm" format as the segmentation code only prefers image in that format. Image is then segmented using the segment_image function and to find the number of segments, num, it is converted to grayscale and the number of colors there then represent the number of segments.</p>
<p>The next step is to create a list of those segments. It is not often possible to create an uchar grayscale image mask with opencv here, because, opencv supports color version from 0 to 255 and in most cases the segments are greater than 255. Thus, we first store, every pixel's value in the previous rgb image with the pixel's location into a text file named "segmented.txt". </p>
<div class="fragment"><div class="line">ofstream myfile;</div>
<div class="line">myfile.open(<span class="stringliteral">&quot;segmented.txt&quot;</span>);</div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> R = 0; R &lt; seg-&gt;<a class="code" href="classimage.html#a7703c5fa4f43e3aedb28c6056ac7d96a">height</a>(); R++)</div>
<div class="line">{</div>
<div class="line">  <span class="keywordflow">for</span>(<span class="keywordtype">int</span> C = 0; C &lt; seg-&gt;<a class="code" href="classimage.html#a51d9e4a8e20b9a5af2e11e0e4b39815a">width</a>(); C++)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordtype">int</span> numR, numG, numB, numGr;</div>
<div class="line">    numR = imRef(seg,C,R).r;</div>
<div class="line">    numG = imRef(seg,C,R).g;</div>
<div class="line">    numB = imRef(seg,C,R).b;</div>
<div class="line">    numGr = imRef(gr,C,R);</div>
<div class="line">    myfile &lt;&lt; numR &lt;&lt; <span class="stringliteral">&quot; &quot;</span> &lt;&lt; numG &lt;&lt; <span class="stringliteral">&quot; &quot;</span> &lt;&lt; numB &lt;&lt; <span class="stringliteral">&quot; &quot;</span> &lt;&lt; numGr &lt;&lt; endl;</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line"><span class="keywordtype">string</span> output_location = <span class="stringliteral">&quot;../images/img.ppm&quot;</span>;</div>
<div class="line">savePPM(seg, output_location.c_str());</div>
<div class="line">myfile.close();</div>
</div><!-- fragment --><p>Now the task was to create a uniform label for each segment, something like, first segment as 1, second as 2 and so on and this was carries out using, </p>
<div class="fragment"><div class="line">vector&lt; vector &lt;int&gt; &gt; mem;</div>
<div class="line">mem.resize(num, std::vector&lt;int&gt;(3, 0));</div>
<div class="line">vector &lt;int&gt; val;</div>
<div class="line">val.resize(num, 0);</div>
<div class="line"><span class="keywordtype">int</span> insertion_mem_index = 1;</div>
<div class="line"></div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> i = 0; i &lt; num; i++)</div>
<div class="line">{</div>
<div class="line">  infile &gt;&gt; R &gt;&gt; G &gt;&gt; B &gt;&gt; GR;</div>
<div class="line">  <span class="keywordflow">if</span>(i==0)</div>
<div class="line">  {</div>
<div class="line">    mem[i][0] = R;</div>
<div class="line">    mem[i][1] = G;</div>
<div class="line">    mem[i][2] = B;</div>
<div class="line">    val[i] = maskValue;</div>
<div class="line">    mask[H][W] = val[0];</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">else</span></div>
<div class="line">  {</div>
<div class="line">    vector &lt;int&gt; query = {R, G, B};</div>
<div class="line">    vector &lt;vector &lt;int&gt; &gt; ::iterator it;</div>
<div class="line">    <span class="keyword">auto</span> pos =find(mem.begin(),mem.end(),query);</div>
<div class="line">    <span class="keywordflow">if</span>(pos != mem.end())</div>
<div class="line">    {</div>
<div class="line">      mask[H][W] = val[pos-mem.begin()];</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">else</span></div>
<div class="line">    {</div>
<div class="line">      mem[insertion_mem_index][0] = R;</div>
<div class="line">      mem[insertion_mem_index][1] = G;</div>
<div class="line">      mem[insertion_mem_index][2] = B;</div>
<div class="line">      maskValue++;</div>
<div class="line">      insertion_mem_index++;</div>
<div class="line">      val[insertion_mem_index] = maskValue;</div>
<div class="line">      mask[H][W] = maskValue;       </div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">  W = W + 1;</div>
<div class="line">  <span class="keywordflow">if</span>(W==im.cols)</div>
<div class="line">  {</div>
<div class="line">    H = H + 1;</div>
<div class="line">    W = 0;</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line">totalMasks = maskValue;</div>
<div class="line"><span class="keywordflow">return</span> mask;</div>
</div><!-- fragment --><p><br/>
 </p>
<h2><a class="anchor" id="commit2_seventh_change_3"></a>
2.7.3) Selective Search Model class and example of selective search based localization</h2>
<p>These introduced files, detectors/global2D/localization/ODSelectiveSearchModel.cpp and <a class="el" href="_o_d_selective_search_model_8h_source.html">detectors/global2D/localization/ODSelectiveSearchModel.h</a>, have the following responsibilities(not in order)</p>
<ul>
<li>calculating histogram of the different features</li>
<li>finding neighbors for each of the clustered region</li>
<li>finding similarities( or closure distance) between two regions based on the histogram of different features</li>
<li>merging the closest regions</li>
<li>removing very small and very big clusters</li>
<li>adding ROIs to images based on merged regions</li>
</ul>
<p>The functions will make more sense with a side by side explanation of the file /examples/objectdetector/od_localize_selective_search.cpp, which specifies an example of selective search based localization of objects.</p>
<p>This selective search has a set of 13 parameters which drive the entire algo here. The example's first part includes the base class, </p>
<div class="fragment"><div class="line"><a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html">od::g2d::ODSelectiveSearchBase</a> ss;</div>
<div class="line"><span class="keywordtype">string</span> imageLocation = <span class="stringliteral">&quot;../examples/objectdetector/Localization_Images/&quot;</span>; <span class="comment">//Para 1</span></div>
<div class="line"><span class="keywordtype">string</span> imageName = <span class="stringliteral">&quot;sample1.png&quot;</span>;           <span class="comment">// Para 1.1</span></div>
<div class="line"><span class="keywordtype">int</span> imgWidth = 640;             <span class="comment">//Para 2</span></div>
<div class="line"><span class="keywordtype">int</span> imgHeight = 480;              <span class="comment">//Para 3</span></div>
<div class="line"><span class="keywordtype">string</span> im = imageLocation + imageName;  </div>
<div class="line">ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a79e26c08ec87036e29afb1e76edb6dbe">acquireImages</a>(im, imgWidth, imgHeight);</div>
<div class="line"></div>
<div class="line">ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#aea7bc44bb35d6c243256485b4e91b350">sp_preProcessed</a> = ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a4647eecec483200d492a1eac07ed10ce">preProcessImg</a>(ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#abcf8809a727bec1c0f1c11fa12c8b95a">img</a>);</div>
<div class="line"></div>
<div class="line"><span class="keywordtype">float</span> sigma = 0.5;              <span class="comment">//Para 4</span></div>
<div class="line"><span class="keywordtype">float</span> k = 580;                <span class="comment">//Para 5</span></div>
<div class="line"><span class="keywordtype">int</span> min_size = 50;              <span class="comment">//Para 6</span></div>
<div class="line">ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a8182564951b3325188e8797a8c494dce">sp</a> = ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a038a71cff9d2a21c66106089ec429557">getSuperPixels</a>(ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#aea7bc44bb35d6c243256485b4e91b350">sp_preProcessed</a>, ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a2ec02c1bbf117bd08f58e412cb00bf88">total_masks</a>, sigma, k, min_size, imageLocation);</div>
</div><!-- fragment --><p>Parameter 1 : Image file location, this is important because all the temporary files created will also be stored here</p>
<p>Parameter 1.1 : Image file name ( <b>make sure you don't name image as img.png or img.ppm</b> )</p>
<p>Parameter 2 : Height to which it has to be resized</p>
<p>Parameter 3 : Width to which it has to be resized</p>
<p>The functions acquireImages and preProcessImg have been discussed earlier.</p>
<p>Parameters 4,5,6 : These will be more specifically understood once selective search algorithm paper is read <a href="http://koen.me/research/selectivesearch/">here</a></p>
<p>getSuperPixels function mentioned above, as stated there too, is for clustering.</p>
<p>The next part comes as creation of the model class object, </p>
<div class="fragment"><div class="line"><a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_model.html">od::g2d::ODSelectiveSearchModel</a> regions[ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a2ec02c1bbf117bd08f58e412cb00bf88">total_masks</a>];</div>
<div class="line"><span class="keywordtype">int</span> min_height = 20;              <span class="comment">//Para 7</span></div>
<div class="line"><span class="keywordtype">int</span> min_width = 20;             <span class="comment">//Para 8</span></div>
<div class="line"><a class="code" href="namespaceod_1_1g2d.html#a5cf5355ae46651c7576e57b4afa3c209">od::g2d::refineRegions</a>(ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a8182564951b3325188e8797a8c494dce">sp</a>, ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a2ec02c1bbf117bd08f58e412cb00bf88">total_masks</a>, regions, min_height, min_width);</div>
<div class="line"><span class="keywordtype">float</span> spSize = ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a8182564951b3325188e8797a8c494dce">sp</a>.size() * ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a8182564951b3325188e8797a8c494dce">sp</a>[0].size();</div>
</div><!-- fragment --><p>Instead of creating a single object, an object is created for every clustered object in the form of array of objects.</p>
<p>Parameter 7 : Any clustered region with a pixels size of less this height is not considered</p>
<p>Parameter 8 : Any clustered region with a pixels size of less this width is not considered</p>
<p>This, discarding of the regions is carried out using the functions refineRegions, </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> refineRegions(vector &lt; vector &lt;int&gt; &gt; sp, <span class="keywordtype">int</span> total_masks, ODSelectiveSearchModel regions[], <span class="keywordtype">int</span> min_height, <span class="keywordtype">int</span> min_width)</div>
<div class="line">{</div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; total_masks; i++)</div>
<div class="line">  {</div>
<div class="line">    regions[i].setLabel(i);</div>
<div class="line">    regions[i].min_x = 100000;</div>
<div class="line">    regions[i].min_y = 100000;</div>
<div class="line">    regions[i].max_x = -1;</div>
<div class="line">    regions[i].max_y = -1;</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> r = 0; r &lt; sp.size(); r++)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">int</span> c = 0; c &lt; sp[0].size(); c++)</div>
<div class="line">    {</div>
<div class="line">      regions[sp[r][c]].size++;</div>
<div class="line">      <span class="keywordflow">if</span>(regions[sp[r][c]].min_x &gt; c)</div>
<div class="line">        regions[sp[r][c]].min_x = c;</div>
<div class="line">      <span class="keywordflow">if</span>(regions[sp[r][c]].min_y &gt; r)</div>
<div class="line">        regions[sp[r][c]].min_y = r;</div>
<div class="line">      <span class="keywordflow">if</span>(regions[sp[r][c]].max_x &lt; c)</div>
<div class="line">        regions[sp[r][c]].max_x = c;</div>
<div class="line">      <span class="keywordflow">if</span>(regions[sp[r][c]].max_y &lt; r)</div>
<div class="line">        regions[sp[r][c]].max_y = r;</div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; total_masks; i++)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">if</span>((regions[i].max_x - regions[i].min_x &gt; min_width) and (regions[i].max_y - regions[i].min_y &gt; min_height))</div>
<div class="line">      regions[i].validity = <span class="keyword">true</span>;</div>
<div class="line">    <span class="keywordflow">else</span></div>
<div class="line">      regions[i].validity = <span class="keyword">false</span>;</div>
<div class="line">  }</div>
<div class="line">}</div>
</div><!-- fragment --><p> Another purpose of this function is to find the boundaries on these irregulary shaped clustered regions. The functions finds the minimum and maximum x and y pixel coordinates of each region</p>
<p>Next step is to find histogram of features of each of the regions, </p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> histSize = 25;              <span class="comment">//Para 9</span></div>
<div class="line"><span class="keywordtype">float</span> hist_range_min = 1;             <span class="comment">//Para 10</span></div>
<div class="line"><span class="keywordtype">float</span> hist_range_max = 255;           <span class="comment">//Para 11</span></div>
<div class="line"><a class="code" href="namespaceod_1_1g2d.html#a4f8d695d48676f65224899d29ecd414b">od::g2d::createModel</a>(ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a8182564951b3325188e8797a8c494dce">sp</a>, ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a2ec02c1bbf117bd08f58e412cb00bf88">total_masks</a>, ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a8d627961a7f7a044de1e1a7c02f4e6ba">gray_mask</a>, regions, histSize, hist_range_min, hist_range_max);</div>
</div><!-- fragment --><p>Parameter 9: Size of the bin in the histogram of features</p>
<p>Parameter 10 and 11: Range of values to be considered while calculating the histogram</p>
<p>The function createModel is designed to find the histogram matrices, </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> createModel(vector &lt; vector &lt;int&gt; &gt; sp, <span class="keywordtype">int</span> total_masks, Mat grayMask, ODSelectiveSearchModel regions[], <span class="keywordtype">int</span> histSize, <span class="keywordtype">float</span> hist_range_min, <span class="keywordtype">float</span> hist_range_max)</div>
<div class="line">{</div>
<div class="line">  Mat regionMask = grayMask.clone();</div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; total_masks; i++)</div>
<div class="line">  {</div>
<div class="line">    regionMask = grayMask.clone();</div>
<div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">int</span> r = 0; r &lt; sp.size(); r++)</div>
<div class="line">    {</div>
<div class="line">      <span class="keywordflow">for</span>(<span class="keywordtype">int</span> c = 0; c &lt; sp[0].size(); c++)</div>
<div class="line">      {</div>
<div class="line">        <span class="keywordflow">if</span>(sp[r][c] != i)</div>
<div class="line">          regionMask.at&lt;uchar&gt;(r,c) = 0;</div>
<div class="line">      }</div>
<div class="line">    }</div>
<div class="line">    <span class="keywordflow">if</span>(regions[i].size&lt;200)</div>
<div class="line">      regions[i].validity = <span class="keyword">false</span>;</div>
<div class="line">    <span class="comment">//Hessian Matrix</span></div>
<div class="line">    regions[i].xx_hist = get_hess_hist_xx(regionMask, histSize, hist_range_min, hist_range_max);</div>
<div class="line">    regions[i].xy_hist = get_hess_hist_xy(regionMask, histSize, hist_range_min, hist_range_max);</div>
<div class="line">    regions[i].yy_hist = get_hess_hist_yy(regionMask, histSize, hist_range_min, hist_range_max);</div>
<div class="line">    <span class="comment">//orientation Matrix</span></div>
<div class="line">    regions[i].orientation_image_hist = get_orientation_hist(regionMask, histSize, hist_range_min, hist_range_max);</div>
<div class="line">    <span class="comment">//Differential Excitation Matrix</span></div>
<div class="line">    regions[i].differential_excitation_hist = get_diff_exci_hist(regionMask, histSize, hist_range_min, hist_range_max);</div>
<div class="line">    <span class="comment">//Color Histogram</span></div>
<div class="line">    <span class="keywordtype">float</span> range[] = { hist_range_min, hist_range_max } ;</div>
<div class="line">    <span class="keyword">const</span> <span class="keywordtype">float</span>* histRange = { range };</div>
<div class="line">    <span class="keywordtype">bool</span> uniform = <span class="keyword">true</span>; <span class="keywordtype">bool</span> accumulate = <span class="keyword">false</span>;</div>
<div class="line">    calcHist(&amp;grayMask, 1, 0, Mat(), regions[i].color_hist, 1, &amp;histSize, &amp;histRange, uniform, accumulate );</div>
<div class="line">  }</div>
<div class="line">}</div>
</div><!-- fragment --><p><b>Note: There's an hidden parameter here, which decides the region's validity. It works on the conception that every region should have atleast a certain number of pixels, here 200. This number can be manipulated if needed</b></p>
<p>Four matrices are calculated:</p>
<ul>
<li><a href="http://mathworld.wolfram.com/Hessian.html">Hessian Matrix</a></li>
<li><a href="http://www.ee.oulu.fi/~jiechen/paper/TPAMI2009-WLD.pdf">Orientation Matrix</a></li>
<li><a href="http://www.ee.oulu.fi/~jiechen/paper/TPAMI2009-WLD.pdf">Differential Excitation Matrix</a></li>
<li>Color Histogram Matrix</li>
</ul>
<p>Hessian Matrix is calculated in xx, xy and yy type double differentiations, the matrices used are ( in same order), </p>
<div class="fragment"><div class="line">kernel_filter_1 = (Mat_&lt;double&gt;(7,7) &lt;&lt; 1.57130243e-04, 7.17839338e-04, 0, -1.76805171e-03, 0, 7.17839338e-04, 1.57130243e-04,</div>
<div class="line">                1.91423823e-03, 8.74507340e-03, 0, -2.15392793e-02, 0, 8.74507340e-03, 1.91423823e-03,</div>
<div class="line">                8.57902057e-03, 3.91926999e-02, 0, -9.65323526e-02, 0, 3.91926999e-02, 8.57902057e-03,</div>
<div class="line">                1.41444137e-02, 6.46178379e-02, 0, -1.59154943e-01, 0, 6.46178379e-02, 1.41444137e-02,</div>
<div class="line">                8.57902057e-03, 3.91926999e-02, 0, -9.65323526e-02, 0, 3.91926999e-02, 8.57902057e-03,</div>
<div class="line">                1.91423823e-03, 8.74507340e-03, 0, -2.15392793e-02, 0, 8.74507340e-03, 1.91423823e-03,</div>
<div class="line">                1.57130243e-04, 7.17839338e-04, 0, -1.76805171e-03, 0, 7.17839338e-04, 1.57130243e-04</div>
<div class="line">            );</div>
<div class="line"></div>
<div class="line"> </div>
<div class="line">kernel_filter_2 = (Mat_&lt;double&gt;(7,7) &lt;&lt; 0.00017677, 0.00143568, 0.00321713, 0, -0.00321713, -0.00143568, -0.00017677,</div>
<div class="line">                0.00143568, 0.0116601, 0.02612847, 0, -0.02612847, -0.0116601, -0.00143568,</div>
<div class="line">                0.00321713, 0.02612847, 0.05854983, 0, -0.05854983, -0.02612847, -0.00321713,</div>
<div class="line">                0, 0, 0, 0, 0, 0, 0,</div>
<div class="line">                -0.00321713, -0.02612847, -0.05854983, 0, 0.05854983, 0.02612847, 0.00321713,</div>
<div class="line">                -0.00143568, -0.0116601, -0.02612847, 0, 0.02612847, 0.0116601, 0.00143568,</div>
<div class="line">                -0.00017677, -0.00143568, -0.00321713, 0, 0.00321713, 0.00143568, 0.00017677</div>
<div class="line">              ); </div>
<div class="line"></div>
<div class="line"></div>
<div class="line">kernel_filter_3 = (Mat_&lt;double&gt;(7,7) &lt;&lt; </div>
<div class="line">        1.57130243e-04, 1.91423823e-03, 8.57902057e-03, 1.41444137e-02, 8.57902057e-03, 1.91423823e-03, 1.57130243e-04,</div>
<div class="line">        7.17839338e-04, 8.74507340e-03, 3.91926999e-02, 6.46178379e-02, 3.91926999e-02, 8.74507340e-03, 7.17839338e-04,</div>
<div class="line">        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,</div>
<div class="line">        -1.76805171e-03, -2.15392793e-02, -9.65323526e-02, -1.59154943e-01, -9.65323526e-02, -2.15392793e-02, -1.76805171e-03,</div>
<div class="line">        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,</div>
<div class="line">        7.17839338e-04, 8.74507340e-03, 3.91926999e-02, 6.46178379e-02, 3.91926999e-02, 8.74507340e-03, 7.17839338e-04,</div>
<div class="line">        1.57130243e-04, 1.91423823e-03, 8.57902057e-03, 1.41444137e-02, 8.57902057e-03, 1.91423823e-03, 1.57130243e-04</div>
<div class="line">              );</div>
</div><!-- fragment --><p>The Orientation matrix is calculated as, </p>
<div class="fragment"><div class="line">Mat get_orientation_hist(Mat regionMask, <span class="keywordtype">int</span> histSize, <span class="keywordtype">float</span> hist_range_min, <span class="keywordtype">float</span> hist_range_max)</div>
<div class="line">{</div>
<div class="line">  Mat kernel_filter_1(3,3, CV_32FC1,1);</div>
<div class="line">  kernel_filter_1 = (Mat_&lt;double&gt;(3,3) &lt;&lt; 1, 1, 1, 1, -8, 1, 1, 1, 1);</div>
<div class="line">  Mat kernel_filter_2(3,3, CV_32FC1,1);</div>
<div class="line">  kernel_filter_2 = (Mat_&lt;double&gt;(3,3) &lt;&lt; 0,0,0,0,1,0,0,0,0);</div>
<div class="line">  Mat kernel_filter_3(3,3, CV_32FC1,1);</div>
<div class="line">  kernel_filter_3 = (Mat_&lt;double&gt;(3,3) &lt;&lt; 1,2,1,0,0,0,-1,-2,-1);</div>
<div class="line">  Mat kernel_filter_4(3,3, CV_32FC1,1);</div>
<div class="line">  kernel_filter_4 = (Mat_&lt;double&gt;(3,3) &lt;&lt; 1,0,-1,2,0,-2,1,0,-1);</div>
<div class="line">  Mat image_filtered_v1;</div>
<div class="line">  Mat image_filtered_v2;</div>
<div class="line">  Mat image_filtered_v3;  </div>
<div class="line">  Mat image_filtered_v4;</div>
<div class="line">  <span class="keywordtype">int</span> temp30; </div>
<div class="line">  <span class="comment">//filtering</span></div>
<div class="line">  filter2D(regionMask, image_filtered_v1, -1, kernel_filter_1,Point(-1,-1), 0,BORDER_DEFAULT );</div>
<div class="line">  filter2D(regionMask, image_filtered_v2, -1, kernel_filter_2,Point(-1,-1), 0,BORDER_DEFAULT );</div>
<div class="line">  filter2D(regionMask, image_filtered_v3, -1, kernel_filter_3,Point(-1,-1), 0,BORDER_DEFAULT );</div>
<div class="line">  filter2D(regionMask, image_filtered_v4, -1, kernel_filter_4,Point(-1,-1), 0,BORDER_DEFAULT );</div>
<div class="line"></div>
<div class="line">  <span class="comment">//Orientation New</span></div>
<div class="line">  <span class="keywordtype">float</span> temp_5;</div>
<div class="line">  <span class="keywordtype">float</span> temp_6;</div>
<div class="line">  <span class="keywordtype">float</span> temp_7_theta;</div>
<div class="line">  <span class="keywordtype">float</span> temp_8_theta_dash;</div>
<div class="line">  <span class="keywordtype">int</span> temp_9_theta_dash_quantized;</div>
<div class="line">  <span class="keywordtype">float</span> quantized_1[12] = {0,1,2,3,4,5,6,7,8,9,10,11};</div>
<div class="line">  <span class="keywordtype">int</span> quantized_count_1 = 0;</div>
<div class="line">  <span class="keywordtype">float</span> orientation_image_matrix[regionMask.rows][regionMask.cols];</div>
<div class="line">  Mat orientation_image = regionMask.clone();</div>
<div class="line">  <span class="keywordflow">for</span>(<span class="keywordtype">int</span> r = 0; r &lt; regionMask.rows; r++)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">int</span> c = 0; c &lt; regionMask.cols; c++)</div>
<div class="line">    {</div>
<div class="line">      orientation_image_matrix[r][c] = 0;</div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">for</span>(<span class="keywordtype">int</span> r = 0; r &lt; regionMask.rows; r++)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">int</span> c = 0; c &lt; regionMask.cols; c++)</div>
<div class="line">    {</div>
<div class="line">      temp_5 = image_filtered_v3.at&lt;schar&gt;(r,c);</div>
<div class="line">      temp_6 = image_filtered_v4.at&lt;schar&gt;(r,c);</div>
<div class="line">      <span class="keywordflow">if</span>(temp_6 != 0 &amp;&amp; temp_5 != 0)</div>
<div class="line">      {</div>
<div class="line">        temp_7_theta = atan(temp_5/temp_6);</div>
<div class="line">      }</div>
<div class="line">      <span class="keywordflow">else</span> <span class="keywordflow">if</span>(temp_6 == 0 &amp;&amp; temp_5 &gt; 0)</div>
<div class="line">      {</div>
<div class="line">        temp_7_theta = M_PI/2;</div>
<div class="line">      }</div>
<div class="line">      <span class="keywordflow">else</span> <span class="keywordflow">if</span>(temp_6 == 0 &amp;&amp; temp_5 &lt; 0)</div>
<div class="line">      {</div>
<div class="line">        temp_7_theta = -M_PI/2;</div>
<div class="line">      }</div>
<div class="line">      <span class="keywordflow">else</span> <span class="keywordflow">if</span>(temp_6 == 0 &amp;&amp; temp_5 == 0)</div>
<div class="line">      {</div>
<div class="line">        temp_7_theta = 0;</div>
<div class="line">      }</div>
<div class="line">      <span class="keywordflow">else</span> <span class="keywordflow">if</span>(temp_6 != 0 &amp;&amp; temp_5 == 0)</div>
<div class="line">      {</div>
<div class="line">        temp_7_theta = 0;</div>
<div class="line">      }     </div>
<div class="line"></div>
<div class="line">      <span class="keywordflow">if</span>(temp_5 &gt;= 0 &amp;&amp; temp_6 &gt;= 0)</div>
<div class="line">      {</div>
<div class="line">        temp_8_theta_dash = temp_7_theta;</div>
<div class="line">      }</div>
<div class="line">      <span class="keywordflow">else</span> <span class="keywordflow">if</span>(temp_5 &lt; 0 &amp;&amp; temp_6 &gt;= 0)</div>
<div class="line">      {</div>
<div class="line">        temp_8_theta_dash = temp_7_theta + M_PI;</div>
<div class="line">      }</div>
<div class="line">      <span class="keywordflow">else</span> <span class="keywordflow">if</span>(temp_5 &lt; 0 &amp;&amp; temp_6 &lt; 0)</div>
<div class="line">      {</div>
<div class="line">        temp_8_theta_dash = temp_7_theta + M_PI;</div>
<div class="line">      }</div>
<div class="line">      <span class="keywordflow">else</span> <span class="keywordflow">if</span>(temp_5 &gt;= 0 &amp;&amp; temp_6 &lt; 0)</div>
<div class="line">      {</div>
<div class="line">        temp_8_theta_dash = temp_7_theta + 2*M_PI;</div>
<div class="line">      }</div>
<div class="line">      temp_9_theta_dash_quantized = floor((temp_8_theta_dash*11)/(2*M_PI));</div>
<div class="line">      orientation_image_matrix[r][c] = temp_9_theta_dash_quantized;</div>
<div class="line">      orientation_image.at&lt;uchar&gt;(r,c) = temp_9_theta_dash_quantized; </div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line"><span class="comment">/*  for(int r = 0; r &lt; img.rows; r++)</span></div>
<div class="line"><span class="comment">  {</span></div>
<div class="line"><span class="comment">    for(int c = 0; c &lt; img.cols; c++)</span></div>
<div class="line"><span class="comment">    {</span></div>
<div class="line"><span class="comment">      cout &lt;&lt; orientation_image_matrix[r][c] &lt;&lt; &quot; &quot;;</span></div>
<div class="line"><span class="comment">    }</span></div>
<div class="line"><span class="comment">    cout &lt;&lt; endl;</span></div>
<div class="line"><span class="comment">  }</span></div>
<div class="line"><span class="comment">*/</span></div>
<div class="line">  <span class="keywordtype">float</span> range[] = { hist_range_min, hist_range_max } ;</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">float</span>* histRange = { range };</div>
<div class="line">  <span class="keywordtype">bool</span> uniform = <span class="keyword">true</span>; <span class="keywordtype">bool</span> accumulate = <span class="keyword">false</span>;</div>
<div class="line">  Mat orientation_image_hist;</div>
<div class="line"><span class="comment"></span></div>
<div class="line"><span class="comment">  /// Compute the histograms:</span></div>
<div class="line"><span class="comment"></span>  calcHist( &amp;orientation_image, 1, 0, Mat(), orientation_image_hist, 1, &amp;histSize, &amp;histRange, uniform, accumulate );</div>
<div class="line">  <span class="keywordflow">return</span> orientation_image_hist;</div>
<div class="line">}</div>
</div><!-- fragment --><p>The differential excitation matrix is calculated as, </p>
<div class="fragment"><div class="line">Mat get_diff_exci_hist(Mat regionMask, <span class="keywordtype">int</span> histSize, <span class="keywordtype">float</span> hist_range_min, <span class="keywordtype">float</span> hist_range_max)</div>
<div class="line">{</div>
<div class="line">  Mat kernel_filter_1(3,3, CV_32FC1,1);</div>
<div class="line">  kernel_filter_1 = (Mat_&lt;double&gt;(3,3) &lt;&lt; 1, 1, 1, 1, -8, 1, 1, 1, 1);</div>
<div class="line">  Mat kernel_filter_2(3,3, CV_32FC1,1);</div>
<div class="line">  kernel_filter_2 = (Mat_&lt;double&gt;(3,3) &lt;&lt; 0,0,0,0,1,0,0,0,0);</div>
<div class="line">  Mat kernel_filter_3(3,3, CV_32FC1,1);</div>
<div class="line">  kernel_filter_3 = (Mat_&lt;double&gt;(3,3) &lt;&lt; 1,2,1,0,0,0,-1,-2,-1);</div>
<div class="line">  Mat kernel_filter_4(3,3, CV_32FC1,1);</div>
<div class="line">  kernel_filter_4 = (Mat_&lt;double&gt;(3,3) &lt;&lt; 1,0,-1,2,0,-2,1,0,-1);</div>
<div class="line">  Mat image_filtered_v1;</div>
<div class="line">  Mat image_filtered_v2;</div>
<div class="line">  Mat image_filtered_v3;  </div>
<div class="line">  Mat image_filtered_v4;</div>
<div class="line">  <span class="keywordtype">int</span> temp30; </div>
<div class="line">  <span class="comment">//filtering</span></div>
<div class="line">  filter2D(regionMask, image_filtered_v1, -1, kernel_filter_1,Point(-1,-1), 0,BORDER_DEFAULT );</div>
<div class="line">  filter2D(regionMask, image_filtered_v2, -1, kernel_filter_2,Point(-1,-1), 0,BORDER_DEFAULT );</div>
<div class="line">  filter2D(regionMask, image_filtered_v3, -1, kernel_filter_3,Point(-1,-1), 0,BORDER_DEFAULT );</div>
<div class="line">  filter2D(regionMask, image_filtered_v4, -1, kernel_filter_4,Point(-1,-1), 0,BORDER_DEFAULT );</div>
<div class="line"></div>
<div class="line">  <span class="comment">//Differential Excitation New</span></div>
<div class="line">  <span class="keywordtype">float</span> temp_1;</div>
<div class="line">  <span class="keywordtype">float</span> temp_2;</div>
<div class="line">  <span class="keywordtype">float</span> temp_3_alpha;</div>
<div class="line">  <span class="keywordtype">float</span> temp_4_quantized_alpha;</div>
<div class="line">  <span class="keywordtype">int</span> quantized[8] = {0,1,2,3,4,5,6,7};</div>
<div class="line">  <span class="keywordtype">int</span> quantized_count = 0;</div>
<div class="line">  <span class="keywordtype">int</span> differential_excitation_image_matrix[regionMask.rows][regionMask.cols];</div>
<div class="line">  Mat differential_excitation_image = regionMask.clone();</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">for</span>(<span class="keywordtype">int</span> r = 0; r &lt; regionMask.rows; r++)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">int</span> c = 0; c &lt; regionMask.cols; c++)</div>
<div class="line">    {</div>
<div class="line">      differential_excitation_image_matrix[r][c] = 0;</div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">for</span>(<span class="keywordtype">int</span> r = 0; r &lt; regionMask.rows; r++)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">int</span> c = 0; c &lt; regionMask.cols; c++)</div>
<div class="line">    {</div>
<div class="line">      temp_1 = image_filtered_v1.at&lt;schar&gt;(r,c);</div>
<div class="line">      temp_2 = image_filtered_v2.at&lt;schar&gt;(r,c);</div>
<div class="line">      <span class="keywordflow">if</span>(temp_2 != 0)</div>
<div class="line">      {</div>
<div class="line">        temp_3_alpha = atan(temp_1/temp_2);</div>
<div class="line">      }</div>
<div class="line">      <span class="keywordflow">else</span> <span class="keywordflow">if</span>(temp_2 == 0 &amp;&amp; temp_1 &gt; 0)</div>
<div class="line">      {</div>
<div class="line">        temp_3_alpha = M_PI/2;</div>
<div class="line">      }</div>
<div class="line">      <span class="keywordflow">else</span> <span class="keywordflow">if</span>(temp_2 == 0 &amp;&amp; temp_1 &lt; 0)</div>
<div class="line">      {</div>
<div class="line">        temp_3_alpha = -M_PI/2;</div>
<div class="line">      }</div>
<div class="line">      <span class="keywordflow">else</span> <span class="keywordflow">if</span>(temp_2 == 0 &amp;&amp; temp_1 == 0)</div>
<div class="line">      {</div>
<div class="line">        temp_3_alpha = 0;</div>
<div class="line">      }</div>
<div class="line">      temp_4_quantized_alpha = floor(((temp_3_alpha + M_PI/2)/M_PI)*7);</div>
<div class="line">      differential_excitation_image_matrix[r][c] = temp_4_quantized_alpha;</div>
<div class="line">      differential_excitation_image.at&lt;uchar&gt;(r,c) = temp_4_quantized_alpha;</div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line"><span class="comment">/*  for(int r = 0; r &lt; img.rows; r++)</span></div>
<div class="line"><span class="comment">  {</span></div>
<div class="line"><span class="comment">    for(int c = 0; c &lt; img.cols; c++)</span></div>
<div class="line"><span class="comment">    {</span></div>
<div class="line"><span class="comment">      cout &lt;&lt; differential_excitation_image_matrix[r][c] &lt;&lt; &quot; &quot;;</span></div>
<div class="line"><span class="comment">    }</span></div>
<div class="line"><span class="comment">    cout &lt;&lt; endl;</span></div>
<div class="line"><span class="comment">  }</span></div>
<div class="line"><span class="comment">*/</span></div>
<div class="line">  <span class="keywordtype">float</span> range[] = { hist_range_min, hist_range_max } ;</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">float</span>* histRange = { range };</div>
<div class="line">  <span class="keywordtype">bool</span> uniform = <span class="keyword">true</span>; <span class="keywordtype">bool</span> accumulate = <span class="keyword">false</span>;</div>
<div class="line">  Mat differential_excitation_hist;</div>
<div class="line"><span class="comment"></span></div>
<div class="line"><span class="comment">  /// Compute the histograms:</span></div>
<div class="line"><span class="comment"></span>  calcHist( &amp;differential_excitation_image, 1, 0, Mat(), differential_excitation_hist, 1, &amp;histSize, &amp;histRange, uniform, accumulate );</div>
<div class="line">  <span class="keywordflow">return</span> differential_excitation_hist;</div>
<div class="line">}</div>
</div><!-- fragment --><p>And the color histogram is calculated as, </p>
<div class="fragment"><div class="line">calcHist(&amp;grayMask, 1, 0, Mat(), regions[i].color_hist, 1, &amp;histSize, &amp;histRange, uniform, accumulate );</div>
</div><!-- fragment --><p>After this the major function, extractRois is called, </p>
<div class="fragment"><div class="line"><span class="keywordtype">int</span> numRounds = ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a2ec02c1bbf117bd08f58e412cb00bf88">total_masks</a>/2;         <span class="comment">//Para 12</span></div>
<div class="line"><span class="keywordtype">int</span> minRegionSize = 200;            <span class="comment">//Para 13</span></div>
<div class="line">vector &lt; vector &lt;int&gt; &gt; pts = <a class="code" href="namespaceod_1_1g2d.html#a8f24a57c183fd50da7fcce9e6e9ef436">od::g2d::extractROIs</a>(ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a2ec02c1bbf117bd08f58e412cb00bf88">total_masks</a>, regions, numRounds, spSize, ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a8182564951b3325188e8797a8c494dce">sp</a>, ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#abcf8809a727bec1c0f1c11fa12c8b95a">img</a>, ss.<a class="code" href="classod_1_1g2d_1_1_o_d_selective_search_base.html#a8d627961a7f7a044de1e1a7c02f4e6ba">gray_mask</a>, minRegionSize, histSize, hist_range_min, hist_range_max);</div>
</div><!-- fragment --><p>The Parameter numRounds indirectly manipulates the number of ROIs to be found in direct proportion.</p>
<p>The function extractROI starts with a loop, </p>
<div class="fragment"><div class="line"><span class="keywordflow">while</span>(checkRounds(totals, regions, numRounds) and value &gt; 0)</div>
</div><!-- fragment --><p>This checkRounds function is, </p>
<div class="fragment"><div class="line"><span class="keywordtype">bool</span> checkRounds(<span class="keywordtype">int</span> totals, ODSelectiveSearchModel regions[], <span class="keywordtype">int</span> numRounds)</div>
<div class="line">{</div>
<div class="line">  <span class="keywordtype">int</span> num = 0;</div>
<div class="line">  <span class="keywordflow">for</span>(<span class="keywordtype">int</span> i = 0; i &lt; totals; i++)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">if</span>(regions[i].validity == <span class="keyword">false</span>)</div>
<div class="line">    {</div>
<div class="line">      num++;</div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line"><span class="comment">//  cout &lt;&lt; &quot;num = &quot; &lt;&lt; num &lt;&lt; endl;</span></div>
<div class="line">  <span class="keywordflow">if</span>(num &gt; numRounds)</div>
<div class="line">    <span class="keywordflow">return</span> <span class="keyword">true</span>;</div>
<div class="line">  <span class="keywordflow">else</span></div>
<div class="line">    <span class="keywordflow">return</span> <span class="keyword">false</span>;</div>
<div class="line">}</div>
</div><!-- fragment --><p> It functions in two way,</p>
<ul>
<li>Checks whether numRounds limits is reached or not</li>
<li>A round is considered only when its validity is false, i.e., if number of rounds with validy false reach numRounds limit then code is stopped</li>
</ul>
<p>Every loop inside extractRoi goes through, </p>
<div class="fragment"><div class="line">vector &lt; vector &lt;int&gt; &gt; sp_neighbors = findNeighbors(regions, total_masks, img, sp);</div>
<div class="line">vector &lt;float&gt; similarities;</div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> i=0; i&lt;sp_neighbors.size(); i++)</div>
<div class="line">{</div>
<div class="line"></div>
<div class="line">  <span class="keywordtype">float</span> sim = calcSimilarities(regions[sp_neighbors[i][0]],regions[sp_neighbors[i][1]], spSize);  </div>
<div class="line"><span class="comment">//      cout &lt;&lt; i &lt;&lt; &quot;    &quot; &lt;&lt; sp_neighbors[i][0] &lt;&lt; &quot; &quot; &lt;&lt; sp_neighbors[i][1] &lt;&lt; &quot;    &quot; &lt;&lt; sim &lt;&lt; endl;</span></div>
<div class="line">  similarities.push_back(sim);</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="comment">//finding closest two regions</span></div>
<div class="line">value = min_element(similarities.begin(), similarities.end()) - similarities.begin();</div>
<div class="line"><span class="comment">//    cout &lt;&lt; &quot;in here&quot; &lt;&lt; endl;</span></div>
<div class="line"></div>
<div class="line"><span class="comment">//merging</span></div>
<div class="line">mergeRegions(value, regions, sp_neighbors, gray_mask, sp, minRegionSize, histSize, hist_range_min, hist_range_max);</div>
<div class="line"><span class="comment">//    cout &lt;&lt; &quot;value = &quot; &lt;&lt; value &lt;&lt; endl;</span></div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> i = 0; i &lt; total_masks; i++)</div>
<div class="line">{</div>
<div class="line">  <span class="keywordflow">if</span>(regions[i].validity == <span class="keyword">true</span>)</div>
<div class="line">  {</div>
<div class="line">    vector &lt;int&gt; temp;</div>
<div class="line">    temp.push_back(regions[i].min_x);</div>
<div class="line">    temp.push_back(regions[i].min_y);     </div>
<div class="line">    temp.push_back(regions[i].max_x);</div>
<div class="line">    temp.push_back(regions[i].max_y);</div>
<div class="line">    pts.push_back(temp);</div>
<div class="line"><span class="comment">//      rectangle(outputImg, Point(regions[i].min_x, regions[i].min_y), Point(regions[i].max_x, regions[i].max_y), Scalar(0, 0, 255));</span></div>
<div class="line">  }</div>
<div class="line">}</div>
</div><!-- fragment --><p>The first line, with the fuction, findNeighbors(), </p>
<div class="fragment"><div class="line"><span class="keywordtype">bool</span> checkNeighbors(ODSelectiveSearchModel a, ODSelectiveSearchModel b)</div>
<div class="line">{</div>
<div class="line">  <span class="keywordflow">if</span>( </div>
<div class="line">    ((a.min_x &lt; b.min_x) and (b.min_x &lt; a.max_x) and (a.min_y &lt; b.min_y) and (b.min_y &lt; a.max_y))</div>
<div class="line">    or</div>
<div class="line">    ((a.min_x &lt; b.max_x) and (b.max_x &lt; a.max_x) and (a.min_y &lt; b.max_y) and (b.max_y &lt; a.max_y)) </div>
<div class="line">    or</div>
<div class="line">    ((a.min_x &lt; b.min_x) and (b.min_x &lt; a.max_x) and (a.min_y &lt; b.max_y) and (b.max_y &lt; a.max_y)) </div>
<div class="line">    or</div>
<div class="line">    ((a.min_x &lt; b.max_x) and (b.min_x &lt; a.max_x) and (a.min_y &lt; b.max_y) and (b.max_y &lt; a.max_y))</div>
<div class="line">  )</div>
<div class="line">  {</div>
<div class="line"></div>
<div class="line">    <span class="keywordflow">return</span> <span class="keyword">true</span>;</div>
<div class="line">  }</div>
<div class="line"></div>
<div class="line">  <span class="keywordflow">return</span> <span class="keyword">false</span>;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line">vector &lt; vector &lt;int&gt; &gt; findNeighbors(ODSelectiveSearchModel regions[], <span class="keywordtype">int</span> total_masks, Mat regionMask, vector &lt; vector &lt;int&gt; &gt; sp)</div>
<div class="line">{</div>
<div class="line">  vector &lt; vector &lt;int&gt; &gt; neighbors;</div>
<div class="line">  vector &lt;int&gt; rows;</div>
<div class="line">  rows.push_back(0);</div>
<div class="line">  rows.push_back(0);</div>
<div class="line">  <span class="keywordtype">int</span> num = 0;</div>
<div class="line">  <span class="keywordflow">for</span>(<span class="keywordtype">int</span> i = 1; i &lt; total_masks-1; i++)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">int</span> j = i+1; j &lt; i+20; j++)</div>
<div class="line">    {</div>
<div class="line">      <span class="keywordflow">if</span>(j&lt;total_masks-2)</div>
<div class="line">      {</div>
<div class="line">        <span class="keywordflow">if</span>(checkNeighbors(regions[i], regions[j]) and regions[i].validity == <span class="keyword">true</span> and regions[j].validity == <span class="keyword">true</span>)</div>
<div class="line">        { </div>
<div class="line">          rows[0] = i;</div>
<div class="line">          rows[1] = j;</div>
<div class="line">          neighbors.push_back(rows);</div>
<div class="line">        }</div>
<div class="line">      }</div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">return</span> neighbors;</div>
<div class="line">}</div>
</div><!-- fragment --><p> For every region, its just next region is foundout and then it is stored as a pair in vector neighbors.</p>
<p>Then, in the loop, </p>
<div class="fragment"><div class="line">vector &lt;float&gt; similarities;</div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> i=0; i&lt;sp_neighbors.size(); i++)</div>
<div class="line">{</div>
<div class="line"></div>
<div class="line">  <span class="keywordtype">float</span> sim = calcSimilarities(regions[sp_neighbors[i][0]],regions[sp_neighbors[i][1]], spSize);  </div>
<div class="line"><span class="comment">//      cout &lt;&lt; i &lt;&lt; &quot;    &quot; &lt;&lt; sp_neighbors[i][0] &lt;&lt; &quot; &quot; &lt;&lt; sp_neighbors[i][1] &lt;&lt; &quot;    &quot; &lt;&lt; sim &lt;&lt; endl;</span></div>
<div class="line">  similarities.push_back(sim);</div>
<div class="line">}</div>
</div><!-- fragment --><p> For every region, similarities are calculated for every neighbor using the function below, </p>
<div class="fragment"><div class="line"><span class="keywordtype">float</span> calcSimilarities(ODSelectiveSearchModel a, ODSelectiveSearchModel b, <span class="keywordtype">float</span> spSize)</div>
<div class="line">{</div>
<div class="line">  <span class="keywordtype">double</span> sim = 0.0;</div>
<div class="line">  sim += compareHist( a.xx_hist, b.xx_hist, 1); </div>
<div class="line">  sim += compareHist( a.xy_hist, b.xy_hist, 1); </div>
<div class="line">  sim += compareHist( a.yy_hist, b.yy_hist, 1); </div>
<div class="line">  sim += compareHist( a.orientation_image_hist, b.orientation_image_hist, 1); </div>
<div class="line">  sim += compareHist( a.differential_excitation_hist, b.differential_excitation_hist, 1); </div>
<div class="line">  sim += compareHist( a.color_hist, b.color_hist, 1);</div>
<div class="line">  sim += 100 * ((a.size + b.size)/spSize);</div>
<div class="line">  <span class="keywordtype">double</span> bbsize = ((max(a.max_x, b.max_x) - min(a.min_x, b.min_x))* (max(a.max_y, b.max_y) - min(a.min_y, b.min_y)) );</div>
<div class="line">  sim += 100*((bbsize - a.size - b.size) / spSize);</div>
<div class="line">  <span class="keywordflow">return</span> sim;</div>
<div class="line">}</div>
</div><!-- fragment --><p>Two regions are close if the similarity measure is less. </p>
<div class="fragment"><div class="line">value = min_element(similarities.begin(), similarities.end()) - similarities.begin();</div>
</div><!-- fragment --><p>The region with closest( minimum) value are then merged, </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> mergeRegions(<span class="keywordtype">int</span> value, ODSelectiveSearchModel regions[], vector &lt; vector &lt;int&gt; &gt; sp_neighbors, Mat grayMask, vector &lt; vector &lt;int&gt; &gt; sp, <span class="keywordtype">int</span> minRegionSize, <span class="keywordtype">int</span> histSize, <span class="keywordtype">float</span> hist_range_min, <span class="keywordtype">float</span> hist_range_max)</div>
<div class="line">{</div>
<div class="line"></div>
<div class="line">  regions[sp_neighbors[value][0]].validity = <span class="keyword">true</span>;</div>
<div class="line">  regions[sp_neighbors[value][1]].validity = <span class="keyword">false</span>;</div>
<div class="line">  regions[sp_neighbors[value][0]].min_x = min(regions[sp_neighbors[value][0]].min_x, regions[sp_neighbors[value][1]].min_x);</div>
<div class="line">  regions[sp_neighbors[value][0]].max_y = max(regions[sp_neighbors[value][0]].max_y, regions[sp_neighbors[value][1]].max_y);</div>
<div class="line">  regions[sp_neighbors[value][0]].min_x = min(regions[sp_neighbors[value][0]].min_x, regions[sp_neighbors[value][1]].min_x);</div>
<div class="line">  regions[sp_neighbors[value][0]].max_y = max(regions[sp_neighbors[value][0]].max_y, regions[sp_neighbors[value][1]].max_y);</div>
<div class="line">  regions[sp_neighbors[value][0]].size = regions[sp_neighbors[value][0]].size + regions[sp_neighbors[value][1]].size;</div>
<div class="line"></div>
<div class="line">  Mat regionMask = grayMask.clone();</div>
<div class="line">  <span class="keywordtype">int</span> i = sp_neighbors[value][0];</div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> r = 0; r &lt; sp.size(); r++)</div>
<div class="line">  {</div>
<div class="line">    <span class="keywordflow">for</span>(<span class="keywordtype">int</span> c = 0; c &lt; sp[0].size(); c++)</div>
<div class="line">    {</div>
<div class="line">      <span class="keywordflow">if</span>(sp[r][c] != i)</div>
<div class="line">        regionMask.at&lt;uchar&gt;(r,c) = 0;</div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">if</span>(regions[i].size&lt;minRegionSize)</div>
<div class="line">    regions[i].validity = <span class="keyword">false</span>;</div>
<div class="line"><span class="comment">//  cout &lt;&lt; i &lt;&lt; &quot; hessian &quot; &lt;&lt; regionMask.channels() &lt;&lt; endl;</span></div>
<div class="line">  <span class="comment">//Hessian Matrix</span></div>
<div class="line">  regions[i].xx_hist = get_hess_hist_xx(regionMask, histSize, hist_range_min, hist_range_max);</div>
<div class="line">  regions[i].xy_hist = get_hess_hist_xy(regionMask, histSize, hist_range_min, hist_range_max);</div>
<div class="line">  regions[i].yy_hist = get_hess_hist_yy(regionMask, histSize, hist_range_min, hist_range_max);</div>
<div class="line"></div>
<div class="line"><span class="comment">//  cout &lt;&lt; i &lt;&lt; &quot; orien&quot; &lt;&lt; endl;</span></div>
<div class="line">  <span class="comment">//orientation Matrix</span></div>
<div class="line">  regions[i].orientation_image_hist = get_orientation_hist(regionMask, histSize, hist_range_min, hist_range_max);</div>
<div class="line"></div>
<div class="line"><span class="comment">//  cout &lt;&lt; i &lt;&lt; &quot; diff&quot; &lt;&lt; endl;</span></div>
<div class="line">  <span class="comment">//Differential Excitation Matrix</span></div>
<div class="line">  regions[i].differential_excitation_hist = get_diff_exci_hist(regionMask, histSize, hist_range_min, hist_range_max);</div>
<div class="line"></div>
<div class="line"><span class="comment">//  cout &lt;&lt; i &lt;&lt; &quot; color&quot; &lt;&lt; endl;</span></div>
<div class="line">  <span class="comment">//Color Histogram</span></div>
<div class="line">  <span class="keywordtype">float</span> range[] = { hist_range_min, hist_range_max } ;</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">float</span>* histRange = { range };</div>
<div class="line">  <span class="keywordtype">bool</span> uniform = <span class="keyword">true</span>; <span class="keywordtype">bool</span> accumulate = <span class="keyword">false</span>;</div>
<div class="line">  calcHist(&amp;grayMask, 1, 0, Mat(), regions[i].color_hist, 1, &amp;histSize, &amp;histRange, uniform, accumulate );</div>
<div class="line">}</div>
</div><!-- fragment --><p>Thus this merging, calculating features, and scanning for new neighbors is continued till while loop continues. The result( all ROIs) is stored in file region_of_interests.txt in the folder example/objectdetector/Localization_Images</p>
<p><b>This selective search algo takes around 20-25 seconds to calculate bounding boxes with the parameters set in the example</b></p>
<p>Happy Coding!!!</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit3"></a>
Commit 3 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/10dea203378cb5c2ae16ca25552e423422e33690">link to commit:CNN_GPU branch added successfully</a>, was issued to add cnn-caffe based applications, for gpu based systems along with cpu based systems,. The commit had changes into 17 files, of which we have talked about most of them in previous commit documentation. Its a combination of set of commits made earler, put here together for better explanation of each of them.</p>
<p>The major addition was to implement mode selector on the basis of mode of compilation of library, </p>
<div class="fragment"><div class="line"><span class="preprocessor">#if(WITH_GPU)</span></div>
<div class="line"><span class="preprocessor"></span>Caffe::SetDevice(0);</div>
<div class="line">Caffe::set_mode(Caffe::GPU);</div>
<div class="line"><span class="preprocessor">#else</span></div>
<div class="line"><span class="preprocessor"></span>Caffe::set_mode(Caffe::CPU);</div>
<div class="line"><span class="preprocessor">#endif</span></div>
</div><!-- fragment --><p>Thus this commit was essential to make sure the library compiled on both cpu and gpu based systems.</p>
<p>Happy Coding!!!</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit4"></a>
Commit 4 </h1>
<p>This commit, <a href="examples/objectdetector/od_cnn_mnist_classification.cpp">link to commit:Modified examples by removing framegenerator header</a>, was issued to make sure caffe libraries and vtk libraries didn't clash while running examples on gpu based systems. The commit had changes into 4 files.</p>
<p>In all the files, the line which included, </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;common/utils/ODFrameGenerator.h&quot;</span></div>
</div><!-- fragment --><p>was erased out, and the issue was marked resolved.</p>
<p>Happy Coding!!!</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit5"></a>
Commit 5 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/484df2ce77c0104b96f47178c7cf5c43d30c8efd">link to commit:Updated Network Design will all essential Layers</a>, was issued to complete all the elements left out in the version 1 of ther network-creator.</p>
<p><b>The new layers added were</b></p>
<p>1) Loss Layers:</p>
<ul>
<li>Hinge Loss Layer</li>
<li>Contrastive Loss Layer</li>
<li>Eucledean Loss Layer</li>
<li>Multinomial Logistig Loss Layer</li>
<li>Sigmoid Cross Entropy Loss Layer</li>
</ul>
<p>2) Data and Extra Layers:</p>
<ul>
<li>Maximum Argument (ArgMAx) Layer</li>
<li>Binomial Normal Log Likelihood (BNLL) Layer</li>
<li>Element wise operation (Eltwise) Layer</li>
<li>Image Data Layer</li>
<li>LMDB/LEVELDB Data Layer</li>
</ul>
<p>These additions almost included all the required layers in caffe, the left out HDF Data layer was added later.</p>
<p>Happy Coding!!!</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit6"></a>
Commit 6 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/20d1fbf8abc00fcaad2c6067240e2eea758f7cdd">link to commit:AAM Classification example added</a>, was issued to add one of my personal researches. This commit included a prediction of Active Appearance Mocel Points on face using Convolutional Neural Networks. Very few works exist on this end, and hence the purpose behind taking up the research. This is a very crude and preliminary model of the research, just for the young users to be encouraged as to the extent to which cnn may work and how opendetection algorithm would help facilitate the same.</p>
<p>To understand AAM facial points, please refer <a href="http://arxiv.org/abs/1410.1037">link</a></p>
<p>The dataset was obtained from <a href="https://www.kaggle.com/c/facial-keypoints-detection/data">link</a></p>
<p>The dataset was trained on the network, </p>
<div class="fragment"><div class="line"><a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;multiple_output&quot;</span></div>
<div class="line">input: <span class="stringliteral">&quot;data&quot;</span></div>
<div class="line">input_shape {</div>
<div class="line">  dim: 1</div>
<div class="line">  dim: 3</div>
<div class="line">  dim: 96</div>
<div class="line">  dim: 96</div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;conv1&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;Convolution&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;data&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;conv1&quot;</span></div>
<div class="line">  param {</div>
<div class="line">    lr_mult: 1</div>
<div class="line">  }</div>
<div class="line">  param {</div>
<div class="line">    lr_mult: 2</div>
<div class="line">  }</div>
<div class="line">  convolution_param {</div>
<div class="line">    num_output: 32</div>
<div class="line">    pad: 2</div>
<div class="line">    kernel_size: 5</div>
<div class="line">    stride: 1</div>
<div class="line">    weight_filler {</div>
<div class="line">      type: <span class="stringliteral">&quot;gaussian&quot;</span></div>
<div class="line">      std: 0.01</div>
<div class="line">    }</div>
<div class="line">    bias_filler {</div>
<div class="line">      type: <span class="stringliteral">&quot;constant&quot;</span></div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;pool1&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;Pooling&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;conv1&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;pool1&quot;</span></div>
<div class="line">  pooling_param {</div>
<div class="line">    pool: MAX</div>
<div class="line">    kernel_size: 3</div>
<div class="line">    stride: 2</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;relu1&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;ReLU&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;pool1&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;pool1&quot;</span></div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;norm1&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;LRN&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;pool1&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;norm1&quot;</span></div>
<div class="line">  lrn_param {</div>
<div class="line">    local_size: 3</div>
<div class="line">    alpha: 5e-05</div>
<div class="line">    beta: 0.75</div>
<div class="line">    norm_region: WITHIN_CHANNEL</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;conv2&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;Convolution&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;norm1&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;conv2&quot;</span></div>
<div class="line">  param {</div>
<div class="line">    lr_mult: 1</div>
<div class="line">  }</div>
<div class="line">  param {</div>
<div class="line">    lr_mult: 2</div>
<div class="line">  }</div>
<div class="line">  convolution_param {</div>
<div class="line">    num_output: 32</div>
<div class="line">    pad: 2</div>
<div class="line">    kernel_size: 5</div>
<div class="line">    stride: 1</div>
<div class="line">     weight_filler {</div>
<div class="line">      type: <span class="stringliteral">&quot;gaussian&quot;</span></div>
<div class="line">      std: 0.01</div>
<div class="line">    }</div>
<div class="line">    bias_filler {</div>
<div class="line">      type: <span class="stringliteral">&quot;constant&quot;</span></div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;relu2&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;ReLU&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;conv2&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;conv2&quot;</span></div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;pool2&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;Pooling&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;conv2&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;pool2&quot;</span></div>
<div class="line">  pooling_param {</div>
<div class="line">    pool: AVE</div>
<div class="line">    kernel_size: 3</div>
<div class="line">    stride: 2</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;norm2&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;LRN&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;pool2&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;norm2&quot;</span></div>
<div class="line">  lrn_param {</div>
<div class="line">    local_size: 3</div>
<div class="line">    alpha: 5e-05</div>
<div class="line">    beta: 0.75</div>
<div class="line">    norm_region: WITHIN_CHANNEL</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;conv3&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;Convolution&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;norm2&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;conv3&quot;</span></div>
<div class="line">  convolution_param {</div>
<div class="line">    num_output: 64</div>
<div class="line">    pad: 2</div>
<div class="line">    kernel_size: 5</div>
<div class="line">    stride: 1</div>
<div class="line">     weight_filler {</div>
<div class="line">      type: <span class="stringliteral">&quot;gaussian&quot;</span></div>
<div class="line">      std: 0.01</div>
<div class="line">    }</div>
<div class="line">    bias_filler {</div>
<div class="line">      type: <span class="stringliteral">&quot;constant&quot;</span></div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;relu3&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;ReLU&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;conv3&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;conv3&quot;</span></div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;pool3&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;Pooling&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;conv3&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;pool3&quot;</span></div>
<div class="line">  pooling_param {</div>
<div class="line">    pool: AVE</div>
<div class="line">    kernel_size: 3</div>
<div class="line">    stride: 2</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;ip1&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;InnerProduct&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;pool3&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;ip1&quot;</span></div>
<div class="line">  param {</div>
<div class="line">    lr_mult: 1</div>
<div class="line">    decay_mult: 250</div>
<div class="line">  }</div>
<div class="line">  param {</div>
<div class="line">    lr_mult: 2</div>
<div class="line">    decay_mult: 0</div>
<div class="line">  }</div>
<div class="line">  inner_product_param {</div>
<div class="line">    num_output: 900</div>
<div class="line">     weight_filler {</div>
<div class="line">      type: <span class="stringliteral">&quot;gaussian&quot;</span></div>
<div class="line">      std: 0.01</div>
<div class="line">    }</div>
<div class="line">    bias_filler {</div>
<div class="line">      type: <span class="stringliteral">&quot;constant&quot;</span></div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line">layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;ip2&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;InnerProduct&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;ip1&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;ip2&quot;</span></div>
<div class="line">  param {</div>
<div class="line">    lr_mult: 1</div>
<div class="line">    decay_mult: 250</div>
<div class="line">  }</div>
<div class="line">  param {</div>
<div class="line">    lr_mult: 2</div>
<div class="line">    decay_mult: 0</div>
<div class="line">  }</div>
<div class="line">  inner_product_param {</div>
<div class="line">    num_output: 30</div>
<div class="line">     weight_filler {</div>
<div class="line">      type: <span class="stringliteral">&quot;gaussian&quot;</span></div>
<div class="line">      std: 0.01</div>
<div class="line">    }</div>
<div class="line">    bias_filler {</div>
<div class="line">      type: <span class="stringliteral">&quot;constant&quot;</span></div>
<div class="line">    }</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line"><span class="preprocessor"># ----------------------------------------------------------------</span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># ----------------- Multi-label Loss Function  -------------------</span></div>
<div class="line"><span class="preprocessor"></span><span class="preprocessor"># ----------------------------------------------------------------</span></div>
<div class="line"><span class="preprocessor"></span>layer {</div>
<div class="line">  <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>: <span class="stringliteral">&quot;prob&quot;</span></div>
<div class="line">  type: <span class="stringliteral">&quot;Sigmoid&quot;</span></div>
<div class="line">  bottom: <span class="stringliteral">&quot;ip2&quot;</span></div>
<div class="line">  top: <span class="stringliteral">&quot;prob&quot;</span></div>
<div class="line">}</div>
</div><!-- fragment --><p><b>Here When cpp version based elements of caffe library were used, the output produced extremely deviant results from the one obtained using python wrapper of caffe</b></p>
<p>The cpp version add the following function into detectors/global2D/detection/ODConvClassification.cpp, </p>
<div class="fragment"><div class="line">std::vector&lt;float&gt; ODConvClassification::classifyMultiLabel()</div>
<div class="line">{</div>
<div class="line"><span class="preprocessor">  #if(WITH_GPU)</span></div>
<div class="line"><span class="preprocessor"></span>  Caffe::SetDevice(0);</div>
<div class="line">  Caffe::set_mode(Caffe::GPU);</div>
<div class="line"><span class="preprocessor">  #else</span></div>
<div class="line"><span class="preprocessor"></span>  Caffe::set_mode(Caffe::CPU);</div>
<div class="line"><span class="preprocessor">  #endif</span></div>
<div class="line"><span class="preprocessor"></span>  Net&lt;float&gt;  net(networkFileLocation, TEST);</div>
<div class="line">  net.CopyTrainedLayersFrom(weightModelFileLoaction); </div>
<div class="line">    </div>
<div class="line">  <span class="keywordtype">float</span> type = 0.0;</div>
<div class="line">  <span class="keyword">const</span> vector&lt;Blob&lt;float&gt;*&gt;&amp; result =  net.Forward(inputBlob, &amp;type);</div>
<div class="line">  cout &lt;&lt; endl &lt;&lt; <span class="stringliteral">&quot;****** OUTPUT *******&quot;</span> &lt;&lt; endl;</div>
<div class="line">  Blob&lt;float&gt;* output_layer = net.output_blobs()[0];</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">float</span>* begin = output_layer-&gt;cpu_data();</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">float</span>* end = begin + output_layer-&gt;channels();</div>
<div class="line">  <span class="keywordflow">return</span> std::vector&lt;float&gt;(begin, end);</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line">std::vector&lt;float&gt; ODConvClassification::runMultiClassClassifier()</div>
<div class="line">{</div>
<div class="line"><span class="preprocessor">  #if(WITH_GPU)</span></div>
<div class="line"><span class="preprocessor"></span>  Caffe::SetDevice(0);</div>
<div class="line">  Caffe::set_mode(Caffe::GPU);</div>
<div class="line"><span class="preprocessor">  #else</span></div>
<div class="line"><span class="preprocessor"></span>  Caffe::set_mode(Caffe::CPU);</div>
<div class="line"><span class="preprocessor">  #endif</span></div>
<div class="line"><span class="preprocessor"></span>  <span class="comment">/* Load the network. */</span></div>
<div class="line">  Net&lt;float&gt;  net_m(networkFileLocation, TEST);</div>
<div class="line">  net_m.CopyTrainedLayersFrom(weightModelFileLoaction);</div>
<div class="line">  Blob&lt;float&gt;* input_layer = net_m.input_blobs()[0];</div>
<div class="line">  num_channels_ = input_layer-&gt;channels();</div>
<div class="line">  input_geometry_ = cv::Size(input_layer-&gt;width(), input_layer-&gt;height());</div>
<div class="line">  input_layer-&gt;Reshape(1, num_channels_,</div>
<div class="line">         input_geometry_.height, input_geometry_.width);</div>
<div class="line">  net_m.Reshape();</div>
<div class="line">  std::vector&lt;cv::Mat&gt; input_channels;</div>
<div class="line">  <span class="keywordtype">int</span> width = input_layer-&gt;width();</div>
<div class="line">  <span class="keywordtype">int</span> height = input_layer-&gt;height();</div>
<div class="line">  <span class="keywordtype">float</span>* input_data = input_layer-&gt;mutable_cpu_data();</div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; input_layer-&gt;channels(); ++i) </div>
<div class="line">  {</div>
<div class="line">    cv::Mat channel(height, width, CV_32FC1, input_data);</div>
<div class="line">    input_channels.push_back(channel);</div>
<div class="line">    input_data += width * height;</div>
<div class="line">  }</div>
<div class="line"><span class="comment">//      net_m.Forward();   # for newer versions of caffe</span></div>
<div class="line">  Blob&lt;float&gt;* output_layer = net_m.output_blobs()[0];</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">float</span>* begin = output_layer-&gt;cpu_data();</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">float</span>* end = begin + output_layer-&gt;channels();</div>
<div class="line">  <span class="keywordflow">return</span> std::vector&lt;float&gt;(begin, end);</div>
<div class="line">}</div>
</div><!-- fragment --><p><b>The issue was that irrespective of the input, the output remained same. This issues still unresolved</b></p>
<p>As a temporary resolt, the following python wrapper was called, </p>
<div class="fragment"><div class="line"><span class="keyword">import</span> numpy as np</div>
<div class="line"><span class="keyword">import</span> caffe</div>
<div class="line"><span class="keyword">import</span> sys</div>
<div class="line"><span class="keyword">import</span> cv2</div>
<div class="line"></div>
<div class="line"><span class="keywordflow">if</span>(sys.argv[5] == <span class="stringliteral">&quot;gpu&quot;</span>):</div>
<div class="line">  caffe.set_mode_gpu()</div>
<div class="line">  caffe.set_device(0)</div>
<div class="line"><span class="keywordflow">else</span>:</div>
<div class="line">  caffe.set_mode_cpu()</div>
<div class="line"></div>
<div class="line"></div>
<div class="line">net = caffe.Net(sys.argv[1], sys.argv[2], caffe.TEST)</div>
<div class="line"></div>
<div class="line">transformer = caffe.io.Transformer({<span class="stringliteral">&#39;data&#39;</span>: net.blobs[<span class="stringliteral">&#39;data&#39;</span>].data.shape})</div>
<div class="line">transformer.set_transpose(<span class="stringliteral">&#39;data&#39;</span>,(2,0,1))</div>
<div class="line"></div>
<div class="line">img = caffe.io.load_image (sys.argv[3])</div>
<div class="line">net.blobs[<span class="stringliteral">&#39;data&#39;</span>].data[...] = transformer.preprocess(<span class="stringliteral">&#39;data&#39;</span>, img)</div>
<div class="line"></div>
<div class="line">out = net.forward ()</div>
<div class="line">predicts = out[<span class="stringliteral">&#39;prob&#39;</span>]</div>
<div class="line"></div>
<div class="line">print <span class="stringliteral">&quot;Predicted label:&quot;</span></div>
<div class="line">predicts = predicts*255</div>
<div class="line">print predicts[0]</div>
<div class="line"></div>
<div class="line">img = cv2.imread(sys.argv[3])</div>
<div class="line">f = open(sys.argv[4],<span class="charliteral">&#39;w&#39;</span>)</div>
<div class="line">for i in range(15):</div>
<div class="line">  x = (predicts[0][2*i])</div>
<div class="line">  y = (predicts[0][2*i+1])</div>
<div class="line">  x = <span class="keywordtype">int</span>(x)</div>
<div class="line">  y = <span class="keywordtype">int</span>(y)</div>
<div class="line">  cv2.circle(img, (x,y), 1, (255,255,255), 2, 8, 0)</div>
<div class="line">  stringVal = str(x) + &quot; &quot; + str(y)</div>
<div class="line">  f.write(stringVal)</div>
<div class="line">  f.write(&#39;\n&#39;)</div>
<div class="line"></div>
<div class="line">cv2.imshow(&quot;img&quot;, img)</div>
<div class="line">cv2.waitKey(0)</div>
</div><!-- fragment --><p>This wrapper code was called from the function of ODConvClassification, </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvClassification::runMultiClassClassifierPythonMode()</div>
<div class="line">{</div>
<div class="line">  <span class="keywordtype">string</span> mode = <span class="stringliteral">&quot;&quot;</span>;</div>
<div class="line"><span class="preprocessor">  #if(WITH_GPU)</span></div>
<div class="line"><span class="preprocessor"></span>  mode = <span class="stringliteral">&quot;gpu&quot;</span>;</div>
<div class="line"><span class="preprocessor">  #else</span></div>
<div class="line"><span class="preprocessor"></span>  mode = <span class="stringliteral">&quot;cpu&quot;</span>;</div>
<div class="line"><span class="preprocessor">  #endif</span></div>
<div class="line"><span class="preprocessor"></span>  <span class="keywordtype">string</span> cmd = <span class="stringliteral">&quot;python ../examples/objectdetector/AAM_Classify/classify.py &quot;</span> + networkFileLocation + <span class="stringliteral">&quot; &quot;</span> + weightModelFileLoaction + <span class="stringliteral">&quot; &quot;</span> + imageFileLocation + <span class="stringliteral">&quot; &quot;</span> + outputFileLocation + <span class="stringliteral">&quot; &quot;</span> + mode;</div>
<div class="line">  system(cmd.c_str());</div>
<div class="line">}</div>
</div><!-- fragment --><p> The calling was done using system() function.</p>
<p>The example, examples/objectdetector/od_cnn_aam_classification_python_mode.cpp, like the mnist classify example has user help option present. The detected points are posted to a file "output.txt", which is stored in the folder, examples/objectdetector/AAM_Classify.</p>
<p>Happy Coding!!!</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit7"></a>
Commit 7 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/e4c931112f1047a8f81695d30f1f81620eb54f67">link to commit:Annotator Version 1 Added</a>, was issued in order to introduce a image annotation tool. Annotation tool is supposed to be very important part of object detection. Almost every cnn based object detection training and classification involves annotation of the datatset. If it is included in the library itself then the users work of fetching annotator from outside would be cleared off.</p>
<h2><a class="anchor" id="commit7_first"></a>
7.1) Features and usage of the version 1 of the annotator </h2>
<p>The features and some usage points involved are:</p>
<ul>
<li>User may load a single image from a location using the "Select the image location" button or the user may point towards a complete image dataset folder.</li>
<li>Even if the user points to a dataset folder, there exists an option of choosing an image from some another location while the annotation process is still on.</li>
<li>Even if user selects a single image, the user may load more single images without changing the type of annotation.</li>
<li>The first type of annotation facility is, annotating one bounding box per image.</li>
<li>The second, annotating and cropping one bounding box per image.</li>
<li>The third one, annotating multiple bounding boxes per image, with attached labels.</li>
<li>If a user makes mistake in annotation, the annotation can be reset too.</li>
</ul>
<p><b>Note: Every image that is loaded, is resized to 640x480 dimensions, but the output file has points of the bounding boxes as the original image size</b></p>
<p>The output files generated in the three cases have annotation details as,</p>
<ul>
<li>First case, every line in the output text file has a image name followed by four points x1 y2 x2 y2, first two representing top left coordinate of the box and the last two representing bottom right coordinates of the box.</li>
<li>Second case, every line in the output text file has a image name followed by four points x1 y2 x2 y2, first two representing top left coordinate of the box and the last two representing bottom right coordinates of the box. The cropped images are stored in the same folder as the original image, with name, &lt;original_image_name&gt;_cropped.&lt;extension_of_the_original_image&gt;</li>
<li>Third case, every line in the output text file has a image name followed by a lebel and then the four points x1 y2 x2 y2, first two representing top left coordinate of the box and the last two representing bottom right coordinates of the box. If there are multiple bounding boxes, then after image name there is a label, then four points, followed another label, and the corresponding four points and so on.</li>
</ul>
<p>To select any of these cases, select the image/dataset and then press the "Load the image" button.</p>
<p><b>First case usage</b></p>
<ul>
<li>Select the image or the dataset folder.</li>
<li>Press the "Load the image" button.</li>
<li>To create any roi, first left click on top left point of the supposed roi and then right click on the bottom right point of the supposed roi. A green rectangular box will appear.</li>
<li>Now, if its not the one you meant it, please click "Reset Markings" Button and repoint the new roi.</li>
<li>If the ROI is fine, press "Select the ROI" button.</li>
<li>Now, load another image or save the file.</li>
</ul>
<p><b>Second case usage</b></p>
<ul>
<li>Select the image or the dataset folder.</li>
<li>Press the "Load the image" button.</li>
<li>To create any roi, first left click on top left point of the supposed roi and then right click on the bottom right point of the supposed roi. A green rectangular box will appear.</li>
<li>Now, if its not the one you meant it, please click "Reset Markings" Button and repoint the new roi.</li>
<li>If the ROI is fine, press "Select the ROI" button.</li>
<li>Now, load another image or save the file.</li>
</ul>
<p><b>Third case usage</b></p>
<ul>
<li>Select the image or the dataset folder.</li>
<li>Press the "Load the image" button.</li>
<li>To create any roi, first left click on top left point of the supposed roi and then right click on the bottom right point of the supposed roi. A green rectangular box will appear.</li>
<li>Now, if its not the one you meant it, please click "Reset Markings" Button and repoint the new roi.</li>
<li>If the ROI is fine, please type an <b>integer</b> label in the text box and press "Select the ROI" button.</li>
<li>Now, you may draw another roi, or load another image, save the file.</li>
</ul>
<p><b>Note: In the third case, the one with multiple ROIs per image, if a boundix box is selected for an image and you are trying to make another and press the reset button, the selected roi will not be deleted. Any selected roi cannot be deleted as of now.</b></p>
<p><br/>
 </p>
<h2><a class="anchor" id="commit7_second"></a>
7.2) Annotation base class </h2>
<p>The base class for the annotator tool is "annotation" presented in the files detectors/global2D/annotation/ODAnnotation.cpp and <a class="el" href="_o_d_annotation_8h_source.html">detectors/global2D/annotation/ODAnnotation.h</a>. It has already been stated as to how the base elements, buttons, text boxes, labels, grids, dropdown menus, etc are implemented. In this commit, for the annotator, the new stuffs were mouse_click_event and selecting folder/file. The view showing rectangular ROIs and croppings are done using OpenCV libraries.</p>
<p><b>Mouse Click Events</b></p>
<p>The on_vbutton_pree function from the file <a class="el" href="_o_d_annotation__image_load_window_8h_source.html">detectors/global2D/annotation/ODAnnotation_imageLoadWindow.h</a> sums up the mouse events, </p>
<div class="fragment"><div class="line"><span class="keywordtype">bool</span> <a class="code" href="classannotation.html#a617e7a4244f077ef8bb7937afce5ff1f">annotation::on_button_press_event</a>(GdkEventButton *event)</div>
<div class="line">{</div>
<div class="line">  <span class="comment">// Check if the event is a left button click.</span></div>
<div class="line">  <span class="keywordflow">if</span> (event-&gt;button == 1)</div>
<div class="line">  {</div>
<div class="line">    <span class="comment">// Memorize pointer position</span></div>
<div class="line">    lastXMouse=<span class="keyword">event</span>-&gt;x;</div>
<div class="line">    lastYMouse=<span class="keyword">event</span>-&gt;y;</div>
<div class="line">    xPressed=<span class="keyword">event</span>-&gt;x;</div>
<div class="line">    yPressed=<span class="keyword">event</span>-&gt;y;</div>
<div class="line">    <span class="comment">// Start moving the view</span></div>
<div class="line">    moveFlag=<span class="keyword">true</span>;</div>
<div class="line">    <span class="comment">// Event has been handled</span></div>
<div class="line">    cout &lt;&lt; lastXMouse &lt;&lt; <span class="stringliteral">&quot; &quot;</span> &lt;&lt; lastYMouse &lt;&lt; endl;</div>
<div class="line"></div>
<div class="line">    <span class="keywordflow">return</span> <span class="keyword">true</span>;</div>
<div class="line">  } </div>
<div class="line">  <span class="comment">// Check if the event is a right button click.</span></div>
<div class="line">  <span class="keywordflow">if</span>(event-&gt;button == 3)</div>
<div class="line">  {</div>
<div class="line">    <span class="comment">// Memorize mouse coordinates</span></div>
<div class="line">    lastXMouse=<span class="keyword">event</span>-&gt;x;</div>
<div class="line">    lastYMouse=<span class="keyword">event</span>-&gt;y;</div>
<div class="line">    xReleased=<span class="keyword">event</span>-&gt;x;</div>
<div class="line">    yReleased=<span class="keyword">event</span>-&gt;y;</div>
<div class="line">    <a class="code" href="classannotation.html#a3967774ad72a66618e1748e3b7f9aee6">createVisualROI</a>(xPressed-10, yPressed-35, xReleased-10, yReleased-35);</div>
<div class="line">    <span class="comment">// Display the popup menu</span></div>
<div class="line"><span class="comment">//    m_Menu_Popup.popup(event-&gt;button, event-&gt;time);</span></div>
<div class="line">    <span class="comment">// The event has been handled.</span></div>
<div class="line">    <span class="keywordflow">return</span> <span class="keyword">true</span>;</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">return</span> <span class="keyword">false</span>;</div>
<div class="line">}</div>
</div><!-- fragment --><p><b>Selecting a file/folder</b></p>
<p>With gtkmm, a file chooser can be created using the function below, </p>
<div class="fragment"><div class="line">Gtk::FileChooserDialog dialog(<span class="stringliteral">&quot;Please choose a file&quot;</span>,Gtk::FILE_CHOOSER_ACTION_OPEN);</div>
<div class="line">dialog.set_transient_for(*<span class="keyword">this</span>);</div>
<div class="line"></div>
<div class="line"><span class="comment">//Add response buttons the the dialog:</span></div>
<div class="line">dialog.add_button(<span class="stringliteral">&quot;_Cancel&quot;</span>, Gtk::RESPONSE_CANCEL);</div>
<div class="line">dialog.add_button(<span class="stringliteral">&quot;_Open&quot;</span>, Gtk::RESPONSE_OK);</div>
<div class="line"></div>
<div class="line"><span class="comment">//Add filters, so that only certain file types can be selected:</span></div>
<div class="line">Glib::RefPtr&lt;Gtk::FileFilter&gt; filter_any = Gtk::FileFilter::create();</div>
<div class="line">filter_any-&gt;set_name(<span class="stringliteral">&quot;Any files&quot;</span>);</div>
<div class="line">filter_any-&gt;add_pattern(<span class="stringliteral">&quot;*&quot;</span>);</div>
<div class="line">dialog.add_filter(filter_any);</div>
<div class="line"></div>
<div class="line">Glib::RefPtr&lt;Gtk::FileFilter&gt; filter_text = Gtk::FileFilter::create();</div>
<div class="line">filter_text-&gt;set_name(<span class="stringliteral">&quot;Text files&quot;</span>);</div>
<div class="line">filter_text-&gt;add_mime_type(<span class="stringliteral">&quot;text/plain&quot;</span>);</div>
<div class="line">dialog.add_filter(filter_text);</div>
<div class="line"></div>
<div class="line">Glib::RefPtr&lt;Gtk::FileFilter&gt; filter_cpp = Gtk::FileFilter::create();</div>
<div class="line">filter_cpp-&gt;set_name(<span class="stringliteral">&quot;C/C++ files&quot;</span>);</div>
<div class="line">filter_cpp-&gt;add_mime_type(<span class="stringliteral">&quot;text/x-c&quot;</span>);</div>
<div class="line">filter_cpp-&gt;add_mime_type(<span class="stringliteral">&quot;text/x-c++&quot;</span>);</div>
<div class="line">filter_cpp-&gt;add_mime_type(<span class="stringliteral">&quot;text/x-c-header&quot;</span>);</div>
<div class="line">dialog.add_filter(filter_cpp);</div>
<div class="line"></div>
<div class="line"></div>
<div class="line"></div>
<div class="line"><span class="comment">//Show the dialog and wait for a user response:</span></div>
<div class="line"><span class="keywordtype">int</span> result = dialog.run();</div>
<div class="line"></div>
<div class="line"><span class="comment">//Handle the response:</span></div>
<div class="line"><span class="keywordflow">switch</span>(result)</div>
<div class="line">{</div>
<div class="line">  <span class="keywordflow">case</span>(Gtk::RESPONSE_OK):</div>
<div class="line">  {</div>
<div class="line">    <span class="comment">// The user selected a file</span></div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Open clicked.&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">    filename = dialog.get_filename();</div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;File selected: &quot;</span> &lt;&lt;  filename &lt;&lt; std::endl;</div>
<div class="line">    <span class="keywordflow">break</span>;</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">case</span>(Gtk::RESPONSE_CANCEL):</div>
<div class="line">  {</div>
<div class="line">    <span class="comment">// The user clicked cancel</span></div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Cancel clicked.&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">    <span class="keywordflow">break</span>;</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">default</span>:</div>
<div class="line">  {</div>
<div class="line">    <span class="comment">// The user closed the dialog box</span></div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Unexpected button clicked.&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">    <span class="keywordflow">break</span>;</div>
<div class="line">  }</div>
<div class="line">}</div>
</div><!-- fragment --><p>A similar code can be used to select a folder, with a slight change in the event as shown below, </p>
<div class="fragment"><div class="line">Gtk::FileChooserDialog dialog(<span class="stringliteral">&quot;Please choose a file&quot;</span>,Gtk::FILE_CHOOSER_ACTION_SELECT_FOLDER);</div>
</div><!-- fragment --><p><br/>
 </p>
<h2><a class="anchor" id="commit7_third"></a>
7.3)Upper Annotator Class </h2>
<p>The ODAnnotator Class invokes the gtkmm object of class annotation, from files detectors/global2D/annotation/ODAnnotator.cpp and <a class="el" href="_o_d_annotator_8h_source.html">detectors/global2D/annotation/ODAnnotator.h</a>. This ODAnnotator class is derived from public elements of ODTrainer class. The invoking is done as,</p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> ODAnnotator::startAnnotator(<span class="keywordtype">int</span> argc, <span class="keywordtype">char</span> *argv[])</div>
<div class="line">{</div>
<div class="line">  <span class="keyword">auto</span> app = Gtk::Application::create(argc, argv, <span class="stringliteral">&quot;org.gtkmm.example&quot;</span>);</div>
<div class="line">  <a class="code" href="classannotation.html">annotation</a> Annotation;</div>
<div class="line">  Annotation.set_default_geometry (10000, 10000);</div>
<div class="line">  app-&gt;run(Annotation);</div>
<div class="line">}</div>
</div><!-- fragment --><p>The object of ODAnnotator class is created in the example, examples/objectdetector/od_image_annotator.cpp.</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit8"></a>
Commit 8 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/872cd3271d0d372437155eece729f757abec157f">link to commit:HDF5 Layer added to network creator</a>, was issued to add HDF5 data layer to the network creator. Many researchers have the habit of using data in HDF5 format and is currently supported by the caffe library. The commit mentioned changes in two files and this layer was added to the extraLayer type layers.</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit9"></a>
Commit 9 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/3001f4f920d36e9fd4b52870aa50e144406bb121">link to commit:Saving mode added to network-creator</a>, was issued to rectify the issue that, while saving the layers from network creator, nothing was being actually written to the file. The commit marked changes in one file, detectors/global2D/training/network.cpp.</p>
<p>The following was added to the function where button which saved the file was pressed, </p>
<div class="fragment"><div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> i = 0; i &lt; numLayers; i++)</div>
<div class="line">{</div>
<div class="line">  myfile &lt;&lt; fullCnnLayers[i]; </div>
<div class="line">}</div>
</div><!-- fragment --><p><br/>
 </p>
<h1><a class="anchor" id="commit10"></a>
Commit 10 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/2d23a32ca381de52b53bcf6c5bff6fa741763ed5">link to commit:Delete a particular layer</a>, was issued to rectify the issue that, the network creator only facilitated deletion of only the latest created layer. It caused difficulties such that a user who might want to delete the second layer had to technically delete the entire network. Thus, this commit, with changes in 6 files resolved the issue.</p>
<p><br/>
 </p>
<h2><a class="anchor" id="commit10_first"></a>
10.1) How to delete a particular layer in network creator </h2>
<p>This can be done, while displaying the network in the display window, below the textview there will be a dropdown list. The list has the name of the layers that have been pushed into the network. Select the layer, and press the button "Delete Selected Layer". Immediate changes will appear on the display window text view space.</p>
<p>The dropdown list is designed in a way that its dynamic to the situation. Once the layer is deleted, its presence from the dropdown list also vanishes, </p>
<div class="fragment"><div class="line">ref_currentLayers-&gt;clear();</div>
<div class="line">combo_currentLayers.set_model(ref_currentLayers);</div>
<div class="line">Gtk::TreeModel::Row row_currentLayers = *(ref_currentLayers-&gt;append());</div>
<div class="line">row_currentLayers[column_currentLayers.m_col_id] = 0;</div>
<div class="line">row_currentLayers[column_currentLayers.m_col_name] = <span class="stringliteral">&quot;Layers&quot;</span>;</div>
<div class="line">row_currentLayers[column_currentLayers.m_col_extra] = <span class="stringliteral">&quot;All Layers&quot;</span>;</div>
<div class="line">combo_currentLayers.set_active(row_currentLayers);</div>
<div class="line"><span class="keywordflow">for</span>(<span class="keywordtype">int</span> i = 0; i &lt; numLayers; i++)</div>
<div class="line">{</div>
<div class="line">  Gtk::TreeModel::Row row_currentLayers = *(ref_currentLayers-&gt;append());</div>
<div class="line">  row_currentLayers[column_currentLayers.m_col_id] = i + 1;</div>
<div class="line">  row_currentLayers[column_currentLayers.m_col_name] = fullCnnLayersName[i];</div>
<div class="line">}</div>
</div><!-- fragment --><p> The dynamic nature is obtained by the code snippet above added to the file <a class="el" href="display_window_8h_source.html">detectors/global2D/training/displayWindow.h</a>.</p>
<p>The pointer and the handle to select a particular layer, append to network, delete particular layers has been brought down by, </p>
<div class="fragment"><div class="line"><span class="keyword">struct </span><a class="code" href="struct_node.html">Node</a> {</div>
<div class="line">    Glib::ustring <a class="code" href="struct_node.html#afa559ea7058661196a500141f93c9205">data</a>;</div>
<div class="line">  Glib::ustring <a class="code" href="struct_node.html#aa22f493762c9c569378ac0bb7da54974">name</a>;</div>
<div class="line">    <a class="code" href="struct_node.html">Node</a>* <a class="code" href="struct_node.html#aaa9e33aa02bd8bcd9282842aa799f2c3">nextLayer</a>;</div>
<div class="line">};</div>
</div><!-- fragment --><p> adding the name section, a type of id, to each layer. Thus all the functions, involving the linkedlist in the file <a class="el" href="node_8h_source.html">detectors/global2D/training/node.h</a> had to be simultaneously changed.</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit11"></a>
Commit 11 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/5407fce78a474d06e22277ec7c185f432ee389bf">link to commit:Now layers in network creator can be inserted in between layers too</a>, was issued to add a new important feature to the network creator. It is quite common while experimenting with cnn that, researchers want to insert a layer in betweem two layers, thus to enable this feature four files have been modified in this commit.</p>
<h2><a class="anchor" id="commit11_first"></a>
11.1) Using the feature of adding layers in between already created layers in network creator</h2>
<p>The pointer to unique id/name had already been created in the previous commit. To make use of this feature, on the display window, select the layer after which you want a new layer. Then press "Add Layer after Selected Layer" button. Then as normal, select a layer and add it.</p>
<p><b>Make sure that only one layer is added</b></p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit12"></a>
Commit 12 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/2472314cb8881afdded02023be1529eaf3a984c9">link to commit:Multiple cropper from same image with labels added</a>, was issued to add a new important feature to the Annotator.</p>
<h2><a class="anchor" id="commit12_first"></a>
12.1) Cropping multiple sections from same image</h2>
<p>With a total of around 200 additions to 4 code source files, this feature would enable user to extract multiple rectangular ROIs from the same same with an attached label with each cropping</p>
<ul>
<li>Select the image or the dataset folder.</li>
<li>Press the "Load the image" button.</li>
<li>To create any roi, first left click on top left point of the supposed roi and then right click on the bottom right point of the supposed roi. A green rectangular box will appear.</li>
<li>Now, if its not the one you meant it, please click "Reset Markings" Button and repoint the new roi.</li>
<li>If the ROI is fine, please type an <b>integer</b> label in the text box and press "Select the ROI" button.</li>
<li>Now, you may draw another roi, or load another image, save the file.</li>
<li>Once the file is saved, the cropped images will be saved in the same forlder as the original image with name as &lt;original_image_name&gt;_cropped_&lt;label&gt;_&lt;unique_serial_id&gt;.&lt;extension_of_the_original_image&gt;</li>
</ul>
<p><br/>
 </p>
<h1><a class="anchor" id="commit13"></a>
Commit 13 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/6ba8bf4dd640e965cfd75d026d5feb2a8aab03a6">link to commit:With the rectangular boxes demo on images corresponding labels will also be shown</a>, was issued to add a new important feature to the Annotator. While annotating with labels on a single image, to enable easeness to the user, labels would also appear with the selected ROIs. This was made possible with </p>
<div class="fragment"><div class="line">std::string text = text_annotationLabel.get_text();</div>
<div class="line"><span class="keywordtype">int</span> fontFace = FONT_HERSHEY_SCRIPT_SIMPLEX;</div>
<div class="line"><span class="keywordtype">double</span> fontScale = 1;</div>
<div class="line"><span class="keywordtype">int</span> thickness = 2;  </div>
<div class="line">cv::Point textOrg(x1, y1);</div>
<div class="line">cv::putText(img, text, textOrg, fontFace, fontScale, Scalar(0,0,255), thickness,8);</div>
</div><!-- fragment --><p> the above snippet in the file <a class="el" href="_o_d_annotation__image_load_window_8h_source.html">detectors/global2D/annotation/ODAnnotation_imageLoadWindow.h</a>, and a few other changes two other files.</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit14"></a>
Commit 14 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/147823e1d082f45fa41ef38f3718a5843c55465c">link to commit:Segnet classifier python version added</a>, was issued to introduce a link to segnet library, a derivative of caffe library. This would allow segnet library users to attach it to opendetection in way as done with caffe library.</p>
<p>To get a better understanding of segnet please refer to <a href="http://mi.eng.cam.ac.uk/projects/segnet/">link here</a></p>
<p>This commit shows a python wrapper based example of segmenting an image using Segnet. The trained model, network file, and the image dataset have been incorporated from <a href="http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html">link here</a></p>
<p>The reason behind putting a python wrapper code into play is the same issue we had while adding AAM classifier (methioned above the blog). The issue is left unresolved.</p>
<p>The python wrapper is called from ODConvClassification class, </p>
<div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvClassification::runSegnetBasedClassifierPythonMode()</div>
<div class="line">{</div>
<div class="line">  <span class="keywordtype">string</span> mode = <span class="stringliteral">&quot;&quot;</span>;</div>
<div class="line"><span class="preprocessor">  #if(WITH_GPU)</span></div>
<div class="line"><span class="preprocessor"></span>  mode = <span class="stringliteral">&quot;gpu&quot;</span>;</div>
<div class="line"><span class="preprocessor">  #else</span></div>
<div class="line"><span class="preprocessor"></span>  mode = <span class="stringliteral">&quot;cpu&quot;</span>;</div>
<div class="line"><span class="preprocessor">  #endif</span></div>
<div class="line"><span class="preprocessor"></span>  <span class="keywordtype">string</span> cmd = <span class="stringliteral">&quot;python ../examples/objectdetector/Segnet_Classify/test.py &quot;</span> + segnetLocation + <span class="stringliteral">&quot; &quot;</span> + networkFileLocation + <span class="stringliteral">&quot; &quot;</span> + weightModelFileLoaction + <span class="stringliteral">&quot; &quot;</span> + imageFileLocation + <span class="stringliteral">&quot; &quot;</span> + imageGroundTruthFileLocation + <span class="stringliteral">&quot; &quot;</span> + colorLocation + <span class="stringliteral">&quot; &quot;</span> + mode;</div>
<div class="line">  system(cmd.c_str());</div>
<div class="line">}</div>
</div><!-- fragment --><p>And the python wrapper is test.py in the folder, examples/objectdetector/Segnet_Classify. This is untimately called out from the file examples/objectdetector/od_cnn_segnet_classification_python_mode.cpp like the amm classifier example.</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit15"></a>
Commit 15 </h1>
<p>This commit, <a href="https://github.com/abhi-kumar/opendetection/commit/5927b27eb28f329b491b539accf75fe3958d127b">link to commit:New feature to annotator: Mask(Segnet), or say non-rectangular roi based annotation with attached labels</a>, was issued to add a new feature to the Annotator.</p>
<h2><a class="anchor" id="commit15_first"></a>
15.1) Non rectangualr ROI with label using annotator</h2>
<p>In segnet, and many other places, annotating an object might require a non rectangular ROI to be marked up. This commit enables the user to mark multiple non rectangular ROIs over an image with attached labels.</p>
<ul>
<li>Select the image or the dataset folder.</li>
<li>Press the "Load the image" button.</li>
<li>To create any roi, Click on the points needed only with left click.</li>
<li>Now, if its not the one you meant it, please click "Reset Markings" Button and repoint the new roi.</li>
<li>If the ROI is fine, please type an <b>integer</b> label in the text box and press "Select the ROI" button. A gree color marking covering the region and passing through the points you have selected will appear.</li>
<li>Now, you may draw another roi, or load another image, save the file.</li>
<li>The output of this file will be saved as filename, followed by an unique id to the ROI, label of the roi, set of points in the roi, then again another id, its label and the points and so on.</li>
</ul>
<p>This was made feasible with around 300 additions to 4 source code files.</p>
<p><br/>
 </p>
<h1><a class="anchor" id="commit16_17_18"></a>
Commits 16, 17 and 18 </h1>
<p>These commits, were made to add popup messages whenever any event is occured, or user does an error while using thr GUIs, Solver Maker, Network Creator and Annotator.</p>
<p>The links to commits:</p>
<ul>
<li><a href="https://github.com/abhi-kumar/opendetection/commit/c21845efa5ccd3db74e1922173a3383e11418d5c">Commit 16: User oriented message boxes added to solver maker</a></li>
<li><a href="https://github.com/abhi-kumar/opendetection/commit/27193588f1b64e100e7154a752f543dc353f834d">Commit 17: User oriented popups added to network creator</a></li>
<li><a href="https://github.com/abhi-kumar/opendetection/commit/0997d5b3a54763541c8478116c0263a7e60c40b7">Commit 18: Added user oriented popups to Annotator</a></li>
</ul>
<p><br/>
 </p>
<h1><a class="anchor" id="commit19"></a>
Conclusion</h1>
<p>Gsoc Documentation added: <a href="https://github.com/abhi-kumar/opendetection/commit/67499c2410da377ea10aa59dea7a4bec8cf8cbd0">Commit 19: Complete blog with link to commits</a></p>
<p>The above is a link to show commits which has links to all the commits -&gt; RECURSION LEVEL INFINITE</p>
<p>This marks a pseudo end to the GSOC term.</p>
<p>The work term of Google Summer Of Code 2016, has had a successful build for the components</p>
<ul>
<li>Building the library on CPU as well as GPU Platforms</li>
<li>Integration of caffe library</li>
<li>Addition of image classification module through c++ version of caffe.</li>
<li>Addition of CNN training module through c++ version of caffe.</li>
<li>A customized GTKMM based GUI for creating solver file for training.</li>
<li>A customized GTKMM based GUI for creating a network file for training/testing.</li>
<li>A customized GTKMM based GUI for annotating/cropping an image file.</li>
<li>A Segnet library based image classifier using a python wrapper.</li>
<li>An AAM model prediction for face images using caffe library.</li>
<li>Selective Search based object localization algorithm.</li>
</ul>
<p><b>Work left to be completed:</b></p>
<p>a) Resolve the issue of cpp version of AAM and segnet based classifier</p>
<p>b) Heat map generator using cnn ( will require time as its is quite research intensive part)</p>
<p>c) Work to be integrated with Giacomo's work and to be pushed to master.</p>
<p>d) API Documentation for the codes added.</p>
<p><br/>
</p>
<h1><a class="anchor" id="oldBlog"></a>
Older sections of the blog</h1>
<h2><a class="anchor" id="gsoc2016_blog_abhishek1"></a>
CNN based object localization and recognition for openDetection library  </h2>
<ul>
<li><a href="https://storage.googleapis.com/summerofcode-prod.appspot.com/gsoc/core_project/doc/1458909012_ProposalGSoC16.pdf?Expires=1463646545&amp;GoogleAccessId=summerofcode-prod%40appspot.gserviceaccount.com&amp;Signature=u8ENsakhMhWcA16h57rY8BzZ9M1NoUOxmRRUSJfSaEjJc%2Fce%2FoknY4K3%2FuQTJI3r0kuJF2P5WcwKYNOJd8zkIo6YDtbqQ5Be%2Bz1jXTbfsUGpN7UvQwElqKlAcAVcKxMPq1KkNpJFzPQhm8tDl9wYyFzoSnV1kLgpBMesbWV3vggt12PFo8IYkbpD4esd%2B9kHK64iNG5AykTJzL0CrMQcPK1a4WLrzBcgS8Dp2z327Ri8W1BtRC1SF4pveEmd6daAuRK3zfVkq8B7NNsvvk7x3QM2lg9WTlrFyTzDNOkp0k0cByusZ5fvZ20eQHbqmsKM8skE7XT0fX98xoiED8bauA%3D%3D">Link to Proposal</a> - <a href="https://summerofcode.withgoogle.com/projects/#5234791976796160">Link to GSoC2016 Project Page</a></li>
</ul>
<p><b>About me</b></p>
<p>I am a final year engineering student from India, pursuing Electrical and Electronics Engineering at Bits-Pilani Goa Campus. Since my first year at the college, I have been interested in the fields of Computer Vision, Machine Learning and Artificial Intelligence. I have completed my undergraduate thesis at Research and Division Labs of Tata Elxsi Pvt Ltd, on the topic "Scene-understanding and object classification using neural networks for autonomous robot navigation". Over the tenure of engineering in the past three and a half years I have worked over a few projects,</p>
<ul>
<li>Using Weber Local Descriptors to match forensics sketeches with their image counterparts</li>
<li>Implementing a new course, on biomedical image processing, which is supposed to be added to the college's academic curriculum</li>
<li>Vehicle detection and tracking</li>
<li>Analysing haar-cascades on face detection application</li>
</ul>
<p><b>Project</b></p>
<p>The project is revolved over integrating object detection and classification module using Convolutional Neural Networks. The following shows the basic components of the work to be completed during the term of Google Summer Of Code, 2016</p>
<ul>
<li>Implement a way to invoke Caffe open source library from the OpenDetection module with a user-friendly code based way ( this will include a tinge of GUI support for instant access)</li>
<li>Implement open source guidance and codes for state-of-the art object localization problems(hypothesis generation) specifically based on selective-search and convolutional neural network (CNN) approaches.</li>
<li>Adding a ground-truth annotation tool to the module with a graphical-user-interface support.</li>
<li>Implementing short, but effective modules like mixed-pooling, recurrent networks to the Convolutional Neural Networks Training dependent on the invoked caffe library.</li>
<li>Adding context based learning CNNs.</li>
<li>Adding user-interface to train and test CNN based classifiers and object detectors.</li>
<li>Adding documentation for the above</li>
</ul>
<p>All the completed and on-going work will be explained in detail here, as the process moves forward.</p>
<p>Happy Coding!!!!</p>
<h2><a class="anchor" id="target1"></a>
Classification of digits in Mnist Library using CNN </h2>
<p>The classification example added to the library involves usage of caffe library. The modalities and usage of the libraries can be studied at</p>
<ul>
<li><a href="http://caffe.berkeleyvision.org/">Caffe Library</a></li>
<li><a href="https://abhishek4273.com/2016/02/07/ann-chapter-3-deep-learning-using-caffe-python/">Using Caffe Library</a></li>
<li><a href="https://github.com/abhi-kumar/opendetection/tree/cnn_cpu">Branch of OpenDetection for the below classifier</a></li>
</ul>
<p>This example involves inclusion of three new files:</p>
<ul>
<li>"opendetection/examples/objectdetector/od_cnn_mnist_classification.cpp"</li>
<li>"opendetection/detectors/global2D/detection/ODConvClassification.cpp"</li>
<li>"opendetection/detectors/global2D/detection/ODConvClassification.h"</li>
</ul>
<p>The Classification example has been implemented over the ODDetector2D class. The new ODConvClassification class inherits from the abstract class ODDetector2D under the namespace <a class="el" href="namespaceod_1_1g2d.html">od::g2d</a>. LEts go over each file briefly.</p>
<p><b><a class="el" href="_o_d_conv_classification_8h_source.html">ODConvClassification.h</a> &amp; ODConvClassification.cpp files</b></p>
<p>The file involves inclusion of the following headers. </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;cstring&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;cstdlib&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;vector&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;string&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;stdio.h&gt;</span></div>
<div class="line"></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/caffe.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/util/io.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/blob.hpp&quot;</span></div>
<div class="line"></div>
<div class="line"><span class="keyword">using namespace </span>caffe;</div>
<div class="line"><span class="keyword">using namespace </span>std;</div>
<div class="line"><span class="keyword">using namespace </span>cv;</div>
</div><!-- fragment --><p>The first set involves the basic C++ headers, while the last three headers are from the caffe library. The namespaces are</p>
<ul>
<li>caffe for Caffe Modules</li>
<li>std for C++ Standard Modules</li>
<li>cv for C++ OpenCV Modules</li>
</ul>
<p>The variables involved are as follows </p>
<div class="fragment"><div class="line"><span class="keywordtype">string</span> weightModelFileLoaction;</div>
<div class="line"><span class="keywordtype">string</span> networkFileLocation;</div>
<div class="line"><span class="keywordtype">string</span> imageFileLocation; </div>
<div class="line">Datum strucBlob;</div>
<div class="line">BlobProto protoBlob;</div>
<div class="line">vector&lt;Blob&lt;float&gt;*&gt; inputBlob;</div>
</div><!-- fragment --><ul>
<li>"weightModelFileLoaction" stores the location of the trained weight caffemodel file.</li>
<li>"networkFileLocation" stores the location of the CNN network file.</li>
<li>"imageFileLocation" stores the location of the image to be classified.</li>
<li>"strucBlob" keeps the details of the blob structure of the image to be compiled.</li>
<li>"protoBlob" creates an initial storage for the input image to be converted from image file to Caffe Blob named "inputBlob"</li>
</ul>
<p>Lets go through the functions involved in the process.</p>
<ul>
<li><div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvClassification::setWeightModelFileLocation(<span class="keywordtype">string</span> location)</div>
<div class="line">{</div>
<div class="line">  ODConvClassification::weightModelFileLoaction = location;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="keywordtype">void</span> ODConvClassification::setNetworkModelFileLocation(<span class="keywordtype">string</span> location)</div>
<div class="line">{</div>
<div class="line">  networkFileLocation = location;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="keywordtype">void</span> ODConvClassification::setImageFileLocation(<span class="keywordtype">string</span> location)</div>
<div class="line">{</div>
<div class="line">  imageFileLocation = location;</div>
<div class="line">} </div>
<div class="line"></div>
<div class="line"><span class="keywordtype">string</span> ODConvClassification::getWeightModelFileLocation()</div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Weight Model File Location = &quot;</span> &lt;&lt; weightModelFileLoaction &lt;&lt; endl;</div>
<div class="line">  <span class="keywordflow">return</span> weightModelFileLoaction;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="keywordtype">string</span> ODConvClassification::getNetworkModelFileLocation()</div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Network Model File Location = &quot;</span> &lt;&lt; networkFileLocation &lt;&lt; endl;</div>
<div class="line">  <span class="keywordflow">return</span> networkFileLocation;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="keywordtype">string</span> ODConvClassification::getImageFileLocation()</div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Image File Location = &quot;</span> &lt;&lt; imageFileLocation &lt;&lt; endl;</div>
<div class="line">  <span class="keywordflow">return</span> imageFileLocation;</div>
<div class="line">}</div>
</div><!-- fragment --> These functions are uite self explanatory. The first three functions are being used to get the location of the reuired files, while the rest are to retrieve these locations.</li>
<li><div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvClassification::setTestBlob(<span class="keywordtype">int</span> numChannels, <span class="keywordtype">int</span> imgHeight, <span class="keywordtype">int</span> imgWidth) </div>
</div><!-- fragment --> This function takes an input image and converts into a suitable format for caffe libraries.<div class="fragment"><div class="line"><span class="keywordflow">if</span> (!ReadImageToDatum(imageFileLocation, numChannels, imgHeight, imgWidth, &amp;strucBlob)) </div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Image File Not Found&quot;</span> &lt;&lt; endl;</div>
<div class="line">  exit(0);</div>
<div class="line">}</div>
<div class="line">Blob&lt;float&gt;* dataBlob = <span class="keyword">new</span> Blob&lt;float&gt;(1, strucBlob.channels(), strucBlob.height(), strucBlob.width());    </div>
</div><!-- fragment --> This snippet reads the image, and creates a structure to save the input image as a blob.<div class="fragment"><div class="line"><span class="keywordflow">if</span> (data.size() != 0) </div>
<div class="line">{</div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; sizeStrucBlob; ++i)</div>
<div class="line">  {</div>
<div class="line">    protoBlob.set_data(i, protoBlob.data(i) + (uint8_t)data[i]);</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line">dataBlob-&gt;FromProto(protoBlob);</div>
<div class="line">inputBlob.push_back(dataBlob);</div>
</div><!-- fragment --> The snippet mentioned above converts the image from the initial(".png") format to the blob format required by the caffe library.</li>
<li>The net is initailized with the network parameters and trained weights using the following snippet.<div class="fragment"><div class="line">Caffe::set_mode(Caffe::CPU);</div>
<div class="line">Net&lt;float&gt;  net(networkFileLocation, TEST);</div>
<div class="line">net.CopyTrainedLayersFrom(weightModelFileLoaction); </div>
</div><!-- fragment --> And the net is asked to move forward and present the probaility using the following snippet.<div class="fragment"><div class="line"><span class="keyword">const</span> vector&lt;Blob&lt;float&gt;*&gt;&amp; result =  net.Forward(inputBlob, &amp;type); </div>
</div><!-- fragment --> This output vector, "result", contains the probabilities for each of the classes, The class with the maximum probobiility or score is the classified class.</li>
</ul>
<p><b>The CMake Changes</b></p>
<p>The od_mandatory_dependency.cmake file has been added a new line</p>
<div class="fragment"><div class="line">find_package( Caffe REQUIRED) </div>
</div><!-- fragment --><p> And thus the inclusion of caffe include directory and caffe libraries. In the CMakeLists.txt file from detectors/global2D directory, the following snippet is added</p>
<div class="fragment"><div class="line">ADD_DEFINITIONS(</div>
<div class="line">    -std=c++11 </div>
<div class="line">  ${Caffe_DEFINITIONS}</div>
<div class="line">)</div>
</div><!-- fragment --><p> This has been done to enable mode choice of caffe runtime, i.e., CPU or GPU, and in this example CPU.</p>
<p><b>Usage</b></p>
<p>The example can be invoked using the following command: (From the build folder)</p>
<p>./examples/objectdetector/od_cnn_mnist_classification ../examples/objectdetector/Mnist_Classify/mnist.caffemodel ../examples/objectdetector/Mnist_Classify/lenet.prototxt ../examples/objectdetector/Mnist_Classify/1.png</p>
<p>The example as shown above takes 3 arguments, the locations of the weight file, network file and the image.</p>
<p><b>Next up will be a simple CNN trainer example.</b></p>
<p>Happy Coding!!!!</p>
<h2><a class="anchor" id="target2"></a>
Training a classifier for digits in Mnist Library using CNN. Part 1 </h2>
<p>This particular inclusion presents a simple trainer in a most crude and easy way possible. The major requirements of CNN training using caffe are</p>
<ul>
<li>Solver file</li>
<li>Training Network file</li>
<li>Image Dataset and a pointer to the Dataset</li>
</ul>
<p>The classification example added to the library involves usage of caffe library. The modalities and usage of the libraries can be studied at</p>
<ul>
<li><a href="http://caffe.berkeleyvision.org/">Caffe Library</a></li>
<li><a href="https://abhishek4273.com/2016/02/07/ann-chapter-3-deep-learning-using-caffe-python/">Using Caffe Library</a></li>
<li><a href="https://github.com/abhi-kumar/opendetection/tree/cnn_cpu">Branch of OpenDetection for the below classifier</a></li>
</ul>
<p>This example involves inclusion of three new files:</p>
<ul>
<li>"opendetection/examples/objectdetector/od_cnn_mnist_train_simple.cpp"</li>
<li>"opendetection/detectors/global2D/training/ODConvTrainer.cpp"</li>
<li>"opendetection/detectors/global2D/training/ODConvTrainer.h"</li>
</ul>
<p><b>Invoking Training module of caffe</b></p>
<p>These lines invoke the trainer: </p>
<div class="fragment"><div class="line">Caffe::set_mode(Caffe::CPU);</div>
<div class="line">SGDSolver&lt;float&gt; s(solverLocation);</div>
<div class="line">s.Solve();</div>
</div><!-- fragment --><p>This snippet points to solver file, the solver file points to the network file. This network file points to the file which in turn points to the dataset.</p>
<p><b>Usage</b></p>
<p>The example can be invoked using the following command: (From the build folder)</p>
<p>./examples/objectdetector/od_cnn_mnist_train_simple ../examples/objectdetector/Mnist_Train/solver1.prototxt</p>
<p>The only argument to be given is the solver file.</p>
<p><b>Next up will be a simple CNN trainer example with a graphical user interface for the solver file.</b></p>
<p>Happy Coding!!!!</p>
<h2><a class="anchor" id="target2_1"></a>
Training a classifier for digits in Mnist Library using CNN. Part 2 </h2>
<p>This commit consists of the same simple trainer from previous commit, except for the fact that it involves a graphical user interface to select solver parameters,The classification exampled added to the library involves usage of caffe library. The modalities and usage of the libraries can be studied at</p>
<ul>
<li><a href="http://caffe.berkeleyvision.org/">Caffe Library</a></li>
<li><a href="https://abhishek4273.com/2016/02/07/ann-chapter-3-deep-learning-using-caffe-python/">Using Caffe Library</a></li>
<li><a href="https://github.com/abhi-kumar/opendetection/tree/cnn_cpu">Branch of OpenDetection for the below classifier</a></li>
<li><a href="http://www.gtkmm.org/en/">GTKMM</a></li>
</ul>
<p>Installing GTKMM Required: In the terminal of ubuntu, type the following </p>
<div class="fragment"><div class="line">sudo apt-<span class="keyword">get</span> install libglib2.0-dev libatk1.0* libpango1.0-dev libcairo2-dev gdk-pixbuf2.0-0 libsigc++-2.0-dev libgtk-3-dev libcairomm-1.0-dev libpangomm-1.4-dev libatkmm-1.6-dev libgtkmm-3.0-dev </div>
</div><!-- fragment --><p><b>Usage</b></p>
<p>From the build folder invoke:</p>
<p>./examples/objectdetector/od_cnn_mnist_train_customSolver</p>
<p>Note:</p>
<ul>
<li>The path to the solver, train network, snapshot have to be set inside examples/objectdetector/Mnist_Train folder for this alpha version.</li>
<li>The GUI has a update button for every parameter. It is not necessary to press each one of them. They have been included for future, eg., when only one parameter from an existing solver need to be updated. This functionality has not been added</li>
<li>After changing the parameters, press the "Save" button and then close the window using the "x" on the top just like closing any window in Ubuntu. A custom exit has not been added yet.</li>
</ul>
<p><b>Next up will be a updates and additions to the GUI.</b></p>
<ul>
<li>Adding a provision: if a solver file exists and the user wants to change only one parameters, it can be done</li>
<li>Creation of CNN network using a simple GUI</li>
</ul>
<p>Happy Coding!!!! </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.6-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="tutorial_root.html">User Guide</a></li>
    <li class="footer">Send your queries <a href="mailto:kripasindhu.sarkar@dfki.de?Subject=OpenDetection" target="_top">here</a>.</li>
  </ul>
</div>
</body>
</html>
