<!-- HTML header for doxygen 1.8.6-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="shortcut icon" type="image/x-icon" href="odlogo_small.ico" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
    <meta name="keywords" content="object detection, object recognition, detection, recognition, vision, computer vision, image processing, point cloud, opens ource"/>
    <meta name="description" content="Open Detection, OD, is a standalone open source project for object detection and recognition in images and 3D point clouds."/>
    <meta name="author" content="Kripasindhu Sarkar"/>
<title>Open Detection: GSoC 2016 Blog - Abhishek</title>
    <link rel="stylesheet" href="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="http://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <style>
        .carousel-inner > .item > img,
        .carousel-inner > .item > a > img {
            width: 70%;
            margin: auto;
        }
    </style>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() {
    if ($('.searchresults').length > 0) { searchBox.DOMSearchField().focus(); }
  });
</script>
<link rel="search" href="search-opensearch.php?v=opensearch.xml" type="application/opensearchdescription+xml" title="Open Detection"/>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="customdoxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Open Detection
   &#160;<span id="projectnumber">1.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>OD</span></a></li>
      <li><a href="tutorial_root.html"><span>User&#160;Guide</span></a></li>
      <li><a href="usergroup0.html"><span>API&#160;Documentation</span></a></li>
      <li><a href="usergroup1.html"><span>GSoC16&#160;Blogs</span></a></li>
      <li><a href="idea_list_gsoc2016.html"><span>GSoC&#160;2016&#160;Ideas</span></a></li>
      <li><a href="installation_instruction.html"><span>Downloads</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
          <div class="left">
            <form id="FSearchBox" action="search.php" method="get">
              <img id="MSearchSelect" src="search/mag.png" alt=""/>
              <input type="text" id="MSearchField" name="query" value="Search" size="20" accesskey="S" 
                     onfocus="searchBox.OnSearchFieldFocus(true)" 
                     onblur="searchBox.OnSearchFieldFocus(false)"/>
            </form>
          </div><div class="right"></div>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('gsoc2016_blog_abhishek.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title">GSoC 2016 Blog - Abhishek </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#gsoc2016_blog_abhishek1">CNN based object localization and recognition for openDetection library  </a></li>
<li class="level1"><a href="#target1">Classification of digits in Mnist Library using CNN </a></li>
<li class="level1"><a href="#target2">Training a classifier for digits in Mnist Library using CNN. Part 1 </a></li>
<li class="level1"><a href="#target2_1">Training a classifier for digits in Mnist Library using CNN. Part 2 </a></li>
<li class="level1"><a href="#target3_1">A GUI for easy-designing of CNN networks for caffe library. Part 1 </a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="gsoc2016_blog_abhishek1"></a>
CNN based object localization and recognition for openDetection library  </h1>
<ul>
<li><a href="https://storage.googleapis.com/summerofcode-prod.appspot.com/gsoc/core_project/doc/1458909012_ProposalGSoC16.pdf?Expires=1463646545&amp;GoogleAccessId=summerofcode-prod%40appspot.gserviceaccount.com&amp;Signature=u8ENsakhMhWcA16h57rY8BzZ9M1NoUOxmRRUSJfSaEjJc%2Fce%2FoknY4K3%2FuQTJI3r0kuJF2P5WcwKYNOJd8zkIo6YDtbqQ5Be%2Bz1jXTbfsUGpN7UvQwElqKlAcAVcKxMPq1KkNpJFzPQhm8tDl9wYyFzoSnV1kLgpBMesbWV3vggt12PFo8IYkbpD4esd%2B9kHK64iNG5AykTJzL0CrMQcPK1a4WLrzBcgS8Dp2z327Ri8W1BtRC1SF4pveEmd6daAuRK3zfVkq8B7NNsvvk7x3QM2lg9WTlrFyTzDNOkp0k0cByusZ5fvZ20eQHbqmsKM8skE7XT0fX98xoiED8bauA%3D%3D">Link to Proposal</a> - <a href="https://summerofcode.withgoogle.com/projects/#5234791976796160">Link to GSoC2016 Project Page</a></li>
</ul>
<p><b>About me</b></p>
<p>I am a final year engineering student from India, pursuing Electrical and Electronics Engineering at Bits-Pilani Goa Campus. Since my first year at the college, I have been interested in the fields of Computer Vision, Machine Learning and Artificial Intelligence. I have completed my undergraduate thesis at Research and Division Labs of Tata Elxsi Pvt Ltd, on the topic "Scene-understanding and object classification using neural networks for autonomous robot navigation". Over the tenure of engineering in the past three and a half years I have worked over a few projects,</p>
<ul>
<li>Using Weber Local Descriptors to match forensics sketeches with their image counterparts</li>
<li>Implementing a new course, on biomedical image processing, which is supposed to be added to the college's academic curriculum</li>
<li>Vehicle detection and tracking</li>
<li>Analysing haar-cascades on face detection application</li>
</ul>
<p><b>Project</b></p>
<p>The project is revolved over integrating object detection and classification module using Convolutional Neural Networks. The following shows the basic components of the work to be completed during the term of Google Summer Of Code, 2016</p>
<ul>
<li>Implement a way to invoke Caffe open source library from the OpenDetection module with a user-friendly code based way ( this will include a tinge of GUI support for instant access)</li>
<li>Implement open source guidance and codes for state-of-the art object localization problems(hypothesis generation) specifically based on selective-search and convolutional neural network (CNN) approaches.</li>
<li>Adding a ground-truth annotation tool to the module with a graphical-user-interface support.</li>
<li>Implementing short, but effective modules like mixed-pooling, recurrent networks to the Convolutional Neural Networks Training dependent on the invoked caffe library.</li>
<li>Adding context based learning CNNs.</li>
<li>Adding user-interface to train and test CNN based classifiers and object detectors.</li>
<li>Adding documentation for the above</li>
</ul>
<p>All the completed and on-going work will be explained in detail here, as the process moves forward.</p>
<p>Happy Coding!!!!</p>
<h1><a class="anchor" id="target1"></a>
Classification of digits in Mnist Library using CNN </h1>
<p>The classification example added to the library involves usage of caffe library. The modalities and usage of the libraries can be studied at</p>
<ul>
<li><a href="http://caffe.berkeleyvision.org/">Caffe Library</a></li>
<li><a href="https://abhishek4273.com/2016/02/07/ann-chapter-3-deep-learning-using-caffe-python/">Using Caffe Library</a></li>
<li><a href="https://github.com/abhi-kumar/opendetection/tree/cnn_cpu">Branch of OpenDetection for the below classifier</a></li>
</ul>
<p>This example involves inclusion of three new files:</p>
<ul>
<li>"opendetection/examples/objectdetector/od_cnn_mnist_classification.cpp"</li>
<li>"opendetection/detectors/global2D/detection/ODConvClassification.cpp"</li>
<li>"opendetection/detectors/global2D/detection/ODConvClassification.h"</li>
</ul>
<p>The Classification example has been implemented over the ODDetector2D class. The new ODConvClassification class inherits from the abstract class ODDetector2D under the namespace <a class="el" href="namespaceod_1_1g2d.html">od::g2d</a>. LEts go over each file briefly.</p>
<p><b><a class="el" href="_o_d_conv_classification_8h_source.html">ODConvClassification.h</a> &amp; ODConvClassification.cpp files</b></p>
<p>The file involves inclusion of the following headers. </p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &lt;cstring&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;cstdlib&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;vector&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;string&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;stdio.h&gt;</span></div>
<div class="line"></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/caffe.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/util/io.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;caffe/blob.hpp&quot;</span></div>
<div class="line"></div>
<div class="line"><span class="keyword">using namespace </span>caffe;</div>
<div class="line"><span class="keyword">using namespace </span>std;</div>
<div class="line"><span class="keyword">using namespace </span>cv;</div>
</div><!-- fragment --><p>The first set involves the basic C++ headers, while the last three headers are from the caffe library. The namespaces are</p>
<ul>
<li>caffe for Caffe Modules</li>
<li>std for C++ Standard Modules</li>
<li>cv for C++ OpenCV Modules</li>
</ul>
<p>The variables involved are as follows </p>
<div class="fragment"><div class="line"><span class="keywordtype">string</span> weightModelFileLoaction;</div>
<div class="line"><span class="keywordtype">string</span> networkFileLocation;</div>
<div class="line"><span class="keywordtype">string</span> imageFileLocation; </div>
<div class="line">Datum strucBlob;</div>
<div class="line">BlobProto protoBlob;</div>
<div class="line">vector&lt;Blob&lt;float&gt;*&gt; inputBlob;</div>
</div><!-- fragment --><ul>
<li>"weightModelFileLoaction" stores the location of the trained weight caffemodel file.</li>
<li>"networkFileLocation" stores the location of the CNN network file.</li>
<li>"imageFileLocation" stores the location of the image to be classified.</li>
<li>"strucBlob" keeps the details of the blob structure of the image to be compiled.</li>
<li>"protoBlob" creates an initial storage for the input image to be converted from image file to Caffe Blob named "inputBlob"</li>
</ul>
<p>Lets go through the functions involved in the process.</p>
<ul>
<li><div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvClassification::setWeightModelFileLocation(<span class="keywordtype">string</span> location)</div>
<div class="line">{</div>
<div class="line">  ODConvClassification::weightModelFileLoaction = location;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="keywordtype">void</span> ODConvClassification::setNetworkModelFileLocation(<span class="keywordtype">string</span> location)</div>
<div class="line">{</div>
<div class="line">  networkFileLocation = location;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="keywordtype">void</span> ODConvClassification::setImageFileLocation(<span class="keywordtype">string</span> location)</div>
<div class="line">{</div>
<div class="line">  imageFileLocation = location;</div>
<div class="line">} </div>
<div class="line"></div>
<div class="line"><span class="keywordtype">string</span> ODConvClassification::getWeightModelFileLocation()</div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Weight Model File Location = &quot;</span> &lt;&lt; weightModelFileLoaction &lt;&lt; endl;</div>
<div class="line">  <span class="keywordflow">return</span> weightModelFileLoaction;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="keywordtype">string</span> ODConvClassification::getNetworkModelFileLocation()</div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Network Model File Location = &quot;</span> &lt;&lt; networkFileLocation &lt;&lt; endl;</div>
<div class="line">  <span class="keywordflow">return</span> networkFileLocation;</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line"><span class="keywordtype">string</span> ODConvClassification::getImageFileLocation()</div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Image File Location = &quot;</span> &lt;&lt; imageFileLocation &lt;&lt; endl;</div>
<div class="line">  <span class="keywordflow">return</span> imageFileLocation;</div>
<div class="line">}</div>
</div><!-- fragment --> These functions are uite self explanatory. The first three functions are being used to get the location of the reuired files, while the rest are to retrieve these locations.</li>
<li><div class="fragment"><div class="line"><span class="keywordtype">void</span> ODConvClassification::setTestBlob(<span class="keywordtype">int</span> numChannels, <span class="keywordtype">int</span> imgHeight, <span class="keywordtype">int</span> imgWidth) </div>
</div><!-- fragment --> This function takes an input image and converts into a suitable format for caffe libraries.<div class="fragment"><div class="line"><span class="keywordflow">if</span> (!ReadImageToDatum(imageFileLocation, numChannels, imgHeight, imgWidth, &amp;strucBlob)) </div>
<div class="line">{</div>
<div class="line">  cout &lt;&lt; <span class="stringliteral">&quot;Image File Not Found&quot;</span> &lt;&lt; endl;</div>
<div class="line">  exit(0);</div>
<div class="line">}</div>
<div class="line">Blob&lt;float&gt;* dataBlob = <span class="keyword">new</span> Blob&lt;float&gt;(1, strucBlob.channels(), strucBlob.height(), strucBlob.width());    </div>
</div><!-- fragment --> This snippet reads the image, and creates a structure to save the input image as a blob.<div class="fragment"><div class="line"><span class="keywordflow">if</span> (data.size() != 0) </div>
<div class="line">{</div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keywordtype">int</span> i = 0; i &lt; sizeStrucBlob; ++i)</div>
<div class="line">  {</div>
<div class="line">    protoBlob.set_data(i, protoBlob.data(i) + (uint8_t)data[i]);</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line"></div>
<div class="line">dataBlob-&gt;FromProto(protoBlob);</div>
<div class="line">inputBlob.push_back(dataBlob);</div>
</div><!-- fragment --> The snippet mentioned above converts the image from the initial(".png") format to the blob format required by the caffe library.</li>
<li>The net is initailized with the network parameters and trained weights using the following snippet.<div class="fragment"><div class="line">Caffe::set_mode(Caffe::CPU);</div>
<div class="line">Net&lt;float&gt;  net(networkFileLocation, TEST);</div>
<div class="line">net.CopyTrainedLayersFrom(weightModelFileLoaction); </div>
</div><!-- fragment --> And the net is asked to move forward and present the probaility using the following snippet.<div class="fragment"><div class="line"><span class="keyword">const</span> vector&lt;Blob&lt;float&gt;*&gt;&amp; result =  net.Forward(inputBlob, &amp;type); </div>
</div><!-- fragment --> This output vector, "result", contains the probabilities for each of the classes, The class with the maximum probobiility or score is the classified class.</li>
</ul>
<p><b>The CMake Changes</b></p>
<p>The od_mandatory_dependency.cmake file has been added a new line</p>
<div class="fragment"><div class="line">find_package( Caffe REQUIRED) </div>
</div><!-- fragment --><p> And thus the inclusion of caffe include directory and caffe libraries. In the CMakeLists.txt file from detectors/global2D directory, the following snippet is added</p>
<div class="fragment"><div class="line">ADD_DEFINITIONS(</div>
<div class="line">    -std=c++11 </div>
<div class="line">  ${Caffe_DEFINITIONS}</div>
<div class="line">)</div>
</div><!-- fragment --><p> This has been done to enable mode choice of caffe runtime, i.e., CPU or GPU, and in this example CPU.</p>
<p><b>Usage</b></p>
<p>The example can be invoked using the following command: (From the build folder)</p>
<p>./examples/objectdetector/od_cnn_mnist_classification ../examples/objectdetector/Mnist_Classify/mnist.caffemodel ../examples/objectdetector/Mnist_Classify/lenet.prototxt ../examples/objectdetector/Mnist_Classify/1.png</p>
<p>The example as shown above takes 3 arguments, the locations of the weight file, network file and the image.</p>
<p><b>Next up will be a simple CNN trainer example.</b></p>
<p>Happy Coding!!!!</p>
<h1><a class="anchor" id="target2"></a>
Training a classifier for digits in Mnist Library using CNN. Part 1 </h1>
<p>This particular inclusion presents a simple trainer in a most crude and easy way possible. The major requirements of CNN training using caffe are</p>
<ul>
<li>Solver file</li>
<li>Training Network file</li>
<li>Image Dataset and a pointer to the Dataset</li>
</ul>
<p>The classification example added to the library involves usage of caffe library. The modalities and usage of the libraries can be studied at</p>
<ul>
<li><a href="http://caffe.berkeleyvision.org/">Caffe Library</a></li>
<li><a href="https://abhishek4273.com/2016/02/07/ann-chapter-3-deep-learning-using-caffe-python/">Using Caffe Library</a></li>
<li><a href="https://github.com/abhi-kumar/opendetection/tree/cnn_cpu">Branch of OpenDetection for the below classifier</a></li>
</ul>
<p>This example involves inclusion of three new files:</p>
<ul>
<li>"opendetection/examples/objectdetector/od_cnn_mnist_train_simple.cpp"</li>
<li>"opendetection/detectors/global2D/training/ODConvTrainer.cpp"</li>
<li>"opendetection/detectors/global2D/training/ODConvTrainer.h"</li>
</ul>
<p><b>Invoking Training module of caffe</b></p>
<p>These lines invoke the trainer: </p>
<div class="fragment"><div class="line">Caffe::set_mode(Caffe::CPU);</div>
<div class="line">SGDSolver&lt;float&gt; s(solverLocation);</div>
<div class="line">s.Solve();</div>
</div><!-- fragment --><p>This snippet points to solver file, the solver file points to the network file. This network file points to the file which in turn points to the dataset.</p>
<p><b>Usage</b></p>
<p>The example can be invoked using the following command: (From the build folder)</p>
<p>./examples/objectdetector/od_cnn_mnist_train_simple ../examples/objectdetector/Mnist_Train/solver1.prototxt</p>
<p>The only argument to be given is the solver file.</p>
<p><b>Next up will be a simple CNN trainer example with a graphical user interface for the solver file.</b></p>
<p>Happy Coding!!!!</p>
<h1><a class="anchor" id="target2_1"></a>
Training a classifier for digits in Mnist Library using CNN. Part 2 </h1>
<p>This commit consists of the same simple trainer from previous commit, except for the fact that it involves a graphical user interface to select solver parameters,The classification exampled added to the library involves usage of caffe library. The modalities and usage of the libraries can be studied at</p>
<ul>
<li><a href="http://caffe.berkeleyvision.org/">Caffe Library</a></li>
<li><a href="https://abhishek4273.com/2016/02/07/ann-chapter-3-deep-learning-using-caffe-python/">Using Caffe Library</a></li>
<li><a href="https://github.com/abhi-kumar/opendetection/tree/cnn_cpu">Branch of OpenDetection for the below classifier</a></li>
<li><a href="http://www.gtkmm.org/en/">GTKMM</a></li>
</ul>
<p>Installing GTKMM Required: In the terminal of ubuntu, type the following </p>
<div class="fragment"><div class="line">sudo apt-<span class="keyword">get</span> install libglib2.0-dev libatk1.0* libpango1.0-dev libcairo2-dev gdk-pixbuf2.0-0 libsigc++-2.0-dev libgtk-3-dev libcairomm-1.0-dev libpangomm-1.4-dev libatkmm-1.6-dev libgtkmm-3.0-dev </div>
</div><!-- fragment --><p><b>Usage</b></p>
<p>From the build folder invoke:</p>
<p>./examples/objectdetector/od_cnn_mnist_train_customSolver</p>
<p>Note:</p>
<ul>
<li>The path to the solver, train network, snapshot have to be set inside examples/objectdetector/Mnist_Train folder for this alpha version.</li>
<li>The GUI has a update button for every parameter. It is not necessary to press each one of them. They have been included for future, eg., when only one parameter from an existing solver need to be updated. This functionality has not been added</li>
<li>After changing the parameters, press the "Save" button and then close the window using the "x" on the top just like closing any window in Ubuntu. A custom exit has not been added yet.</li>
</ul>
<p><b>Next up will be a updates and additions to the GUI.</b></p>
<ul>
<li>Adding a provision: if a solver file exists and the user wants to change only one parameters, it can be done</li>
<li>Creation of CNN network using a simple GUI</li>
</ul>
<p>Happy Coding!!!!</p>
<h1><a class="anchor" id="target3_1"></a>
A GUI for easy-designing of CNN networks for caffe library. Part 1 </h1>
<p>It is always a tedious job in designing networks in a ".prototxt" file. It was a need of the hour to reduce the time spent on creating this file from scratch. The latest commit includes a user interface intended to facilitate this job. The modalities and usage of the libraries can be studied at</p>
<ul>
<li><a href="http://caffe.berkeleyvision.org/">Caffe Library</a></li>
<li><a href="https://abhishek4273.com/2016/02/07/ann-chapter-3-deep-learning-using-caffe-python/">Using Caffe Library</a></li>
<li><a href="https://github.com/abhi-kumar/opendetection/tree/cnn_cpu">Branch of OpenDetection for the below classifier</a></li>
<li><a href="http://www.gtkmm.org/en/">GTKMM</a></li>
</ul>
<p>Installing GTKMM Required: In the terminal of ubuntu, type the following </p>
<div class="fragment"><div class="line">sudo apt-<span class="keyword">get</span> install libglib2.0-dev libatk1.0* libpango1.0-dev libcairo2-dev gdk-pixbuf2.0-0 libsigc++-2.0-dev libgtk-3-dev libcairomm-1.0-dev libpangomm-1.4-dev libatkmm-1.6-dev libgtkmm-3.0-dev </div>
</div><!-- fragment --><p><b>Usage</b></p>
<p>After building the library invoke the following command from build directory</p>
<div class="fragment"><div class="line">./examples/objectdetector/od_cnn_network_design </div>
</div><!-- fragment --><p>This will pop up the desined window. The layers have been divided into 6 major categories, activation, critical, normalization, loss, extra, and data. The drop down menus are available and after selection of any of the option from the drop-down menu and pressing the "Append Layer" button next to it , a user can fill the required properties. This layer then can be added or left out as per user's wish. A display button has been provided to visualize the network.</p>
<p><b>Features</b></p>
<p>1) The <b>activation</b> category includes the following activation layers</p>
<ul>
<li>Absolute Value (AbsVal) Layer</li>
<li>Exponential (Exp) Layer</li>
<li>Log Layer</li>
<li>Power Layer</li>
<li>Parameterized rectified linear unit (PReLU) Layer</li>
<li>Rectified linear unit (ReLU) Layer</li>
<li>Sigmoid Layer</li>
<li>Hyperbolic tangent (TanH) Layer</li>
</ul>
<p>2) The <b>critical</b> category includes the most crucial layers</p>
<ul>
<li>Accuracy Layer</li>
<li>Convolution Layer</li>
<li>Deconvolution layer</li>
<li>Dropout Layer</li>
<li>InnerProduct (Fully Connected) Layer</li>
<li>Pooling Layer</li>
<li>Softmax classification Layer</li>
</ul>
<p>3) The weight initializers include the following options</p>
<ul>
<li>Constant</li>
<li>Uniform</li>
<li>Gaussian</li>
<li>Positive Unit Ball</li>
<li>Xavier</li>
<li>MSRA</li>
<li>Bilinear</li>
</ul>
<p>4) One more important feature included is that user can <b>display</b> the layers and simultaneously during the display <b>delete</b> the unwanted layers.</p>
<p><b>Current limitations and modalities to be added soon</b></p>
<p>This commit involves a alpha version of the GUI just for the review from users. The following the modalities that will be <b>updated</b> in the course of gsoc</p>
<ul>
<li>Concatenation layer in critical category has not been added yet</li>
<li>User will be able to generate the file, but cannot edit it once the GUI is closed. The next commit will include the interface to read a created file and edit further. Also the modality to be added is to give memory to the application so that it may edit any one particular layer even after creation.</li>
<li>The bias filler for the networks has the only constant type filler in this commit.</li>
<li>User has no option to validate the network connection in this version. The next modalities may include a tester that would allow users to validate the network modalities, i.e., the connections between layers, the layer proerties, naming conventions, etc.</li>
<li>The normalization, loss, and extra layers types' property editor will be added soon. Please see the layers in each after invoking the GUI.</li>
<li>The data layer with "image" and "LMDB" type inputs will be added soon.</li>
</ul>
<p>Happy Coding!!!! </p>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- HTML footer for doxygen 1.8.6-->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="tutorial_root.html">User Guide</a></li>
    <li class="footer">Send your queries <a href="mailto:kripasindhu.sarkar@dfki.de?Subject=OpenDetection" target="_top">here</a>.</li>
  </ul>
</div>
</body>
</html>
